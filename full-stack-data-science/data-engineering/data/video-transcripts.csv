video_id,datetime,title,transcript
Ot2c5MKN_-w,2024-11-20T13:31:14.000000,Multimodal AI: LLMs that can see (and hear),multimodal models are capable of processing and generating multiple types of data for example models that can convert text prompts into images transcribe speech and even caption images in this video I'll introduce three ways of developing multimodal AI systems by building on top of large language models I'll start with a highlevel overview of each approach then share python code demonstrating how to use use llama 3.2 Vision to solve various image to text tasks and if you're new here welcome I'm Shaw I make videos about the things I'm learning about and building in AI if you enjoyed this content please consider clicking the Subscribe button that's a great no cost way you can support me in all the videos that I make the goal of this video is to give a highlevel introduction to multimodal models and describe a specific type of these models which buil on top of large language models starting from the basic question of what is a multimodal model here I'll Define them as AI systems that can process multiple data modalities either as input output or both some notable examples of multimodal models include GPT 4 which can take in text images and audio and return back text another one is black forest's flux model which takes in US user text prompts and generates images based on those prompts or even a AI system likeo AI which can take in text prompts from users and generate music although these are all examples of multimodal models they are all developed in very different ways so here I'm going to talk about a specific type of multimodal model which buils on top of large language models these are sometimes called multimodal large language models or even multimodal language models I know the naming of these things may not be so great but that's what they call it in the literature and so the basic idea here is that we take a large language model whether it's pre-trained or we take the architecture of a large language model and we augment it in some way to be able to process multiple data modalities what that might look like we have our llm we augment it to also be able to take in images and audios and then add some components to the end of it so that it can also generate images in audio the key idea here is that these multimodal language models use llms as a core reasoning engine we might ask ourselves why use language models to go multimodal aren't these optimized specifically for text well there are a couple reasons why we might want to do this one llms have demonstrated a strong ability to acquire World Knowledge meaning that through the model's pre-training it learns a lot about the world and it also learns a lot about Concepts which we might not initially associate with text there was a popular paper I can pop it up on the screen called Sparks of AGI put out by Microsoft and there they were showing examples of how gp4 was able to draw images and understand spatial Concepts and do spatial reasoning even though it was strictly trained on text so it seems that even though large language models are only trained on text they also learn Concepts which may be helpful for processing other types of data another thing is that text prompts is a natural way to enable zero shot capabilities so the meaning of zero shot is basically using a model to perform a task that it was never trained on specifically text prompts which are native to large language models are a natural way to steer a model to perform these novel tasks through basic text descriptions so here I'm going to talk about three General paths to developing these multimodal language models so the first is going to be augmenting llms with basic tools the second is going to be augmenting llms using so-called adapters and then the third and Final Approach is to create a unified model which is trained from scratch so starting with path one this consists of adding external modules to do X to Tex text or text to X where X represents any arbitrary data modality it could be images audio EEG data time series data it could be anything so the way this works is that we'll have our large language model which is only capable of taking text in and spitting text out and if we wanted to say add the ability to process audio or speech a very simple way of doing this would be to pass any input speech into the system through a speech to text model like whisper and then passing that subsequent text into the large language model to do reasoning we simply just prompt the large language model to say do tasks based on the transcript for example if you wanted to summarize YouTube videos you could take the audio transcribe it with whisper and then pass the transcription to the large language model and simply prompt it by saying please summarize this YouTube video transcript or something like that we can also do something similar on the back back end so let's say we wanted to generate an image we can tell the large language model to generate a prompt that will be passed into a image generation model for example it could generate a prompt for flux and then flux will generate an image and indeed this augmentation of large language models with external tools is how the early versions of chpt was able to process multiple data modalities and so some pros of this is that this is very simple to implement you don't have to train any models you don't have to be a machine learning engineer you can get by with basic software engineering and connecting some apis together the other Pro is that no training data is needed you don't need to train a speech detex model you don't need to train an image to text model you don't need to train any adapters as we'll see in the next slide you can just use all these pre-trained models out of the box and Stitch them together to fit your needs but of course there are some cons one is that this type of system will have limited capabilities for example there may not be a pre-trained model that reasonably translate an arbitary modality into text for example if you have sensor data collecting information about the weather there may not be a natural way to translate that into text and pass it into the large language model another limitation here is that you can only customize the model through prompting the large language model so this may make it challenging to steer the model's performance in a particular direction another note about customization is that since these are all independently developed modules there's no guarantee that they're going to play nice together so it's going to take a lot of tricks and prompt engineering to get this to work well so a way we can unlock better customization of this type of multimodal model is by training so-called adapters to glue together different components of the system so specifically adding encoders and decoders to the llm which are aligned via fine-tuning so to show what this looks like we still have our llm which only takes in text and outputs text then let's say if we wanted to add images we can use a so-called image encoder like clip which takes the image and translates it into a dense Vector representation and then what we can do is train a relatively small number of parameters called an adapter to translate the dense Vector representation of the image encoder to a representation that is compatible with the lm's internal concept space and so this can be done in a few different ways popular Ways by using image caption pairs so you can train this adapter such that the image representation is similar to the llms representation of the corresponding caption and then we can do an analogous thing on the output side let's say we wanted to generate an image we can get a image decoder which translates a dense Vector representation into an actual image this might be a stable diffusion model and then we'll glue together the vector output of the large language model to the vector input of a stable diffusion model using a decoding adapter a key thing here is that there isn't a need to retrain the image encoder image decoder or large language model in this case the only things that need to be trained are the adapter weights so either the encoding adapter or decoding adapter or both the fire emojis are indicating the trainable parameters while the snowflake emojis are indicating the parameters which are frozen some of the pros to this approach is that you get better customization through adapters you have a way of aligning the different pre-trained models so that they work nicely together another upside is data efficiency since we're only training a relatively small number of parameters through these adapters you don't need massive training data sets you can probably get good performance with on the order of hundreds or thousands of examples the cons of course is that this requires training data while in the previous approach we didn't require any training data so this is a bit more work on the machine learning engineering side and also this is a bit more technically sophisticated than before so now you can't really get by with just basic software engineering and API calls you're going to need to put on your data engineering hat you're going to need to put on your data scientist hat and your machine learning engineer hat in order to develop a system like this and I also want to point out a few nuances here so there are a wide range of models in the literature that follow this approach and sometimes they'll take it one step further in that not only training the adapters but after doing so there'll be a second stage of fine-tuning where they'll do an endtoend fine-tuning of the system another Nuance here is that sometimes people will add cross attention layers inside the large language model which will allow the model to mix the different modalities together and another Nuance is that you know maybe you want to bring your own pre-trained model so you don't want to use a readily available model like clip you want to create your own image encoder on your own custom data set but then you want to plug that into an existing large language model so there can be a lot of nuance here than the simple picture I've shared and so if you want to get a better sense of all the different ways that this approach can look here are several examples from The open- Source Community now it seems llama 3.2 vision is gaining some traction and then there are several others as well the final way that we can make large language models multimodal is by mixing modalities at pre-training in other words training the llm to be multimodal from scratch this will greatly simplify the inference architecture because the model can natively take in any data modality and spit out whatever data modality you train it to do so some key pros of this is that you get seamless modality integration you don't have to worry about external modules and pre-trained models and gluing them together with adapters it's kind of like how Apple builds their products to be seamlessly integrated end to end as opposed to like a gaming PC which hobbyists will source and put together on their own another Pro of this is faster inference times since data doesn't have to be piped and pass through different models and everything is just passing through a single model there's less computation that needs to be done so there will be faster inference times and this is something that was observed with GPT 40 when it first came out but of course the cons of this approach is that it comes with many difficult technical challenges so the models that we've seen that use this approach come from teams of dozens of researchers and engineers at top institutions so this isn't going to be like a fun weekend project for anyone just like training large language models from scratch comes with massive data and compute requirements training a multimodal large language model from scratch will require even more because of the added modalities unless you've got a team in tens of millions of dollars behind your back it's unlikely that You' be able to develop a model like this that's comparable to what we're seeing in today's landscape some examples of this that I'll call out are presumably GPT 40 and Gemini although they don't share all the details of how these models are developed it seems that they do have some kind of unified pre-training and then some other ones in the open source Community are emu3 blip and chameleon so now that we have a basic idea of how these multimodal language models are developed let's see what they can do so in this example I'm going to use llama 3.2 Vision to perform a variety of image-based tasks so in order to run this example I'm going to use ol Lama which allows you to run these models locally on your machine so you don't need to make any API calls in order to get olama installed on your machine you'll need to go to their website ama.com and then you'll need to download it for your operating system another requirement is to install their python Library so it's a simple pip install olama jumping into the code there's only one import so we'll just import ol llama and then we'll download the model using this line of code here so here we're grabbing llama 3.2 vision and this will be the 11 billion parameter version and then we can use this model to do basic visual question answering so it's actually pretty amazing in Just One Import and then one line of code we're already ready to start using this model but anyway we can pass an image and question to the model using this syntax here so we'll do. chat and this is very similar to the Syntax for open AI API so you specify the model and there are several models available on oama so we're going to use the one we just imported and then we'll Define the messages we'll just have one message which will be stored in this list of dictionaries so we specify the role this message is coming from the user the content of the message is the text associated with it which is going going to be what is in this image we can also pass in images these are saved in a folder in the project directory called images and it's a photo of me sitting we can also pass in multiple images it's not limited to just one and then with that we can just print the response of the model this is what the image looks like it's a photo of me from a networking event in fireside chat and then I asked llama 3.2 what is in this image and this is what it said it said this image shows a man sitting on a yellow Automan with his hands clasped together he is wearing a black polo shirt with a name tag that says Shaw and a black baseball cap with the white text that reads the data on Ray preneur the background of the image appears to be an office or lounge area with a large screen on the wall behind him displaying a presentation slide there are also several chairs and tables in the background suggesting that this may be a meeting room or common area for employees to gather and work so I would say for the most part it does a pretty good job my PO is actually more of a gray than a black but I guess that's pretty close it's spelled entrepreneurs wrong but it's actually pretty amazing that I was able to read that I can't really read that from this angle this is indeed a lounge area behind me in a co-working space so does a pretty good job just from this image without any additional context another thing we can do is streaming so on my machine I have the M1 chip but it still took like 30 seconds or a minute to generate that pre previous response to make that waiting a bit more bearable we can enable streaming so this is kind of like when you go to chat GPT you type in a prompt and it'll start spitting out text one word at a time it's just better from a user experience standpoint so we can easily enable streaming with olama using this syntax so we just have the same. chat same exact inputs but then we just have this stream equals true argument and then what we can do is print text from the Stream in a for Loop and then if we run this we'll see the response appear one word at a time I asked a slightly different question the second go around I asked it can you write a caption for this image and it gave a different answer which is pretty interesting slightly different prompts but quite different responses I'm not going to read through this but some notable things is that instead of the data onr preneur it said the data Enthusiast so it's interesting how that changed and then also it kind of caught the fireplace the second time around it started describing the paint and the design of the background so I think this is a fun thing to play around with locally on your machine passing in images and asking questions about it could be a fun thing to do describing objective images is one task but describing humor in memes requires a different level of let's say intelligence so let's see how well the model can explain memes to me basically the same exact code as before but now we're just going to pass it a different image and a different prompt so I'm asking to explain this meme to me so the meme looks like this this is from a previous video and a LinkedIn post so I asked llama 3.2 to explain the meme to me and this is what it said the meme depicts Patrick Star from SpongeBob SquarePants surrounded by various AI tools and symbols the caption raids trying to build with AI today the image humorously illustrates the challenges of using AI in building projects implying that it can be overwhelming and frustrating so I think it got the gist of the meme it recognized Patrick which is kind of amazing when you think about how this model was trained using that path two of aligning an image encoder with llama 3.2 is internal concept space and then the final thing is optical character recognition the code looks exactly the same but now we're going to prompt it to say can you transcribe the text from the screenshot in a markdown format and then we'll pass it in a different image so this is from a LinkedIn post describing five AI projects and so not only do I want it to parse this text and write it out I want it in the markdown format so let's see how it does it says here's the transcript of the text in a markdown format and it does a pretty good job the only thing that's missing here is that this should be a header so there should be a hashtag or a couple hashtags in front of it but everything else it did a good job you know I didn't see any expelling mistakes and then it got the numbering and the bullet points right the amazing thing about these emerging multimodal systems is really this zero shot capability no before if I wanted to do OCR I needed an OCR specific model and that model may not do markdown but now I have a single model that can not only do markdown it could probably do HTML it could do plain text you can easily customize these models simply by changing the prompt no fine-tuning or model training required I barely scratched the surface with these three examples but hopefully this gives you an idea of what these models can do so this was actually the first video in a larger series on multimodal AI in the next video of the series we're going to talk about multimodal embedding models so we actually already talked about an example of such a model in clip when talking about path two toward creating one of these multimodal language models and basically the way clip works is that it'll take in both text and images and it will represent them in a shared Vector space such that the text for example a silhouette of a dog will appear close to an image of a silhouette of a dog and similarly the text and image of a silouette of a cat might look like this ama's logo and the text Ama might look like this and then Python programming language would be relatively close to Python's logo so I'll talk about how we can train these models and what we can use them for in the next video of this series and if you enjoyed this video and you want to learn more check out the blog posted in towards data science and although this is going to be a member only story you can access it completely for free using the friend Link in the description below and as always thank you so much for your time and thanks for watching
gUJJB235DVs,2024-11-18T15:11:07.000000,5 AI Projects You Can Build This Weekend #python #ai,five AI project ideas that you can implement this weekend one resumé Optimizer use open ai's python API to adapt your resume to specific job descriptions number two YouTube lecture summarizer take those long lectures collecting dust in your watch later playlist and summarize them to see if they're worth keeping around number three PDF organizer if you're like me you have an unreasonable number of research papers on your desktop you can build the tool to generate text embeddings for each PDF and then use a simple clustering algorithm to organize them into folders number four is multimodal semantic search searching text is easy but what about images and videos now you can use multimodal embedding models to put text and images in the same search space then finally desktop question answering you can connect a multimodal database
bAe4qwQGxlI,2024-10-25T13:18:08.000000,I Built an AI App in 4 days... here's how.,I built a web app to translate YouTube videos into blog posts the crazy thing is before building this I had absolutely no web development experience yet I was still able to build and deploy this project in Just 4 days so how did I do that in this video I'm going to share my full process for building this app and all the tools I used to go from idea to deployment and if you're new here welcome I'm Shaw I make videos about D data science and Entrepreneurship and if you enjoy this content please consider clicking the Subscribe button that's a great no cost way you can support me in all the videos that I make for a bit of context my background is in data science which is basically all about using data and machine learning to solve problems while this means I've done a lot of programming coding to process data or train models is very different than building consumer software this is a common struggle for most data scientists who might be very comfortable with python but never learn how to ship SAS products with my savings on track to hit $0 this quarter I was determined not to be one of these data scientists after spending too much time on Indie hacker Twitter I figured the best way to learn this skill set was by doing it that's why this quarter my goal is to ship one product every single month until until something makes money my first such product is a tool for converting YouTube videos into blog posts it took me just 4 days to build and deploy the initial prototype for this app and here I'm going to share exactly how I did it the first day I was starting from scratch I had my goal of shipping a product in less than a month but hadn't settled on an idea my approach to picking product ideas came from advice I got from Steven Wolfram who said the product time building is a product that I want my interpretation of this advice is to solve your own problems this led me to three product ideas a YouTube thumbnail generator a YouTube clip finder and a YouTube video to blog converter after doing some basic research and playing around with chat GPT I settled on this last idea because I felt it was the one I could implement the fastest I had also seen a job post on upwork paying a few hundred to do exactly this with an idea settled my next step wasn't to start coding instead I started designing the website and user interface for me starting with the design was actually a necessity because I had no idea how to code the thing my approach to doing the design was to First create a brand blueprint in other words creating a color palette picking fonts and designing a logo for the product for the color palette I used a website called cooler to quickly find colors I liked then brought everything together in canva having this brand blueprint made it pretty easy to design a simple UI at the end of day one I had something that looked like this on day two the goal was to implement the front end based on my canva design since my goal was to ship the product by the end of the month I didn't have time to learn HTML and a front-end framework the fastest way forward would be to use the language I'm most com able with python lucky for me a few months ago a python library was released to do exactly this the library is called Fast HTML which allows developers to build modern web apps in Python I spent the morning learning fast HTML by watching a lecture from the creators and reading through their documentation then in the afternoon I started coding the front end based on my canva design my first step was to take a screenshot of the design and pasted into cursor to get me started even though fast HTML is a new library that Claude and GPT haven't been trained on I uploaded this text file from Fast html's GitHub repository to give cursor more context which actually worked surprisingly well this experience sold me on using cursor and AI assistance for coding I'd also use chat GPT from time to time for tasks such as writing CSS styling this workflow of trying AI chat in cursor with Claude first then going to chat GPT if needed became my standard approach by the end of the day I had a site that looked very similar to my initial design by day three I had coded my website but it didn't do anything The Next Step was to implement the backend I started by asking chat GPT to write me a prompt for creating blog posts based on YouTube video transcripts I tinkered with this prompt in chat GPT a bit and experimented manually with different videos when I had something that looked good I implemented the whole process in Python the two key libraries I used here were the YouTube transcript API and open ai's python API since I've used these libraries for past projects I was able to repurpose most of the code from these examples that was the extent of the AI I used for this initial version I could have done something more sophisticated like allowing users to upload PDFs or fine-tuning a model on my own YouTube videos and blog posts but that would a taken time I don't have right now if this product is something that people actually want then I'll go back and make these improvements the rest of the backend implementation was just making the website work via basic web development which I was all learning on the Fly by day four I had a working prototype of the app running locally on my machine but you can't really sell a product that only runs on your computer the goal of day four was to deploy my app to the internet to avoid abuse from spammers and Bots I set up Google oo this is basically that signin with Google option that you see on websites these days the benefits of requiring people to sign in with Google are one I don't have to manage sensitive information like user passwords and two I don't need to worry about verifying that the users are human while setting up ooth sounds simple enough it took me most of the morning to get this working after that I bought a custom domain for the app I paid $70 for the domain yb. from Squarespace and then finally I deployed the app on Railway this was the fastest option because there was example code for deploying a fast HTML app to railway in their documentation after a handful of failed deployments and switching my domain provider to Cloud flx I was finally up and running here's a demo of me using that early version of the app I used it to create a blog post for a podcast that I hosted last year typically it takes me at least an hour to write a first draft for a blog like this however using this tool I was able to do that in just 5 minutes and then after just one day of this blog being monetized on medium I've made $112 from it which is already some real value for me personally in just four days I went from idea to production this is the world now thanks to python libraries like Fast HTML coding tools like cursor and deployment services like Railway developers can Implement ideas faster and validate them in public while this project would have taken me weeks without these tools it was still a prototype so it had limitations such as ooth users had to be manually defined in the Google Cloud console there wasn't a database hooked up to the app so users could use it without any limits and most importantly there was no stripe integration so no way of making any money from it it took me another 8 days to make an MVP version for which any Google user could sign in usage metrics were stored in a SQL light database stripe integration was set up and created a landing page that included a demo for people who hadn't signed up yet you can try out the latest version of the app completely for free using the link in the description below although shipping an app in one month may have sounded crazy a few years ago this is becoming normal based on my experience with this first product here are three key tips for those trying to do something similar first build with what you know if I had to learn JavaScript or react before even starting this project I'd still probably be watching YouTube tutorials Second Use AI tools coding assistants like cursor and stbt have become the norm in programming if you aren't using them you're probably moving two times slower than you would otherwise third and finally it's not just about building I know this video focused on the development side of the product but what are just as important if not more are the idea that you build and how you Market it for ideas I'd take wolfram's advice and solve your own problems this allows you to move faster and increases the likelihood of validation because you are your own customer for marketing that's still something I'm trying to figure out but when I find something that works I'll be sure to share it here if you have any questions about my process or any of the tech I used please let me know in the comments and as always thank you so much for your time and thanks for watching
4QHg8Ix8WWQ,2024-10-17T12:50:12.000000,Fine-Tuning BERT for Text Classification (Python Code),"massive Transformer models like GPT 40 llama and Claude are the current state-of-the-art in AI however many of the problems we care about do not require a 100 billion parameter model there are countless problems we can solve with smaller language models in this video I'm going to share one example of this by fine-tuning Bert to classify fishing URLs I'll start by covering some key Concepts then walk through example python code and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship if you enjoy this content please consider clicking the Subscribe button that's a great no cost way you can support me in all the videos that I make so the title this video has a few pieces of jargon fine-tuning bird and text classification so let's talk about each of these terms one by one starting with fine-tuning this is when we adapt a pre-trained model to a particular task through additional training using an analogy we can think of the pre-trained model like a slab of marble that we can refine and chisel into a statue which in this analogy would be our fine-tuned model and typically the pre-trained model in this approach to machine learning is a self-supervised model meaning that the labels used to train the pre-trained model aren't manually defined but rather are derived from the structure of the data itself for example when talking about Text data we can use the inherent structure of text in order to do word prediction we could take a sentence like adapting a pre-trained model to a particular task and simply train the model to recursively predict the next tokens so maybe adapt would be used to predict ing adapting would be used to predict a adapting a would be used to predict pre adapting a pre would be used to predict train and so on and so forth so the key upside of this is that humans do not need to annotate or label the training data on the other hand the fine-tuned model will typically be trained in a supervised way in other words the targets are manually defined by humans so the reason we might want to develop a model in this way is because since the pre-trained model doesn't require any man manual annotation of training data that means we can train on much larger training data sets we no longer have that human bottleneck for example Bert which is the model we're going to be focusing on in this video was trained on 3 billion words in other words we have billions of examples that this model can learn from this is in contrast to a typical fine-tuning task which might consist of a few thousand examples so we're talking about six orders of magnitude difference in the number of examples today this is the prevailing Paradigm for developing state-of-the-art machine learning models there is an initial step of pre-training which will typically use some kind of self-supervised learning which allows the model to learn a tremendous amount of knowledge on a massive training data set and then the pre-trained model can be refined to be a bit more helpful through fine-tuning another benefit of developing models like this is that it unlocks the democratization of AI because developing these massive pre-trained models like llama or mistel or Claude or GPT 40 requires tremendous resources that no individual or typical research group will be able to pull off however splitting the model develop ment process into these two steps of first pre-training and then fine-tuning enables the pre-training to be done by specialized research Labs such as open AI Google meta anthropic mistal so on and so forth who can then make their pre-trained Foundation models publicly available for fine-tuning another Nuance here is that we aren't restricted to just doing one iteration of fine-tuning really any fine tun tuned model can serve as the starting place for additional fine-tuning so maybe we'll take this fine-tuned model and then we'll do some additional fine tuning to make it even more specialized for a particular use case and this is exactly the story of Chad GPT which was developed through three phases it started with the unsupervised pre-training then there was a supervised fine-tuning step and then there was a final step of reinforcement learning which refined the model even further that's all I'm going to say about fine-tuning in this video but if you want to learn more I do a deep dive in a previous video of this series the second piece of jargon in the title of this video is Bert even though this Paradigm of pre-training and fine-tuning models got super popular when open AI released chat gbt and shared that they were able to do that through this three-step process of pre-training fine-tuning and then additional fine tuning this is an approach that's been around since at least 2015 and one of the early models that really exploited this idea was Google's Bert which was released in 2019 ber is a language model developed specifically with fine-tuning in mind in other words the researchers at Google created Bert so that other researchers and individuals could repurpose the model through fine tuning on additional tasks in order to do this Bert was trained on two tasks the first task is masked language modeling what that means is is if we have the sentence the cat blank on the mat masked language modeling consists of using the context before and after this masked word this hidden word in order to predict it so given this input sequence this can be passed to Bert and then Bert will predict what the masked word is this is in contrast to models like GPT which don't do masked language modeling but rather causal language modeling where the training task is next word or next token prediction as opposed to masked language modeling the benefit of doing masked language modeling versus causal language modeling is that the model can use context from both sides of the sentence it can use both the text before and after the masked word in order to make a prediction intuitively this additional context gives the model more information for making predictions but that's not the only thing that Bert was trained on the second task that it was trained to do is next sentence prediction what this looks like is given two sentences A and B so here sentence a is Bert is conceptually simple and empirically powerful and then sentence B is it obtains new state-of-the-art results on 11 NLP tasks Bert is trying to take sentence pairs like this and output a label of is next or is not next this is a sentence pair taken from the original ber paper which is reference number one and so this is indeed the next sentence alternatively we could have a pair of sentences that are not a match so instead we might have Bert is conceptually simple and empirically powerful and then the next sentence is the cat sat on the mat and so this would be not the next sentence and would receive a different label the intuition behind having this second task of next sentence prediction in addition to mask language modeling understanding the relationship between two sentences like sentence a and sentence B is important for Downstream tasks like question answering or sentence similarity so training Bert on these two different tasks allows it to be effectively fine-tuned on a wide range of tasks the last thing we're going to talk about before getting into the example code is text classification which consists of assigning a label to text sequences and actually the next sentence prediction task we saw in the previous slide is an example of text classification however there are countless other examples of where text text classification might be handy one is Spam detection so given an input sequence of text which is a incoming email we could develop a model to predict whether that email is Spam or not spam similarly if we have incoming it tickets we could develop a text classification model to take those text sequences and categorize the it tickets into the appropriate tiers finally one could use a text classification model to do sentiment analysis on customer reviews in other words to analyze the number of happy customers and the number of unhappy customers with all the jargon covered fine-tuning we covered Bert and we covered text classification let's see a concrete example of doing this so here I will fine-tune Bert to identify fishing URLs this will actually be similar to the example I shared in my fine-tuning video from last year however soon after posting that video someone had pointed out an issue in that example that I had missed and if we look at the training metrics it's actually pretty easy to spot the training loss is decreasing as expected then if we roughly look at the accuracy it improves from the early Epoch to the last epox but it's a bit shaky so actually the best performing Epoch was number three and it seems to wiggle around for the remainder of training but the most obvious red flag is if we look at the validation loss so this is actually monotonically increasing during training which is the opposite of what we want to happen so this is a clear sign of overfitting this wasn't something I had caught so I'm glad someone had pointed this out on the medium article here I'm going to do another example where we don't see this overfitting so for this example we'll be using Python and the hugging face ecosystem so we'll import the data sets library from hugging face which gives us a data structure for loading our training testing and validation data we'll import some things from the Transformers Library the auto tokenizer class the auto model for sequence classification training arguments class and the trainer we'll import the evaluate library from hugging face which has some handy model evaluation functions we'll also import numpy which will be used to compute some of these metrics and then we'll also import this data collator from the Transformers Library which we'll talk about in a little bit with our Imports we'll load our data set so this is something I've made freely available on the hugging face Hub it's a data set of 3,000 examples 70% of which are used for training 15% of which are used for testing and then the final 15% are used for independent validation with our data loaded in we can load the pre-trained model so here we're going to be using Google's Bert more specifically Bert base uncased which consists of 110 million parameters which by today's standards is Tiny But at one point this was a pretty big model we'll load in the tokenizer for this model what this will do is take in arbitrary text sequences and convert them into integer sequences based on what the Burt model is expecting finally we can load in the Bert model but slap on top of it a binary classification head so all we have to do is use this autom model for sequence classification class and use this fir pre-trained method and we'll pass into it the model path which is what we defined here the number of labels the number of classes and then a mapping for the classes so the ID is going to be an integer and then we'll have a label for each class zero will correspond to a safe URL and one will correspond to a unsafe URL with our model loaded in and our classification head slapped on top of it let's set the trainable parameters by default when we loaded in our model in this line of code it initializes all the model parameters as trainable so all 110 million parameters plus the parameters in the binary classification head are all ready to be trained however if you're just running this example on your laptop like I did that's going to be pretty computationally expensive and potentially unnecessary so to help with that here we're going to freeze most of the parameters in the model we can do that pretty easily so here I have a for Loop that's going to go through the base model and it's going to freeze all the base model parameters to just break this down a little bit we have this model object which has this base model attribute and then that base model attribute has this named parameters method which will return tupal corresponding to all the parameters in the model basically what we can do is go through all the base model parameters and set this requires grad property to false after running this all the base model parameters are frozen and only the parameters in that classification head that we slapped on top of the model are trainable another name for for training a model like this is called transfer learning where we leave the base model parameters Frozen and only train a classification head that we add on top of it however this might result in a pretty rigid model because we can only refine the parameters in that classification head so one thing we can do is we can unfreeze the base model parameters in the final two layers so in the pooling layers of this model one way we we can do this is that we can loop back through the base model parameters and then if we see the word Pooler in the name of the model we can just unfreeze those parameters the result of all this is that we freeze all the model parameters except for the last four layers this allows us to keep the computational cost down for fine tuning while also giving us a fair amount of flexibility and of course you can free free or unfreeze any of the model parameters that you like and this would be like a fun thing to experiment with on your own with our model ready to be trained next we need to pre-process our data this is actually going to be pretty simple we'll Define a function called pre-process function which will take in an arbitrary sequence of text and tokenize it so it'll translate a string of words into a sequence of integers according to to the Bur tokenizer additionally I add this truncation flag what that will do is ensure that none of the input URLs are too long so by default I think this will truncate all the input sequences to 512 tokens and then with that function defined we can just apply this pre-processing step to all the data sets in our data set dictionary that we imported earlier so this will tokenize the training testing and validation data sets and return it in this tokenized data variable and then another thing we can initialize at this point is a data collator and so we took care of input sequences that might be too long but when we are training a model it's important that every example in a batch is the same size and so even though we won't have any examples greater than 512 integers we will have examples with fewer than that so to ensure all the samples in a batch have a uniform length we can create this data collator which will automatically do that for us during training okay and then the final step before we actually train the model is to Define evaluation metrics these are the metrics that will be printed during the training process here I'll use two evaluation metrics the accuracy and the Au score these are both loaded from the evaluate library then I will Define this function called compute metrics which will comp comp the accuracy and Au score for any example so the input of this function will be a tuple it'll consist of predictions and labels predictions will be a logit it will be a number between minus1 and 1 while labels will be the ground truth so this will be either zero or one to convert the logits into probabilities so basically to map a number between -1 and 1 to a number between 0 and 1 we'll apply the softmax function which looks like this this will compute probabilities for both cases for both the URL is safe and the URL is not safe so let's only look at the probabilities for the URL being not safe and then we'll use that to compute the Au score we do that by passing in the positive class probabilities so the probability that the URL is not safe and the ground truth and we'll round it to three decimal plates that gives us our Au score and then we can predict the most probable class so predictions will consist of two numbers it'll be a loic corresponding to a safe URL and the logic corresponding to a unsafe URL we'll just do ARG Max so it'll just return which element is larger and then we can pass that into this line of code to compute the accuracy so we'll compare the predicted class with the ground truth and we'll round that number to three decimal places then we'll return a dictionary which consists of the accurate and Au score while this may have been a lot of Hoops to jump through this will be nice during training because at each Epoch our trainer will print out the accuracy and Au score for us now we're ready to start training the model we'll Define our training parameters we'll set the learning rate at 2 to Theus 4 batch size as eight and number of epoch as 10 we'll put all these into a training arguments variable so we can set our output directory which I set as Bert fishing class classifier teacher the reason this teacher is here we'll talk about near the end we'll set the learning rate defined here we'll put the per device training and evaluation batch size which is just going to be eight number of epoch is 10 and then logging evaluation and save strategy so logging strategy sets how often the trainer will print the training loss eval strategy sets how often the trainer will print the evaluation metrics that we defined earlier so Au and accuracy and then we can also set our save strategy so we can have the model get saved at every Epoch just in case something goes wrong we can refer back to that latest save and then we'll set that the trainer will load the best model at the end so if the 10th EPO isn't the best model maybe it was the eighth Epoch we'll use that one instead of the last one with all the training arguments set we are ready to find tune our model we'll pass everything into our trainer pass in our model the training arguments from before our TR training data set our evaluation data set the tokenizer the data cator and the compute metric function and then simply run trainer. Trin so this I think took about 15 minutes to run on my laptop didn't use a GPU or anything but here are the results we can see the training loss is steadily decreasing we see the validation loss is a little rocky at times but it eventually goes down and then we have our accuracy in AU C so we can see the accuracy is for the most part increasing and then the Au is steadily increasing so this is what we want to see we don't want our validation loss to be monotonically increasing we want it to be decreasing with training but we can go one step further this is evaluating our model on the training data and the testing data but let's look at it on the independent validation data set this validation data was not used for training the model parameters or to the hyperparameters so it gives us a fair evaluation of the model to apply the model to the validation data will'll generate predictions for it like this then we will extract the logits and the labels from the predictions and then we'll just pass these into the compute metric function defined earlier and then we can see that these are the results the accuracy and Au score are comparable to what we're seeing for the testing data set in the previous slide so accuracy about 0.89 Au about 0.9 5 if we go back we see accuracy 0.87 Au 0.95 pretty similar so that's a good sign that this model is probably not overfitting at the end of this we have a binary classification model that consists of a little over 110 million parameters however as a bonus we can actually shrink this model even further and that's exactly what I do in a previous video of this series where I talk about model compression for for large language models the example in that video I take the model that we just created here and reduce its memory footprint by 7x using model distillation and quantization the key upside of these compression techniques is that we can shrink the computational requirements for our models without sacrificing performance in fact for the example that I walk through in this video the performance actually goes up a little bit that brings us to the end if you enjoyed this video and you want to learn more check out the blog and towards data science there I cover some more details that I may have missed here and even though this is a member only story you can access it completely for free using the friend Link in the description below and as always thank you so much for your time and thanks for watching"
tMiQIxSX64c,2024-10-10T13:50:57.000000,5 AI Projects You Can Build This Weekend (with Python),the best way to develop your AI skills is by building projects however sometimes figuring out what to build is the hardest part of getting started in this video I'll share five AI projects that you can build with python fast I'll break down the steps and python libraries for implementing each idea and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoy this content please consider clicking the Subscribe button that's a great no cost way you can support me in all the videos that I make the number one mistake people make when thinking of project ideas is they start with the question how can I use this new technology while this can be a fine way to learn a new tool there is a better way good projects start with a different question what problem can I solve this not only makes for a better story when sharing with potential employers but solving problems is how you translate Technic Tech skills into value the following five projects all take this problem first approach you can take these ideas and Implement them directly or even better use them as inspiration for solving a problem that you face personally an effective yet timec consuming part of applying the jobs is adapting your resume to different job descriptions while automating this process would have been a pretty Advanced project a few years ago with today's large language models this this is a simple API call here's a step-by-step breakdown of how you can do this today first start by creating a markdown version of your resume note that this is something that chat GPT can do for you second go to chat GPT and experiment with different promp templates that take the markdown version of your resume and a job description and output a new resume in the markdown format third move from chat GPT to python by using open AI python API to prompt GPT 40 mini to dynamically rewrite your resume and then fourth and finally convert your markdown resume to PDF using the markdown and PDF kit python libraries while this is something we can readily use Chad GPT for the upside of implementing this process in Python is that we can easily scale it up to hundreds of resumés if you're having trouble getting started I have some starter code freely available in the blog post Linked In the description I'm always eager to add new technical videos to my watch later playlist however I'm usually not as eager to actually watch them so these videos tend to sit around for weeks if not months a project that could help with this is a tool that watches the videos for me and generates concise summaries with key points here's one way to do that first given a YouTube video link extract the video ID using regular Expressions Second Use the video ID to extract the transcript via the YouTube transcript API python Library third experiment with different chat GPT prompts that effectively summarize the transcript and then fourth automate this whole process by using open ai's python library from a technical perspective this is very similar to the first project a key difference however is that we will need to automatically extract video transcripts and feed them to a large language model if you want to see how to do this check out the example code in the blog post my watch later is not the only place I hoard technical information another cache is my desktop which is riddled with hundreds of unread research papers since manually reviewing these papers would be very timec consuming let's see how AI can help we can use text embeddings to translate each paper into a dense Vector representation with which similar articles can be clustered together using a traditional machine learning algorithm like K means here's a more detailed breakdown first read the abstract from each research article using the P muw PDF python library next use the sentence Transformers library to convert these abstracts into text embeddings and store them in a pandis data frame third use your favorite clustering algorithm from sklearn to group The embeddings based on similar it then fourth and finally create folders for each cluster and move each file into the appropriate folder the key step for this project is generating the text embeddings which I talked more about in this video there I talk about what they are and share example code for using them a couple of months ago I helped a company design a basic rag system to search over a set of technical reports a key challenge with searching over reports like this is that key information is often represented in plots and figures as opposed to text one way we can incorporate this visual information into the search process is using a multimodal embedding model which can represent text and images in the same embedding space here are the basic building blocks for that first given a PDF chunk it into sections and extract the images using p muw PDF Second Use a multimo embedding model to represent chunks and images as dense vectors you can find such a model on the hugging face Hub and access it using the Transformers python Library third repeat this process for all PDFs in the knowledge base fourth given a user query pass it through the same embedding model used for the knowledge base fifth compute the cosine similarity between the query embedding and every item in the knowledge base using SK learn the sixth and final step is to return the top K most similar items as search results the most important step of this project is how the PDFs are chunked the simplest way to do this is to use a fixed character count with some overlap for each chunk it's also helpful to capture metadata such as file name and page number for each chunk and image I share some basic boilerplate code for doing this in the blog post for this video over the past year I've helped about a 100 businesses and individuals build their AI projects by far the most common project people ask for is a knowledge-based question answering system this is something we can build in a straightforward way building on top of the previous project given our documents are already chunked and stored in a database we can convert the multimodal Search tool into a multimodal rag system first perform a search over the knowledge base just like we did in Project number four second combine the user query with the topk search results and pass them to a multimodal model this can be something like GPT 40 or llama 3.2 Vision third create a simple gradio user interface for the question answering system this project essentially combines projects 2 and four however it includes the essential component of a user interface for that we can use a dashboarding tool like GR radio which allows us to create a chat UI with just a few lines of code okay we covered a lot here so I wanted to close with two key takeaways the first is to start with the problem not the technology solving problems puts your effort into a larger context and is ultimately what generates value second is to use tools like chat GPT and cursor to help you be a more productive programmer issues that you used to block me for hours if not days a few years ago can now be resolved in minutes using coding assistance is the new Norm in programming so I encourage you to use these tools to help you learn faster and build Bolder projects if you have questions on any of these project ideas let me know in the comments and as always thank you so much for your time and thanks for watching
7Oy2NmPwJXo,2024-09-26T23:24:35.000000,I Quit My Job… Here’s How Much I Made 1 Year Later,"14 months ago I made a big life change and left my sixf figure data science job to pursue entrepreneurship full-time the burning question people have when I tell them about this is how do you make money here I want to pull back the curtain and share all the different ways I scraped together cash in my first year of Entrepreneurship this is not meant to be a flex but rather it's meant to serve as a realistic reference for anyone considering a similar journey and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoy this content please consider clicking that subscribe button that's a great no cost way you can support me in all the videos that I make before diving into my income sources here's a quick recap of my journey so far in 2019 I stumbled into data science while getting my physics PhD in 2020 I started making technical content and sharing it on medium and YouTube in 2021 I began picking up freelance gigs through upwork and referrals in 2022 I completed my PhD and started working at Toyota as a data scientist then in 2023 I left my full-time role to make my side hustles my main Hustle the upcoming breakdown is based on my revenue between July 2023 when I left my job and July 2024 I'll review my nine different income sources in descending order and share details about how each work my big biggest Revenue Source was my blog on medium which earned me $612 349 this works via mediums partners program where writers get paid based on engagement on members only stories while I've written articles on a wide range of topics my top earning stories tend to be the ones on data science topics published in towards data science my second biggest income Source was AI Consulting contract which earned me $582 38 since I had some freelance experience under my belt when I left my job I expected this to be my main source of income however that's not how things worked out one reason for this is that I wasn't actively applying for jobs on upwork like I was doing back in grad school instead I relied solely on inbound leads that came through YouTube and other content channels over the 6 months that I was doing AI Consulting contract S I took 36 Discovery calls two of which turned into contracts during this time I also realized that Consulting was too much like working a job there would be weekly calls email exchanges and spending my time on other people's goals rather than my own that's why in March 2024 I stopped taking on AI Consulting contracts if you're curious about the details of that decision I talk more about it in a previous video my next biggest income Source was YouTube which made paid me $566 26 similar to medium's partners program YouTubers get paid to make content the key difference however is that monetization mainly comes from ad Revenue rather than premium memberships my top earning YouTube videos tend to track with my top earning medium articles however I get significantly more inbound leads from YouTube than medium which makes YouTube potentially a lot more valuable the fourth biggest in income Source was paid calls which earned me $3,263 34 this is a lighter version of the AI Consulting work that I mentioned earlier rather than working on medium to long-term projects with clients these calls are limited to short 30 to 60 Minute interactions I like this because it doesn't require the overhead cost of Discovery calls contract writing and scheduling meetings clients can directly book times and pay for services all through my C Ally page which makes this a much more convenient exchange for both sides okay now we're starting to get into the long tale of my Revenue sources the fifth Source came from a single 90-minute AI workshop for a large company I charged $2,250 for the session plus the recording while this opportunity sort of fell out of the sky I realized two things after doing it one I enjoyed putting the workshop together and two I could provide this as a formal service to businesses this is what led me to launch my custom AI training service last month Beyond being another Revenue Source a key benefit of this offer is that it's a great opportunity for me to develop my sales and marketing skills which are probably my biggest weakness as an entrepreneur my sixth Revenue Source came from a referral which paid me $1,000 even though I closed the door on project-based Consulting work back in March I still get inquiries about these opportunities rather than simply saying no to these opportunities and moving on I passed these leads to a list of AI consulting firms that I trust and out of the seven introductions I did like this one of them closed and that firm paid me a $11,000 referral fee the seventh way I made money was through buy me a coffee this is a platform that allows audiences to support creators although I don't advertise this much I made $533 3 from this the two main places that this is linked is the give a tip button at the bottom of my medium articles and the support button on the GitHub repo with all my example code eighth on the list was a book review which paid me $350 a couple of years ago I made a series of videos and blogs on causality which led to the opportunity to review a recent textbook called causal inference in Python basically I was given an advanc copy of the textbook and I shared my technical notes and suggestions to improve the text and the final income Source was a brand deal this consisted of putting a link in the description of how to make a custom Gmail signature video I was paid a flat fee and commission for anyone that purchased their product through that affiliate link I made a total of $233 42 for sharing that link in the description for 6 months plus commission on one purchase adding all these income sources together comes to a grand total of $24,000 64492 while this is nowhere near as much as I got paid as a data scientist it's also not a trivial amount of money which gives me just enough confidence to keep going I've learned a lot and made a lot of mistakes this past year that's why I want to end with three takeaways that would have been helpful to me before taking the leap first if you've got savings don't be afraid of zero income months I had about one year's worth of expenses in savings when leaving my full-time job however after just 3 months of focusing on content creation rather than making money I got ansy and reallocated my focus to contract work in hindsight this was a mistake I had a ton of Runway to experiment with riskier Ventures such as content creation or product development it was really too early to focus on safer Ventures such as freelance the second takeaway is that some things can only be learned after taking the leap while starting things like content creation and freelance early in my career were certainly beneficial many of the things I've learned this past year were only possible by committing to entrepreneurship full-time at the top of the list is learning how to stomach the emotional and financial uncertainty of this path which nothing else can prepare you for the third and final takeaway is that multiple income sources is not necessarily a good thing although having some income diversification is good having nine different Revenue sources like me is a sign that something's not working yet or I'm spreading myself too thin that's why I'm slowly learning to say no to Opportunities and focusing my efforts on content creation and product development that wraps up my first year as a full-time entrepreneur while entrepreneurship is not easy and not for everyone I hope this video provided at least some value to those pursuing a similar Journey if you have any questions feel free to drop them in the comments below and I'll happily share more about my experience and as always thank you so much for your time and thanks for watching"
ZVVkdXHqEuM,2024-09-23T15:45:12.000000,Knowledge Distillation Explained in 60 Seconds #deeplearning,knowledge distillation explained in 60 seconds knowledge distillation is a way to train machine learning models it works by taking a larger teacher model and using it to train a smaller student Model A simple way to do this is to generate synthetic data from the teacher model and use it to train the student this is exactly what Stanford researchers did in training their alpaca model which learned to respond to user prompts just like Chad GPT a more powerful but sophisticated way of doing this is using the teacher model's logits this provides a richer training signal to the student model than just a true false label if you want to learn more about knowledge distillation I share a code example on my YouTube channel
reXoKNC_Wx4,2024-09-20T18:15:44.000000,Quantization Explained in 60 Seconds #AI,here's quantization explained in 60 seconds may have heard of quantization in the context of efficient fine-tuning methods such as Q Laura it might sound like a scary and sophisticated word it's a very simple idea when you hear quantizing just think splitting a range of numbers into buckets for example there are infinite numbers between 0 and 100 there 27 55.3 83.7 823 and so on could quantize this infinite range by splitting it into buckets based on whole numbers that means 27 would get mapped to 27 55.3 would go to 55 and then 83.7 823 would go to 83 you can even go one step further split this range into even bigger buckets these numbers would go to 20 50 and 80 it's recently gained popularity in AI this is because it allows us to take massive AI models and Shrink their computational requirements by quantizing the model par
9joIFeKuf04,2024-09-16T14:01:44.000000,Python Explained in 60 Seconds #programming,here's python explained in 60 seconds python is a programming language in other words it's a way we can give precise instructions to computers the reason this is helpful is because we can use Python to automate almost any task this could be scraping websites analyzing data creating reports building chat Bots and so much more one of the reasons python has become so popular is because it is the go-to language for doing Ai and dat data science but it's also used in webdev finance game development and so many other places another upside of python is that it's a relatively highlevel programming language which means it's closer to English than the zeros and ones that computers understand this makes it easier for beginners to learn and use and now with tools like chat GPT everyone has access to a competent and patient coding tutor which makes it easier than ever to learn how to code
pNg2DJ4spXg,2024-09-12T16:36:12.000000,Python QuickStart for People Learning AI [Mini-Course],"python is the go-to programming language for doing Ai and data science although no code Solutions exist if you want to build fully custom AI projects and products learning python is still essential in this video I'm going to give a beginner-friendly quick start guide to learning python specifically for AI projects and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship if you enjoyed this content please consider subscribing that's a great no cost way you can support me in all the videos that I make this video is going to be broken up into three main sections first I'm going to start with a highle introduction of python and who this video is made for second I'm going to walk through the fundamentals and basics of python and then finally I'm going to share an example AI project showing how we can use Python to create a research paper summarizer and keyword extractor starting with the super basic question of what is python python is a programming language which is simply a way to give computers precise instructions to do things that we can't do or just don't want to do one example of this from my world is I'll send follow-up emails after Consulting calls with clients so the current process is we'll have the call I'll get on my laptop and type up a follow-up email and then I'll send the client the email however if this was something that started to get out of hand and I ended up spending hours every week writing follow-up emails I might create a python script that would do this instead of me typing up the email after the call I could create a python script to do this automatically and this is just one random example there are very few constraints on the things that you can do with python and AI another thing I want to highlight is that coding now is easier than ever two of the major reasons for that are Google and chat GPT so for example if I wanted to figure out how I can send emails automatically with python my first thing might be I just go to Google and type that into the search bar and then I'll get results that look like this so stack Overflow if you're not familiar is a very popular developer Forum where people are constantly sharing their questions and more experienced programmers are sharing answers the power of things like Google and stack Overflow is that it Taps into the wisdom of the crowd so key thing here is that no individual knows everything about python however if you get a collection of people together their Collective knowledge far exceeds any single individual and this is something that you can tap into using forums like stack Overflow or just generally browsing the web the second thing that's really become a game changer for a lot of developers is chat GPT and other tools like it another option here is instead of typing this into Google is you might type this same prompt into chat gbt so if you type this into chat gbt it'll give you a step-by-step answer as well as example code however that's not the only benefit of using chat gbt for coding questions one of the most powerful aspects of it that I really enjoy is the ability to ask follow-up questions so if there's something that doesn't really make sense here like what is the meaning of MIM or why is it called SMTP lib just these random questions that I might have and these might be questions if you asked a regular developer or program they might get annoyed at you because you're getting way too into the weeds or getting way too pedantic but Chachi BT is eternally patient and can answer almost any beginner level question with a high degree of accuracy I'm making this tutorial with a specific audience in mind namely people who are learning Ai and have done some coding but are brand new to python some examples of this might be someone who works as a consultant or even a business intelligence analyst another demo I have in mind are students or recent graduates but also seasoned developers who might just be new to python this is not meant to be a comprehensive introduction or overview into python rather the goal here is to Simply give you just enough to get you started coding your very first AI project and doing that as quickly as possible so with that let's just jump into the basics of python the first thing is that most computers come with python pre-installed so the first thing to do is to check if you have python installed on your machine if you're on Mac or Linux you can do this through the terminal app or if you're on Windows you can do this via command prompt or even Powershell I can show you what that looks like real quick here we have terminal opened up and I'm simply just going to type in Python if I do that we can see the python version that we're using and has some basic instructions with this we are in Python right now and we can go go ahead and start coding however if you type Python and this does not come up that means you're going to need to install python on your machine that's actually pretty straightforward if you are on Mac you can go to this link here if you're on Windows you can go to this link here and of course I'll share these in the description below but just seeing what that looks like this is the official python documentation and if you go to this python website link here it'll have a set of downloads for stable releases of python so if you're on Mac you can go ahead and click this thing and it will download an installer you can use to get python on your machine and a similar thing for Windows alternatively there's a popular python package system for data science called Anaconda this is available for Windows Mac and Linux so this is another way you can install Python and this has some handy features beyond the base installation of python now that we've confirmed we have python installed let's start digging into some key Concepts and code the first thing I want to talk about are data types which are simply a way to classify data so it can be processed appropriately and efficiently one example of this are strings which are character sequences which we can use to represent arbitrary text here are some examples of strings hello and hello spelled with numbers and special characters we can also go back to our terminal here and start typing strings so hello is an example of a string we can also write strings with single quotation marks so this is another way to write strings fundamentally there's no difference between using the double quotes and single quotes for Strings but what that does allow you to do is to use quotations in your strings themselves for example if we wanted to have the string Shaw said python is awesome the double quotation marks is going to cause an error however if we use the single quotes oh oops messed it up however if we use the single quotes we can have this be a string and the last thing I want to show here is we can also use triple quotes if we want to have strings across multiple lines and then you'll see that this new line character will appear in our string some other data types that we can use to represent numbers are ins which are integers and floats which are numbers that contain decimal points so going back to our terminal an example of an in is 1 2 42 you know just like in math floats would be anything with the decimal so 1.0 3.14 and then we can add two of the same data types together so we can add two integers we can add two floats we can even add an integer and a float and then python will automatically convert the integer into a float before doing the operation and we can even add strings together so here's an example of that so here we've added a string together another words we've concatenated it however we can add a number to a string so if we try that we'll get an error although ins and floats can be added together ins floats and strings cannot be added together if we wanted to do that we would have to write one as a string another fundamental data type are lists which are ordered collections of values some examples of that are shown here we can have a list of strings like a C we can have a list of integers like 1 2 3 we can also have a list with mix types so we could have a 2 and 3.14 and we can even have a list of lists this flexibility makes lists a very handy data type when doing AI in data science projects the final data type I'm going to talk about are dictionaries which are key value pair sequences what that looks like is a little different than a list so instead of the square brackets we have these curly brackets and then we have key value pairs where each item in the dictionary consists of two parts a key which will always be a string and a value which can be any data type and so a simple dictionary might have a single item with a key called name and a value of Shaw which is my name something a bit less trivial is to have a dictionary with multiple key value pairs corresponding to the name age and interests of a individual so here have the name as Shaw have age as 29 so you notice that this is an integer and then interests are organized in a list so we have multiple data types in this dictionary but we can also combine the list in dictionary data types so we can have a list of dictionaries here instead of just having a dictionary corresponding to one person we have two dictionaries corresponding to two different people so we have sha who's 29 interested in AI music and bread and then we have iffy who's 27 who's interested in marketing YouTube and shopping the last thing is we can also have a dictionary within a dictionary the way that looks is we might have a key value pair corresponding to a user and the value for this item will be another dictionary which includes user information then the other items in the dictionary might be the last login date and the membership tier of that user the next thing I want to talk about are variables which are abstract representations of underlying data so far we've just written data types explicitly as Sha or a set of numbers or a list or a dictionary if you had to continuously write out lists and dictionaries in your python code it would become very tedious very quickly this is where variables come in what we can do is instead of writing sha over and over over again in our code we might create a variable called username and set it equal to Shaw and then anytime we wanted to plug in the user's name we could just use the variable instead of writing it explicitly so if we printed the username variable we would see Shaw in the command line a more sophisticated example is we can Define two other variables one called the users age and another one called user interests which are a int and list respectively and then we can print those out in this way the output of this is Shaw is 29 years old his interests include AI music and bread not only does this save us from writing strings ins and lists over and over again in our Python scripts it also allows our scripts to be much more flexible because now let's say we wanted to print this sentence for another user so instead of just hardcoding this instead of writing this explicitly in our script we can simply plug in a new username a new user age and a new new user interests list another thing I want to call out here is here we're seeing yet another way to write a string so we saw we could write a string with single quotes double quotes and triple quotes here is an example of a formatted string this is helpful when you dynamically want to convert a set of data types to a string here we're seeing username which is a string user age which is an INT and user interests which is a list all dynamically concatenated together as a string and then we can print the resulting string like this these days where a lot of AI code might involve creating prompts and sending prompts to large language models formatted strings are a very handy functionality when creating prompt templates for these types of use cases so far we've covered data types and then creating abstract representations of these data types as variables now let's see how we can write Python scripts scripts are essentially text files with thep extension the simplest way to create a python script is to go to a basic text editor and instead of saving your text file with the txt extension you just change it to a py extension and it'll get saved as a python file which can then be understood by your python installation as being a program as opposed to a text file however this isn't always the most convenient way to write python script scripts rather it's common to use an IDE which stands for integrated development environment and all this is is a application that enables you to write Python scripts and really any other script for a programming language some very popular ones in the space are VSS code py charm and among data scientists Jupiter lab is very popular just to show you what that looks like here we have VSS code I'm going to open a folder I'm going to open this folder called python quickart which is freely available on the GitHub and so we see that there already some files here but I'm actually just going to create a new one so I'll create a new one and I'll call it python example and then I'll be sure to create it with thep extension and now we'll have this script here and we can start writing a python script writing a program here I'm going to create the ceremonial first program that every developer writes which is print hello world what this is going to do is if we go to our terminal I'm going to quit out of python I'm going to clear this I'm going to hit LS which allows us to see all the folders and files in the current directory and we see that our python example.py is sitting right here I'll type Python and then I'll copy paste this here we run that we see our program prints hello world that's the simplest python script that we might create and I used vs code to do that but you can of course use whatever IDE or application you like to create Python scripts another one that I've been playing around with recently is called cursor which kind of went viral on Twitter and the thing with cursor is that it's AI native it integrates large language models into the user experience to help you write code faster and of course there are other idees and co-pilots that do things like that as well next we're going to talk about loops and conditions at this point it'll probably be best if you're running these code examples to be typing these into a python script which you can run because these examples are going to run over multiple lines it won't be something that you can easily write and debug within the terminal itself and I can do that a little bit as we go along first let's talk about a loop and all a loop does is it runs a chunk of code multiple times one of the most popular Loops that you're going to C is a for Loop here's a very simple for Loop that's going to iterate over a sequence of numbers so Range Five creates a sequence of numbers of length five spanning 0 1 2 3 and four then what the for Loop does is that it's going to iterate along this sequence one element at a time then we're going to print each element as we go along so if we did this this is what would get printed out in the command line we can quickly see what this looks like with our VSS code script so let's just write 4 I in range five we'll make sure to put the colon there and then we'll just do print I and we'll save that then we'll open up our terminal again I'll hit clear and then I'm going to run that same command from before so Python and then python D example.py so if we print that we see our hello world from earlier but then we're also seeing each element in that range being printed out another example is we can iterate over items in a list let's say we have our user interests again of AI music and bread we're not just limited to iterating over numbers in a sequence we can also iterate over the elements themselves in lists if you're used to programming in c or one of the lower level languages this might seem like magic or something at least it was for me when I first started learning Python and this makes a lot of use cases pretty convenient when you can iterate over items themselves so if we run this script what's going to get printed out are each item in this list AI music and bread finally we can also iterate over items in a dictionary so if we create a dictionary and then here I'm going to iterate over the keys in this dictionary keys will extract each key from this dictionary and give us the ability to iterate over them so what this is going to be is a sequence of keys it's going to contain name age and interests so we're going to iterate over those and then we can access the values of the dictionary by using the key so here it's going to print the key itself like name and then we can print the value corresponding to that key using this syntax we have the user dictionary square brackets and then the key that we want to reference so if we run this this is the output that we'll get so we'll have name equals sha name is the first key sha is the value corresponding to that key then we have age and interests next let's talk about conditions which allow us to program logic with python a fundamental example of this are if else statements one way we can use this is if we wanted to check if the user is older than 18 for that we would simply write something like if the user's age here we're referencing the age value Val in the user dictionary here we're very intuitively checking if the value of the user's age is greater than or equal to 18 and then we have our colon here and then if this statement is true then python will execute the following command it will print user is an adult recall the value for the user age was 29 so this is true thus python should print this string here and indeed that's what happens alternatively we can also have an else not only will it do something if it's true if it's not true we can have python do another command here we're going to check if the user is older than 1,000 and if not we're going to print they have much to learn here the user's age is 29 which is not greater than or equal to 1,000 so instead of executing this command it's going to jump down to this else and print this and so this is what we'll see in the command line but we can also bring loops and conditions together so an example of this is if we wanted to count the number of users in a list that are interested in bread here we're going to define a user list which is a list of dictionaries and it's going to have two elements we'll have the first element being a dictionary corresponding to the user sha and then we'll have a second dictionary correspond responding to the user ify then we're going to initialize a count variable so we have this variable called count and we're going to set it equal to zero then what we're going to do is we're going to Loop through each element in this user list so we have two elements here so we'll start with this element and then go to the second element then for each user we're going to evaluate this condition we're going to see if bread is in the user's interest breaking this down for user one is going to be sha user is going to be a dictionary then we're going to extract the value corresponding to the interests key that's going to be this list here and then we're going to evaluate whether bread appears in that list in this first case bread indeed is in the list so Python's going to execute this line of code which takes the count variable which is currently zero it adds one to it and then it updates the count variable from being 0 to one and for people who are new to programming this may be unintuitive CU When we see equal sign we might think of math where the left hand side is supposed to equal the right hand side however equal here isn't saying that the left hand side is equal to the right hand side what it's saying is we're assigning this variable count equal to whatever is on the right hand side here so this is a very common way of updating a variable in a for Loop so we just did that for element one now we're going to go to element number two iy and we're going to run the same chunk of code so we'll evaluate the interests we'll see if Brad is in there and we can see that bread is not in iy's interests so nothing is going to happen it's not going to execute this line of code and it'll just continue so at the end of this we can print the count and what we'll see is that one user is interested in in bread and so you can already see through the very simple elements that we've talked about so far we've talked about lists strings integers dictionaries we've talked about for Loops we've talked about if El statements we've talked about printing values and just with these basic building blocks we're able to create this more sophisticated program that counts the number of users interested in bread next let's talk about functions these are operations we can perform on specific data types we've already seen some examples of functions one is the print function which simply prints the input to the command line here we have a for Loop that's going to iterate through all the keys in a dictionary and print the key and the corresponding value so here we're using user dict so if we recall it's just this dictionary here we run this script it's just going to print the key value pairs in this dictionary another example of a function is type which Returns the data type of an input variable here we're going to do a similar thing but instead of printing the value corresponding to the key we're going to print the type of the value if we do that what's going to Output is not the values themselves but the type of each value so name is a string age is an in in and interests is a list this is super handy because a lot of times you'll have these errors popping up in your code and simply printing variables and their types can often give you a better understanding of what's going on and help you debug the program another function is len which Returns the length of an input variable here we're going to print not the type of each value but the length of each value however when we run this we actually get a type error because this function Len is not defined for integers so although we got something for name the length of name was four because it consists of four characters Len is not defined for integers so it threw an error and as a bonus if we were to compute the length of interests we would get three because it consists of three elements Len is an example of a function that's only defined for specific data types so unlike print and type which are defined for every data type Len is not however there are several other examples of data type specific functions here's a set of functions defined specifically for Strings so let's say we have the string sha which lives in this dictionary here if we wanted to make all the characters in this string lowercase we could use this lower function that'll print the string like this we can similarly make every character uppercase and that'll print this we can also split strings to convert them into lists so we can split sha on H and a and what that'll create is a list with two elements corresponding to S and W another handy function for Strings is do replace which allows us to replace an arbitrary sequence of characters in a string with another so let's say we wanted to replace every W in this string with wh i n this will print shahen which which is my full name similarly there are a set of functions specifically for lists so let's say we have this list of interests here AI music and bread if we wanted to add an element to the end of this list we can use the append function if we did append entrepreneurship that would update the list to this if we wanted to remove a specific element from the list we can use the pop function and specify which element we want to remove so if we did zero that will remove the zeroth element of this list and return this finally if we wanted to insert an element into a specific place in the list we can use the do insert function specifying the element and where we want to insert that element so we did do insert one AI we would insert AI into the second place of this list and then finally we have a set of functions for dictionaries if we wanted to extract all the keys from a dictionary we can use the dot Keys function which is something we've already seen before so this will extract all the keys from a dictionary which we can iterate through using a for Loop for example we can do the same thing for values using the values function so now we see the values from the dictionary we can also access the items of a dictionary not just the keys or the values but the key value pairs that'll look like this and then if we print the items of the dictionary we see that name is no longer included then if we wanted to add a new item to a dictionary we can simply specify the new key name in square brackets of that dictionary and set that equal to a specific value so if we wanted to add name back in we could use this and then printing the items we can see that name has been added back into the dictionary however we're not just limited to Python's out of thebox functions we can also o create our own custom functions so these are called userdefined functions an example of this might be taking in a user dictionary and printing out all the users's information in a sentence so this is the same thing we did earlier using a formatted string however here we're going to define a function to do it you'll also notice that I Ed the triple quotes to create a string here that describes what the function does this is called a doc string and this is just a handy way to document what your function is doing so that other people can better understand your code and also helping your future self understand what this function is doing once we Define this we can use this function by passing in our user dict and it'll output this string which I call description which we can then print we print this we see it says sha is 29 years old and is interested in music however if we just wanted to do this once there's not a whole lot of upside in creating a custom function but if we wanted to generate this user description many many times functions are super helpful because now when we want to do the same thing for another user we simply pass in a new user dictionary to this user description function and print the result so if we want to do the same thing for iy we see it says iy is 27 years old and is interested in marketing next we can do something a bit more complicated and create a function to count the number of users interested in an arbitrary topic this will be pretty similar to what we did before but instead of just looking at the number of users interested in bread we'll have the topic of Interest be an input to the function here we'll initialize the count again as zero then we have our if condition within a for Loop so we'll Loop through an inputed user list we'll see if the inputed topic is in the user interests and if it is We'll add one to the running count then once we make make it through all the users in the list we'll return the count with this function we can define a user list so we'll have two users we'll have User Dictionary corresponding to Shaw and then we'll have new user dictionary corresponding to iffy and we'll select an arbitrary topic of shopping and then we'll compute the count using our userdefined function here then we'll print the count using a formatted string what the output of this will be is one user interested in shopping so far we've seen the power and flexibility of python and in fact python can be used to implement any arbitrary program but of course if we had to implement everything we wanted to do from scratch in Python this can easily become very timec consuming one of the key upsides of python is that there is a vibrant developer community and a robust set of open-source python libraries what this means is for virtually any anything you might want to implement in Python there's likely already an open-source library that exists for that so talking about the AI and data science space Here's a visualization of some handy python libraries of course this is a non-comprehensive overview but this is a set of libraries that will be helpful to any data scientist or AI engineer so under data frames which are convenient ways to structure data for analysis we have pandas and polers for data visualization we've got matplot lip Seaborn and plotly for basic machine learning we've got sklearn and XG boost for deep learning popular libraries are pytorch and tensorflow for more modern generative AI tasks there's the Transformers library and open AI API and then for web stuff for making API calls and formatting html text there's the request library and beautiful soup 4 respectively in order to install these libraries we can use pip which is Pyon 's default package manager so if we wanted to install say numpy which is a fundamental library for doing math in Python we can simply type pip install numpy at the command line with numpy installed we can do several things which are not possible with basic python the first is we can create this array data type which mimics a vector or a matrix from linear algebra so if we want to create a one-dimensional matrix we could do it like this if we wanted to multiply that matrix by two we could could simply multiply by two if we want to create a two-dimensional Matrix we can concatenate together multiple one-dimensional matrices here we see a 3X3 Matrix and then if we wanted to do matrix multiplication we can use the Matt mole function in numpy and so this will automatically do matrix multiplication for us and of course there's several other things that you can do in numpy which you can find in the example code on the GitHub repository and the last thing I'll say about libraries is creating virtual ual environments when using open- Source python libraries in your projects every project is going to have a set of dependencies a set of libraries that it needs in order to execute successfully for these types of projects it's a best practice to create a virtual environment all this does is keep track of all the libraries and their versions needed for your project to do that we do python dmvnv this is saying that we want to create create a new python virtual environment and then we can give it any name we like so here I called it my- EnV then we can activate this virtual environment with mac and Linux you can type Source my- EnV /bin activate or if you're in Windows you can just run this batch file like this then with that virtual environment activated you can install all the libraries you like using pip now let's get into the example AI project so up until this point we've just covered the basics of python we haven't done anything that could be considered AI so let's see what implementing a simple AI project might look like with python here what I'm going to do is write a program that can summarize and extract keywords from research papers this example along with all the other code Snippets we've seen so far in this tutorial are freely available at the GitHub repository linked down here and in the description below our first step is going to be to install all the required libraries for this example just like before where we did pip install and had a specific package name another way we can install libraries is from a text file that lists several libraries that are necessary for a project here I've created a requirements. text file available at the GitHub which lists all the libraries we can actually look at that here so I've opened it up in vs code and we can see that it lists several Library names along with their versions so when we run pip install dasr requirements what pip does is it goes line by line through this text file and installs each of these libraries and the correct version so we can actually see what that looks like real quick so we can do python DMV EnV and I'll call it my environment like before and then since I'm using Mac I can write Source my so whatever virtual environment name you chose bin SL activate now you can see that our terminal looks kind of different we have this my EnV sitting here what we can do next is to install all the requirements we see that I have the requirements file sitting right here in the current directory so I can install all of them like this so I'll do pip install Dash R and then I'll put the name of the requirements file.txt I'll run that then pip will go ahead and just automatically install all the project libraries then we can double check that by doing pip list so this will list all the packages and we can see that everything has indeed been installed next we can go into our python script and import the necessary libraries we're going to import fits which will allow us to extract text from PDFs we're going to use the open AI python API and then we're going to import Cy finally I will import my open AI API key here I have a separate python script called sk. piy and what I'm doing is importing a variable called myor SK myor SK is a string which contains my open AI API key I'm importing it in this way just for security reasons I don't want to hardcode my API key in the example code that's just not a good practice because if you forget that you put your API key in a script and you share it with someone or put it up on GitHub then other people will be able to use that API key and they might rack up a bill that you'd be responsible for just to see what this sk. py file looks like so if we go here I don't actually have my API key here because this is the version on GitHub but what you can do is just copy paste your API key into this python file and it should work fine alternatively if you're just running this locally and you want to do something fast you can just skip this step and just hardcode your API key but of course just be careful that you're not sharing that file with anyone we'll import this variable from this sk. piy script and then we'll set our open a. API key to my secret key next we're going to define a function that will read in the text from a PDF this function is actually kind of long so I'm going to break it down into three parts first I'm going to Define our function that's called extract abstract and if you're not familiar an abstract is like a summary paragraph of a research paper that is at the beginning of that paper extract abstract will take in one input which will be a string specifying the path of the PDF that you want to extract the text from given that PDF path we're going to grab the text from the first page of that PDF file the way we do that with this fits library is we do fits. openen PDF path as PDF basically what this is going to do is create this PDF object that allows us to access the text in the PDF so here we're going to get the first page of the PDF like this so we're grabbing the zeroth element from this PDF object and that'll correspond to the first page and then we can specifically grab the text from this first page using this function here. get text and then we'll just say text here now all the text from the first page is represented by this text variable next what we can do is extract the abstract from this text so text is just a string going to be a bunch of characters so what we want is to find where in that string the abstract lives the way I'm doing it here is that I'm going to find a start index and an end index basically at what element does the abstract start in the string and then at what element does the abstract end here what I do is I take all the text I make it all lowercase and then I do this find function so what this find function is going to do is going to find the word abstract in this text and then it's going to return the index number corresponding to where abstract is in this larger piece of text then we don't just want the start position we also want the end position of the abstract so this one's a bit more tricky because the PDF could come in many different formats could just have the abstract on the first page it could also have the abstract on the first page and then go into the introduction or many other possible situations so here what I do is if the word introduction exists in the text what I do is I find where introduction is then I set that as the end index however if introduction is not in the text I'll just set the end index as as none so basically we'll only care about where the abstract starts and then we'll just consider the text until the end of the first page what I do here is I grab the text starting from our start index all the way to the end index and then I use this strip function which will remove any trailing or leading white space from the text and then there's one more step because there's always the situation where the word abstract does not appear on the first page so we need to account for that basically what we're doing here is we're checking if the start index exists so instead of checking whether start index is equal to minus1 this is checking if start index is not equal to minus1 so if it's not equal to minus1 that indicates that abstract is on the first page of the PDF and so we can move forward with the abstract as we have it however if start index is equal to ne1 that means the abstract does not appear on the first page so we'll actually return none that's our extract abstract function next we're going to extract a summary and keywords given in abstract so we're defining another function here it'll take in the abstract we'll create a prompt using a formatted string The Prompt that we're going to send to gbt 40 mini is summarize the following paper abstract and generate no more than five keywords and then we're going to dynamically insert the abstract into the prompt then we're going to make our API I call here we're using open ai's chat completions API we can specify the model we want to use which here we use GPT 40 mini which is the cheapest and fastest model open AI has and then we'll send the model our prompt openi API has the ability to create system messages as well as user messages here we just have the system message as you are a helpful assistant and then the user message so this would be as if we went to chat gbt and type something in will be the prompt that we def find up here and then we set the temperature to 0.25 so loosely speaking temperature essentially sets the randomness of the model's responses a very low temperature means that the response will be very predictable and a very high temperature means that the response will be very unpredictable we'll make this API call and this is essentially like us typing this prompt into chat GPT and then getting a response we can extract the response like this the response will have a lot of different components to it but here we're going to extract the first choice we're going to extract the message from that First Choice and then we're going to extract the content from that message and then we'll return that as our summary and keywords finally we can bring everything together and so here we're going to grab the PDF path from the command line what this enables is that the user can type in the path of the PDF they want to summarize on the command line and then python will grab that text and store it as this PDF path variable and then we can pass the PDF path variable to our extract abstract function so this will return the abstract for us then we can use our summarize and generate keywords function to generate the summary however if the abstract is a nun type so basically the abstract did not exist on the first page of the PDF python won't execute this chunk of code and instead we'll jump to the else bit and print abstract not found on first page that's the end of the script once we've written that we can run that from the command line so we'll do python summarize DP paper. py so that's what I called the script in this case and then here is the command line argument that we're going to pass to this python script this is our PDF path so I have this files folder and in that folder I have a PDF called attention is all you need which is a PDF of the famous Transformers paper and then if we run this this is the output we get at the command line so we get a summary of the attention is all you need paper and then we get a set of keywords courtesy of gp4 many here we covered a ton of information however there's still so much to explore when it comes to Python and Ai and I find the best way to navigate this ocean of information is to learn by doing in other words by building your your own AI project in order to do that here I'm going to share three tips first and foremost use Google and chat GPT generously I don't know a single programmer who does not use Google or chat GPT or some combination as a sidekick any time that they're coding something up and it's simply because that a lot of times you run into error messages that just don't make sense and this is especially true at the beginning of your python Journey where you just don't have a lot of experience these tools are just great ways to help you accelerate your learning and progress next is figuring it out is a key skill that you need to develop as a programmer what this comes down to is just having the right mindset that even though something doesn't make sense to me right now I'm capable of figuring it out instead of saying things like I don't know how that works change your mindset to I don't know how that works yet so often we are just one Google search or one explanation away from unlocking something that doesn't make sense and this is something you'll run into over and over again as You Learn Python finally take the example code that I shared here and hack it to create your first AI project now that we have these large language models which are capable of solving so many problems and accomplishing a wide range of tasks integrating them into your Python scripts enable you to offload a large amount of software development and logic to the large language model which allows you to move a lot faster if you enjoyed this video but you want to learn more check out the blog and towards data science there I covered details I probably missed here and although this is a member only story you can access it completely for free using the friend Link in the description below I'll also call out the GitHub repository which contains all the example code that we saw in this tutorial and as always thank you so much for your time and thanks for watching"
FLkUOkeMd5M,2024-09-01T01:53:13.000000,Compressing Large Language Models (LLMs) | w/ Python Code,large language models have demonstrated impressive performance across a wide range of use cases while this is largely due to their immense scale deploying these massive models to solve real world problems can be challenging in this video I'm going to discuss how we can overcome these challenges by compressing llms I'll start with a highlevel overview of key Concepts and then dive into a specific example with python code and if you're new here welcome I'm I make videos about data science and Entrepreneurship if you enjoy this content please consider subscribing that's a great no cost way you can support me in all the videos that I make last year the Mantra and AI seemed to be bigger is better where the equation for creating better models was more data plus more parameters plus more compute and we can see over time large language models just kept getting larger and larger this is a figure from reference 11 down here and we can see over time models kept getting bigger and bigger so in 2018 large meant something around 100 million parameters in 2019 with gpt2 we were up to 1 billion parameters then came gpt3 which was around 100 billion parameters then we have more recent language models which have a trillion parameters or more there's no doubt that this equation actually works GPT 4 is objectively better than gpt3 and everything that came before it however there's a problem with creating bigger and bigger models simply put bigger models come with higher costs so just to put this into computational terms a 100 billion parameter model is going to take up 200 GB of storage and then if you want to use this model you got to fit this massive thing into the memory of your machine so needless to say this comes with high compute costs so it's probably not something that'll run on your laptop you're going to need a lot more compute than that it comes with higher Financial costs and then of course it comes with a higher environmental cost but what if there was a way we could make these Large Scale Models a lot smaller this is the motivation of model compression which aims to reduce the size of a machine learning model without sacrificing its performance so if we're able to pull this off by taking a massive model and shrinking it down to a smaller model this means we could run one of these models on our laptop or even on other devices like cell phones or SmartWatches or other types of devices not only does this Foster greater accessibility for this technology it also promotes user privacy because these models can be run on device and user information does not need to be sent to a remote server for inference this also means less Financial cost and of course the negative environmental impact can be a lot smaller here I'm going to talk about three different ways we can compress these models the first is quantization the second is pruning and the third approach is called knowledge distillation starting with quantization although this might sound like a scary and sophisticated word it's a very simple idea quantization consists of lowering the Precision of model parameters and you can think of this like taking a high resolution photo and converting it to a lower resolution one that still captures the main features of the image and to put this into computational terms you might take a model whose model parameters are represented in fp32 so using 32 bits and translating the parameters into int 8 just to get a sense of this the number seven represented in fp32 looks something like this so the computer has to keep track of all these binary digits just to encode a single parameter however if that same parameter is represented in in eight that'll look something like this it's a fourth of the memory footprint if you want more details on how quantization Works under the hood I talked more about it in a previous video of this series on Q Laura I'll link that here for those who are interested when it comes to implementing quantization on large language models there are two popular categories of approaches the first is called post trining quantization and the basic idea here is you train your model and then quantize it the key upside of this is that this allows you to take models that other people have trained and then you can take them and quantize them without any additional training or data curation using this approach you can take these off-the-shelf models that might be encoded in fp32 and convert the parameters to 8bit or even 4bit however if you want to compress Beyond 4bit post-training quantization typically leads to a degradation in model performance for situations where even more compression is needed we can turn to another set of approaches known as quantization aware training this essentially flips the order where one quantizes the model first and then trains IT training models in lower Precision is a powerful way to get compact models that still have good performance this means parameters can be encoded with even fewer than four bits for example in reference number six the authors were able to create a one bit model that mat matched the performance of the original llama model but of course the downside of quantization aware training is that it is significantly more sophisticated than post-training quantization because one has to train the quantized model from scratch the second compression approach is pruning which consists of removing unnecessary components from a model so an analogy here is that pruning is like clipping off dead branches from a tree it reduces the tree's size without harming it to put this in terms of parameters we might have a 100 billion parameter model but then through pruning we might reduce it down to 60 billion parameters while there are a wide range of pruning approaches out there they can be broadly classified into two categories the first is called unstructured pruning which consists of removing individual weights or parameters from the model showing that visually if this is our original model here unstructured pruning might consist of zeroing out the these two weights in the model the key benefit of unstructured pruning is that since it's operating on this granular scale of individual weights it can result in a significant reduction of non-trivial model parameters however there's a key caveat here since we're just taking model weights and turning them into zeros this is going to result in sparse Matrix operations to get predictions from our model in other words The Matrix multiplications involved in generating a prediction from our model will consist of a lot of zeros and this isn't something that normal Hardware can do any faster than nonsparse Matrix operations which means that one needs specialized Hardware that's designed to optimize these sparse Matrix operations in order to realize the benefits of unstructured pruning on the other hand we have structured pruning which instead of removing individual weights it removes entire structures from the model this can be think things like attention heads neurons or even entire layers so visually what this might look like is if this is our original model we might remove this entire neuron from the model which does not result in the sparse Matrix operations that we see in unstructured pruning while this does result in less opportunities for model reduction it allows one to completely remove parameters from the model if you want to explore specific unstructured and structured pruning techniques check out out reference number five which provides a nice survey of these approaches the final way we can compress an llm is via knowledge distillation this is where we transfer Knowledge from a larger model into a smaller one this is just like how we all learn at school where we have a teacher who has much more experience in a particular subject transferring their knowledge to the students in the context of large language models the teacher model might have a 100 billion parameters which are then distilled to a student model which might have just 50 billion parameters again there are a couple of ways that we can achieve this the first is using soft targets which consists of training the student model using the logits from the teacher model what that means is let's say we have our teacher model here and let's say it performs sentiment analysis so given a chunk of text it'll label that text as either positive sentiment or negative sentiment the way these model models work is that the raw outputs are not just a positive or negative prediction but rather there's a prediction for each class known as a logit for example let's say the logit for the positive class is 0.85 and the logit for the negative class is minus 0.85 what this is indicating is that the input text is more likely to be positive sentiment than negative sentiment and this is exactly how text generation models like l 3.1 or gp4 work under the hood however instead of having two output logits these models will have tens of thousands of output lits corresponding to each token in its vocabulary these lits are then converted into probabilities and then these probabilities can be sampled to generate text one token at a time so we can actually use these loits to do knowledge distillation so the way that works is we'll take our smaller student model have it generate predictions and then we'll compare those predictions to the teacher model's predictions for the same input text and the reason these are called Soft targets is because the predictions of the student model aren't compared to a zero or one ground truth but rather a softer fuzzier probability distribution this turns out to be an effective strategy because using all the output logits from the teacher model provides richer information to the student model to learn from another another way to achieve knowledge distillation is instead of using logits to train the student model one can use synthetic data generated by the teacher model a popular example of this was the alpaca model which took synthetic data generated from the original chat GPT and used it to perform instruction tuning on llama 7B in other words chat GPT was used to generate these input output pairs of input prompts from users and out put responses from the model which were then used to endow this llama 7B model with the ability to follow instructions and follow user prompts so now with a basic understanding of the key Concepts behind model compression let's see what this looks like in code as always the example code here is freely available on the GitHub additionally all the models derived here and the data set used for training is also freely available on the hugging face Hub and we'll be using python for this example as well as pytorch the example here is we're going to take a text classifier and compress it using knowledge distillation and quantization so that's one thing I actually forgot to mention is that these three different compression approaches of quantization pruning and knowledge distillation are often independent of one another which means that we can combine multiple approaches to achieve maximum model compression so here I'm going to combine knowledge distillation with quantization to achieve a 7x reduction in model size the first step here is to do some imports many of these are hugging face libraries so data sets is from hugging face we'll import some things from Transformers we're going to import some things from pytorch and then finally I'm going to import some evaluation metrics from psyit learn then I'll import the data set with one line of code and this is something I've made available on the hugging face Hub it consists of a training testing and validation data set with a 70515 split so it's 2100 examples of data in the training data set and then 450 examples in the testing and validation and the data set consists of two columns the First Column are website URLs and the second column is a binary label of whether that URL is a fishing website or not a fishing website so this is actually a very practical use case used by email providers or cyber security folks that may want to ensure that links are safe before presenting them to end users with the data loaded in we'll load in our teacher model here to speed things up I used the freely available GPU on Google collab so I'm importing that GPU as a device here next I'm going to load in the teacher model which is a model I ftuned on This fishing classification task we can load in the model's tokenizer and the model itself using these two lines of code here and then using this two method I'm loading the model onto the GPU then we can load in our student model so here we're going to create a model from scratch I'm going to copy the architecture of distill bir to initialize the model however I'm going to drop four of the attention heads from each layer additionally I'm going to drop two of the layers from the model in other words each attention layer has 12 attention heads so I'm going to reduce that to eight and the original architecture has six layers and I'm going to reduce that down to four then I'm going to use this distill BT for sequence classification object what that does is it'll load in this distill BT architecture with these modifications and then slap on top of it a classification head in other words the model instead of generating text is going to perform text classification we're also going to load the student model onto the GPU and just to get a sense of the scale here the teacher model has 109 million parameters and takes up 438 megab of memory while the student model here here consists of 52.8 million parameters and takes up 211 MB of memory the reason I'm using relatively small models by today's standard is that this is what I can easily run on the free GPU on collab but if you have beefier gpus or more compute at your disposal you can take this code and just plug in bigger models and it should work just fine so the data set that we loaded in consists of plain text with a label so before we can actually use the this data we'll need to tokenize it here I defined a simple pre-processing strategy so what's happening here is each URL is being converted into a sequence of Tokens The Tokens are being truncated so they're not too long and then within each batch of examples the shorter sequences are going to be padded so all the examples have the same length and this is important so we can convert it into a pytorch tensor and efficiently do the computation with the gpus this the pre-processing function the the actual transformation happens in this line of code so we take the data set and then we map it into tokenized Data making sure that we are using batches and then we're converting it into a pytorch format where we have columns for the tokens the attention mask and the target labels another thing we need to do is Define an evaluation function so this will allow us to compute evaluation metrics during model training and so there's a lot happening here so I'm going to go through it line by line first we're putting the model into eval mode instead of training mode we're initializing two lists one list is for model predictions another list is for labels here we're going to disable gradient calculations then batch by batch we're going to do the following first we're going to load all the data onto the GPU so that's the input tokens the attention mask and the labels then we're going to perform the forward pass so we're going to compute model outputs and then we're going to extract the logits from the outputs this lits variable here will actually consist of two numbers one one corresponding to the probability that the URL is fishing and another corresponding to the probability that the URL is not fishing so in order to turn this into a binary classification in other words the URL is fishing or is not fishing we can take the ARG Max of this logits variable and then that'll be our prediction and then we can append the predictions and the ground truth label to these lists we initialized earlier once we do that for all the batches we can compute the accuracy precision recall and F1 score for all the data in one go next we're going to define a custom loss function and the way we're going to do that is we're going to use both soft Targets in other words the logits from the teacher model and the ground truth labels and so the way we're doing that here is we're going to compute a distillation loss as well as a hard loss and then we're going to combine those into a final loss so to get the distillation loss we'll first compute the soft targets so these are the teachers logits and then we're going to convert those logits into probabilities in order to generate probabilities from the teacher models logits we can use the soft Max function and it's common practice to divide the teacher logits by a temperature parameter which will increase the entropy of the probability distribution so we generate a probability distribution corresponding to the teacher's prediction and then a probability distribution corresponding to the students prediction and now that we have two probability distributions one from the teacher model one from the student model we can compare their differences using the KL Divergence pytorch has a built-in method that does that so we can just easily compute the difference between these two probability distributions using this line of code here and then we can compute the hard loss so instead of comparing the student model's predictions to the teacher model's predictions we're going to compare them to the ground truth label and then we'll use the cross entropy loss to compare those probabilities distributions and then finally we can combine these losses by adding them together and adding this Alpha parameter which controls how much weight we're giving to the distillation loss versus the hard loss next we'll Define the hyperparameter so here I use a badge size of 32 put the learning rate as .001 we'll do five Epoch we'll set the temperature that we use in our loss function at two and then we'll set the alpha so the relative weights of the distillation loss versus the hard loss as 0.5 so we'll give equal weight to both types of losses then we'll Define our Optimizer so we'll use atom then we'll create two data loaders we'll have a data loader to control the flow of batches for the training data as well as the testing data then we'll train the model using pytorch so we put the student model into train mode and then train it we have two for Loops here so we have one for the epoch one for the batches and it's a similar thing as to what we saw in the evaluation function so we'll load each batch onto the GPU we'll compute the outputs of the teacher model and then since we're not training the teacher model there's no need to calculate gradients so we can avoid that using this syntax here then we'll pass through the student model to generate its outputs and extract its logits we'll compute the loss value using our distillation loss that we defined earlier and then we'll perform the back propagation sorry the script was too long so I have to extend it like this but once we make it through every single batch we can print the performance metric tricks after each Epoch so we'll print the accuracy precision recall F1 score for the teacher model and then the accuracy precision recall F1 score for the student model and then we'll be sure to put the student model back into train mode because this evaluate model function that we defined earlier puts it into eval mode so I know this was a ton of code maybe way more code than you were hoping to get into but here are the results of the training so we have five Epoch here and we can see the loss is going down which is a good sign so it bumped up in Epoch 4 but then it dropped back down in Epoch 5 which is very normal and then we can compare the performance of the teacher and student models so of course since we're not updating the teacher model its accuracy is going to stay the same across all Epoch cuz it's not changing but we can see the student model performance get better and better across each Epoch and then once we get to Epoch number five the student model is actually performing better than the teacher across all evaluation metrics next we can evaluate the performance of the teacher and student models using the independent validation data set so the training set is used to update model parameters the testing data set is used in tuning the hyperparameters of the model and the validation set wasn't touched so this will give us a fair evaluation of each model and for that we again see that the student model is performing better than the teacher model across all evaluation metrics this is one of the other upsides of model compression if your base model if your teacher model is overparameterized meaning that it has way too many internal parameters relative to the task that it's trying to achieve actually compressing the model not only reduces the memory footprint but also it can lead to better performance because it's removing a lot of the noisy and redundant structures within the model but we can go one step further so we did knowledge distillation let's see how we can quantize this model first I'll push the student model to the hugging face Hub and then we'll load it back in using the bits and bytes integration in the Transformers Library so we'll use the bits and bytes config so we'll load it in for bit we'll use the normal float data type described in the Cur paper and all this is is a clever way of doing the quantization to take advantage that model parameters tend to be normally distributed so you can be a bit more clever in how you quantize the values and I talk more about that in the Cur video that I mentioned earlier next we'll set the compute data type as brain float 16 and then finally we'll do double quantization which is another thing described in the Cur paper once we set up this config we can simply just load in our student model from the hugging face hub using this config file so the result of that is we have still the same number of parameters 52.8 million but we've reduced the memory footprint so we went from 21 megab down to 62.7 megab then comparing that to our original teacher model we started with we cut the number of model parameters in half and then we reduced the memory footprint by about 7x but we're not done yet so just cuz we reduced the model size that doesn't mean that we still maintain the performance so now let's evaluate the performance of the quantize model here we see we actually get another performance game post quantization so intuitively we can understand this through the aam's razor principle which says that simpler models are better so this might be indicating that there's even more opportunity in knowledge distillation for this specific task all right so that brings us to the end if you enjoyed this video and you want to learn more check out the blog in towards data science and although this is a member only story like all my other videos you can access it completely for free using the friend Link in the description below additionally if you enjoyed this video you may enjoy the other videos in my llm series and you can check those out by clicking on on the playlist linked here and as always thank you so much for your time and thanks for watching
kmrekqjWE8o,2024-08-26T14:15:08.000000,Why You Should “Keep Going” #entrepreneurship,you often hear entrepreneurs say things like keep going and just don't quit here's statistically why this is good advice this plot shows survival rates of businesses established in 1994 all the way up to 2003 it demonstrates the low odds of entrepreneurial success with 50% of businesses closing within 5 years however something interesting happens when we look at this same curve for businesses that have been around for a few years here we see the curve incrementally moving up the longer the business stays alive in other words the probability of a business's survival increases the longer it survives for example although only 50% of new businesses make it to year five these odds jump to 57% just after one year in business so this is a reminder to all the entrepreneurs out there keep going each day that you survive your odds of success go up
3JsgtpX_rpU,2024-08-16T14:02:17.000000,3 AI Use Cases (that are not a chatbot),"the roles that have the highest score are it professionals it seemed like it professionals was a promising customer avatar for these workshops we have Consultants they seem like a promising customer here and then finally data analyst seems like a promising Avatar these actually were the three avatars that seem to be most interested in this type of Workshop large language models have taken over the business world and all the big companies are trying to use generative AI although tools like chat GPT are clear cly powerful what's not so clear is how businesses can use this technology to drive value in this video I'm going to talk about three ways to use AI for something that every business cares about sales I'll talk about each use case at a high level and then dive into example python code using real world data and if you're new here welcome I'm Shaw I'm a data scientist turned entrepreneur and if you enjoy this content please consider subscribing that's a great no cost way you can support me and all the content that I make for most businesses that I've interacted with using AI typically means building a chatbot a co-pilot an AI assistant or agent while these are great solutions to some problems they are far from a cure all a key challenge is that large language models are inherently unpredictable getting them to solve a particular problem in a predictable way often comes at a major cost for example popular tools like chat GPT and GitHub co-pilot were notoriously losing money in 2023 however businesses can use AI for more than just building chatbots and the like here I'm going to talk about three AI use cases that are not a chatbot these are all going to be in the context of sales which is something that every business has to do the first use case is data augmentation the second is structuring unstructured data and the third one is lead scoring I'm going to talk through each of these use cases one by one and then dive into some example python code implementing these use cases on real world data from my business data augmentation consists of adding data to your data set this can look two different ways you can either add more variables to your data set so here more columns or you can add new rows or examples and the reason this is powerful is because businesses can use data to solve problems and make decisions for example a popular business are data vendors who sell data to companies to help them make decisions one of the most popular data vendors that we all know is FICO they generate these credit scores for basically every potential lender and then sell them to financial services companies that are writing loans for people another reason data augmentation is valuable to businesses is because it can help augment their machine learning and data analytics efforts an example of that in a sales context is let's say we have have a list of names and a resume for each person on that list if we wanted to do some kind of analysis on these resumés we might want to extract key information such as years of experience or industry from each of these resumés there are a few ways we could go about this so one way is we can just read every resume and manually write out the years of experience or the industry of that person but of course this would be very tedious and if you have thousands or tens of thousands of resumés this is not something that's practical another idea is to try to automate this process with some kind of computer program but if you have resumés coming from many different sources this also may not be feasible because people format their resumés in many different ways so this is one area that large language models can help because they can better comprehend a body of text like a resume to extract things like years of experience or industry more readily from text another key AI use case that a lot of people are talking about is structuring unstructured data so what does that mean structured data is simply data that can be represented by rows and columns typically consisting of numbers on the other hand unstructured data cannot fit into a table examples of this are text documents PDFs audio files images these type of data are not organized in a table the reason that this distinction is important is because when it comes to using data it's much easier to extract insights from it and do analysis is on it when it's in a structured format while if it's unstructured you're going to have to do some extra steps before you can do any kind of analysis on that data one of the exciting things we're seeing these days with large language models is the Improvement of the so-called text embeddings and if you're not familiar text embeddings I got a whole video on it so if you want to dive into the details you can check out that video but at a high level all a text embedding does is it takes a chunk of text and translates it into a meaningful set of numbers so basically what this means is we can take a stack of résumés or any type of text and organize it into a structured format into a table that we can readily analyze and then the final use case I'm going to talk about is lead scoring essentially what this consists of is taking customer data and from that information generating a prediction of How likely it is that that customer will buy your product or service what this typically looks like is you'll take data such as job title the revenue of the company that they work at customers's Behavior how much they interacted with your website or social media platforms or something like that and then where the lead came from as well as many other potential indicators and all this information is synthesized into a single metric that represents How likely it is that that customer will buy your product while lead scoring has been around for a very long time it's not something new that came around with large language models however the opportunity that large language models have presented is not so much in generating the score itself but rather improving the inputs that we can pass into lead scoring models now I want to bring together all three of these use cases and apply them to a case study from my business so I've recently been trying to validate doing AI workshops as a new Revenue source for my business my first step in validating this idea was reaching out to a 100 people on LinkedIn the Outreach script look something like this I'd say hey I'd add some personalization and then just ask for their feedback on doing a AI workshop and gave them some details so from these 100 DMS 58 people responded which is pretty good 18 people said yes they would attend and 19 people said no they wouldn't attend then everyone else who responded is basically a maybe the goal of this initial Outreach is to identify promising customer avatars so what I did here is I sent out DMS to a wide range of people of all different job titles and backgrounds and then did an analysis trying to find the common factors between the people that responded said yes and said no here I'm going to walk through the code of this analysis which involves the three use cases we saw earlier and as always the code is freely available at the GitHub repository however I can't share the data because I was analyzing people's resumés and don't want to share that publicly due to privacy concerns okay so here we have the example code for the first use case we start as always by importing some helpful python libraries I'm using polers to organize the data in data frames I'm using the open AI python API to do the data augmentation and then finally I import numpy the first step here is loading the data there was actually a Step Zero which consisted of extracting text from people's resumés where I got the resumés was actually from LinkedIn so if you go to anyone's LinkedIn profile and you go to more and then you do save to PDF what will happen is LinkedIn will aut automatically generate a resume of that person which will look something like this for everyone that I reached out to I went and manually downloaded this resume and then I have this python script here which will actually crop the resume to only keep this white area and extract the text from it that's what this code does it extracts the text and then it saves it in a polar data frame consisting of two columns one corresponding to the name and then the second Corr responding to the text from the resume so that's what we're importing here I saved it to a CSV file and so now I'm reading it in as a data frame at this point the only two variables in our data frame are the names and the resumés so let's see what it would look like to try to extract the years of experience from the text in people's resumés here I'm going to use the open AI python API to do this that'll involve us defining two different prompts so I have this system prompt which will be used in open ai's chat completions API this is the system prompt you are a resumé analysis assistant your task is to classify RS into one of the following experience level buckets based on the number of years of professional experience listed in the resume and then a defines these five buckets and then it gives some more instructions so I actually generated this system prompt using chat GPT I just asked it to create me a system prompt for chat GPT and I think that tends to be a pretty good starting point for prompts because Lar language models tend to think differently than we think so it's better to have data generated from a large language model if you're going to pass it in to another large language model typically just a rule of thumb but we also need a user prompt that we're going to pass into the model the way of implemented that is using this prompt template so this is essentially a function that takes in the text of a resume and then dynamically inserts that text into this prompt then we can just pass this whole thing to the open aai API to generate a response what this prompt is asking is to take the text from this resume and then output either 1 2 3 4 or five based on the levels of experience so once we have that we can pass each resume over to the open AI API to generate a response to basically classify each person's level of professional experience and so the way that looks is very similar to what we saw in the AI assistance video so if you haven't checked that out I'll link it on the screen here basically what's happening is we're going through the data frame one row at a time we are extracting the text from the resume passing it into the prompt template and defining that as our prompt and then what we do is we pass the system prompt and the user prompt both to GPT 40 there are also a few arguments that we can specify which will improve the responses from the model so here I just want the model to respond with a single number however large language models like I said earlier are inherently unpredictable you'll ask it to do a very straightforward task but then it'll have some weird formatting maybe instead of just returning the number it'll first try to put experience level colon and then the number in its completion so that's a very reasonable thing to do is just trying to format the text in a nice way but if we're trying to parse the responses from the model in a predictable way that's going to cause some problems the way I control for that is I just set the max number of token that the model can respond with to one which will hopefully correspond to a number between 1 and five I set the number of completions as one so this will only generate one completion but you could also have it generate multiple completions and then you can pick the completion that was returned most frequently and then finally I set the temperature at 0.1 and temperature essentially increases the randomness of the responses so a very high temperature will generate completions that don't make any sense and then a very very low temperature will generate completions that are very predictable so what I do is for each resume I generate the completion and then store it in this experience level list it's also worth calling out that there are multiple models we can use here I use GPT 40 That's apparently the most intelligent large language model open AI has I also tried GPT 40 mini which was significantly faster and it's about 20 times cheaper we can see this here I think I ran it twice with GPT 40 mini which cost a total of 4 cents but running it twice with GPT 40 cost 1.20 so it's about 30 times more expensive to use GPT 40 than GPT 40 mini so which makes the most sense will just depend on your use case in an earlier version of this I used GPT 40 mini but instead of just generating one response I generated three responses although I didn't do a proper comparison of the two I feel that the result of running GPD 40 mini multiple times was similar to running gp40 just once for a small use case like this it's not super important which is better GPT 40 mini done three times versus GPT 40 just run once but you can imagine if you're working with a lot more data and it's a difference of $6,000 versus $60,000 then that's something that'll make a huge difference okay so this experience level list all the elements are strings so I just convert them to integers and store them in a numpy array and then they look like this this and then I add this numpy array as a new column in the original data frame this is what it looks like there people's names here so I have to blur it out but now we went from just having name and resume to having name resume and experience level but of course we could have done other things like job title industry and other pieces of information that are obvious by looking at someone's resume but if you have a th000 or 10,000 resume is not viable for a human to go through and read everything you can just pass it off to a capable large language model to extract that information dynamically use case number two structuring unstructured data although extracting data from text in the way that we just did is sort of structuring unstructured data using text embeddings is a lot cheaper and captures much more information from the underlying text here I'm going to do some imports again we're going to use polers for the data frames and then I'm using this sentence Transformers Library so open AI has text embeddings via their API but you know their API money on the other hand sentence Transformers is completely free and open source so I'm going to use that here here I'm going to load in the data that we just generated from the previous notebook and then doing a couple things here so here I'm doing some string manipulation to the rume text I'm removing any trailing white space and then I'm removing the first line which includes the person's name the person's first and last name probably doesn't have a whole lot of useful information for us in doing these text embeddings so it's better to just remove it and then also added this thing here to replace the people's names with numbers so I can show it in this example one of the great things about the sentence Transformers library is that they have a wide range of compatible embedding models that you can choose from all mini LM L6 V2 is by far the most popular but then there's this other one that I'm using here which is paraphrase multilingual mpet base V2 and the reason I'm using this is that it works on both sentences and paragraphs but of course there are many other text embedding models that you can choose from I like to go to hugging face and look at their model repository if you select this filter sentence similarity all the embedding models will come up so we see that all mini LM L6 V2 is here this seems to be a very popular one I haven't used it before it's from by it's called BG M3 Alibaba has it nomic AI so there are tons of embedding models so many from sentence Transformers here so we're going to load in the model using the sentence Transformers module we can generate the embeddings like this what is happening here is we take the resumé column from the data frame we convert it to a list and then we pass it into the embedding model to generate the embeddings and then here I'm doing some fanciness so I'm defining a schema for a new polar data frame to organize all of these embeddings and then I append this DF embeddings data frame to the original data frame then it looks something like this we have the name here the resume and the experience level so this is everything we generated in the first notebook but now we we have all these embeddings there's 768 of them so what we've done now is essentially converted this resumé column into a set of numbers that we can use for further analysis which leads us right into use case number three which is lead scoring and customer segmentation here I've got a whole lot of imports we've got polar and numpy again mat plot lib will allow us to make some plots for training the lead scoring model and some machine learning stuff I'm going to use sklearn I've got random Forest classifier imported logistic regression the Au score which is a way we can evaluate binary classification models and then finally a function to generate a train and testing data set so first step is we're going to import the data that we just created in that previous notebook but now we're going to import Outreach data so we haven't seen this yet what this consists of is a list of everyone's names their rle kind of defined by me whether they responded to the Outreach and whether they said yes no or maybe that's what's in this DF Outreach then we're going to join these two data frames together so we join DF and DF Outreach and we'll drop any duplicates then again I have this line of code here to replace people's names with numbers so I can show it in this example code so the basic idea of lead scoring is that you're going to take some inputs like job title revenue of company customer Behavior so on and so forth and you want to predict the probability that that person buys your product I'm actually going to do something a little different since I don't have sales data I only have whether they said yes or no to the offer I'm going to train two different models one model is going to predict the probability that a person will say yes to my offer based on their resume and then another model to predict the probability that a person will say no to my offer based on their resume and the resumés will be represented by all the different text embeddings which is what's happening in this line of code here as well as experience level which we generated in that first notebook but as a first step since we have 768 of these embedding Dimensions I'm going to do a dimensionality reduction and so the way I'm going to do that is I'm going to train this auxiliary model so this model isn't going to predict the probability of someone saying yes or no but rather it's just going to predict the probability that they resp respond to my Outreach and here I'm going to use a random force classifier and I set a very large number of estimators with a pretty narrow depth just because I'm trying to get a sense of the most important predictors in predicting whether someone will respond or not do that in two lines of code so we initialize the classifier and then we fit it to our data we can then evaluate the classifier using the Au score Au score of 100 means it's perfect but of course it's overfitting because because we used all the data to train it but we don't really care about the performance of this model cuz we're not actually going to use it directly the only reason I trained this model is so I can get a sorted ranking of the most important features now we have a list of the features which are most helpful in predicting whether someone will respond and then these features are least helpful and it's funny that the experience level variable that we went through so much trouble of generating is one of the least predictive variables here which kind make sense so the people that would attend this Workshop they're from all different experience levels they'll be complete beginners or people who have been working in some field for decades now that we have a sense of which predictors are most helpful now we're going to train the yes and no models what I do first is I'm going to create a new data frame that only involves people who responded to the Outreach because if someone didn't respond then they had no way of saying yes or no so it's not really fair to put them into the training data from this new data frame I'll Define two columns one corresponding to whether they said yes and then another corresponding to whether they said no once I have that I can train the yes and the no model here I do it in a little sophisticated way I do it in a for Loop just because I didn't want to copy paste the same code for the yes model and no model cuz it's very similar so what I do here is I initialize a list that will save the classification models here I have a list of the target names the target names are what we just created here yes and no and then this is sending how many of the variables to use in the yes and no models and so again we have 769 variables that we could use but here I'm only going to use the 75 most predictive based on this initial analysis I did up here and then I initialize two more lists to store the evaluation metrics of the models based on the training and testing data and then here I Define the feature names to use in these models this is a data frame involving the most important features and then I'm just going to extract the variable names up until the 75th row so I'm going to essentially grab the 75 most important predictors now I just do this little for loops and so the first model corresponds to the yes model so this will get passed in I'll create two data sets have the predictors here and the target here then I'll create a train test split where I use 20% of the data for testing 80% for training then I'll train this logistic regression model and then once that model's trained I'll just append it to this list I initialized earlier and then I'll compute the Au score for the train and testing data set this will happen for the yes model and the no model and then at the end of that this is the result this is the performance of each model so the first is the yes model the second is the no model this is the performance on the training data and then this is the performance on the testing data so the big biggest problem with this analysis here is that there are only 100 examples really for something like this and we have so many embeddings you probably want at least a thousand examples or on that order but I still think there's some clues that can be extracted from this small data set at this point we've trained both a model to predict the probability that someone will say yes and the probability that someone will say no and so what I do here is I apply both of these models to the entire data set that's generating this array yes score and this array no score which just has the probabilities of yes and no respectively for each person in that original data frame then I'm adding The Columns to the original data frame the first is corresponding to this yes score so essentially the probability that someone will say yes to the offer the no score is essentially the probability that a person says no and then I generate this score which I'm defining as the yes score minus the no score so it's kind of like you want to maximize the probability that someone says yes while minimizing the probability that someone says no so this is just like a hacky way of doing that and then finally I'm adding yes and no columns to the original data set okay so now we've generated these scores for every lead in the data set now let's do some analysis good thing to do in any kind of analysis is to plot a histogram here's a histogram of the scores higher score is better we can also break this down by role this is pretty insightful so we can see that the roles that have the highest score so basically they have the highest yes score and lowest no score are it professionals and this kind of agrees with my intuition through doing the Outreach it seemed like it professionals was a promising customer avatar for these Workshops the next role here is data architect but this isn't something we can take too seriously because there was only one person with this role in the data set next we have Consultants so this also aligns with my intuitions they seem like a promising customer here data manager seem promising but again there's only two of them and then finally data analyst seems like a promising Avatar these actually were the three avatars that I had written down in my notes that seem to be most open or most interested in this type of Workshop but it's not just about like who is the best customer but who's like the anti- customer who do you want to avoid and make sure you don't reach out to it's interesting like data scientists and dat students are among the bottom which again aligns with my experience data SS seem to be split like half people like are so anti- generative Ai and llms others are excited about it but neither is super interested in doing these workshops because they either already know the stuff if they're into it and if they're not into it they don't care to learn this stuff of course there are data scientists out there who would attend workshops like this but bu and large as a population they're not super into it and then students were another one typically students just don't have money money to pay for a workshop like this so that also aligns with my experience you know another thing we can do which I just thought of just so that experience level doesn't go to waste we can include that here as well so even though it wasn't very predictive we can still use it to segment the data a bit okay so this is interesting people with the least amount of experience so just beginners seem to score most highly and then these midc careers so if we go back to this data augmentation script one was the entry level folks so people with 0 to one years of experience and then three were the midlevel folks so people in their careers for a bit so they seem to be the most promising avatars based on this analysis and then everyone else has negative score so more likely to say no than yes that's an interesting pattern of course like there are only four people that have 0 to two years of experience so take that with the grain of salt and overall there are only 100 people in this data set so this whole analysis needs to be taken with the grain of Sal and then the last thing we can do is Define customer segments and so this is helpful because now what this allows you to do instead of like getting so granular for someone's score you know sales is a lot of times very unpredictable so the more precise your metrics are when it comes to this kind of analysis probably the worse off your strategy is going to be in practice it just makes your strategy very fragile if it's dependent on the third decimal place of this score that you generated using machine learning that's why it's really common to Define grades based on these scores so basically what you do is you take all the people in some sample you generate scores for them and then based on that sample you'll Define segments so you'll basically say anyone with a score higher than 0.05 will put in grade A anyone with a score below minus 0.05 will put in grade C and then everyone in the middle will put in Grade B the way I did it is that I put the bottom 20% in grade C the top 10% in grade A and then everyone else I put in Grade B so that's what this is showing here and then what we can do is we can use these grades and then apply it to New resumés or New Leads so as you identify a list of a thousand potential people to reach out to or 10,000 or 100,000 or a million just like more leads than you could ever reach out to or maybe it costs a lot of money and effort to reach out to a lead so you want to like distill it down to the most important you can take resumés pass it into this pipeline of the yes score and the no score and then compute the grade now you have different segments of customers of a grade leads b-grade leads and c-grade leads this might be a better way of segmenting it because let's say we just did the roles I identified the Consultants data analysts and it professionals as good people to reach out to and then data scientists and data students as bad people to reach out to but then we see a data scientist appearing right here with a grade of a and they had a pretty solid yes score and a low no score it's just that they didn't respond and maybe if they had responded they would have said yes to the offer that's one of the reasons why doing the lead scoring and then segmenting customers based on the lead score can be beneficial Beyond some generalization of a particular role or something else okay so that brings us to the end again the code is freely available on the GitHub and a Blog will be coming out soon if you enjoyed this topic of real world AI use cases that are not a chatbot I have about a dozen other use cases that I didn't include here but I could include in a future video so if that's something that you'd be interested in please let me know in the comment section below and as always thank you so much for your time and thanks for watching"
bwW1I6JlI30,2024-08-12T17:47:20.000000,How to Learn Anything #learning #shorts,had a PhD in physics here's my four-step framework for learning anything step one consume content I'm always shocked by how much Clarity I can get from Reading one good article or watching One Good YouTube video this is usually enough to get the ball rolling on learning something new step two mentorship learning from those ahead of me is one of the greatest hacks I've discovered for developing a new skill set and the best info usually comes from those that are just one or two steps ahead step three do it learning from content and others can only take you so far eventually you need to put your learnings into practice and do it and step four teach in my view teaching is the ultimate way to learn forcing myself to construct a narrative around a set of ideas is a great way to get more clarity this is the number one reason why I make YouTube videos
3PIqhdRzhxE,2024-07-29T19:20:52.000000,Local LLM Fine-tuning on Mac (M1 16GB),with the rise of open- source models and efficient fine-tuning methods it's never been easier to build custom ml solutions for example anyone with a single GPU can now fine-tune a large language model on their local machine which is exactly what I did in a previous video of this series however since my machine is an M series Mac which doesn't have an Nvidia GPU I had to use the free GPU on Google collab to run that example this is somewhat disappoint pointing because using collabs free gpus is somewhat restrictive and not as convenient as running something on my local machine that's why in this video I'm going to share an easy way to fine-tune an llm locally on Mac and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoy this content please consider subscribing that's a great no cost way you can support me in all the videos that I make if you've been keeping up with machine learning over the past decade then you're probably familiar with Nvidia and all their different gpus it turns out that gpus are super helpful for machine learning because they can much more efficiently train and run machine learning models than traditional CPUs they've also demonstrated an ability to take Nvidia from a hundred billion valuation all the way up to 3 trillion in the past 5 years nvidia's dominance of the GPU Market has greatly influenced the available open-source tools for running and training neural networks the result of this is that a lot of open- source tools work seamlessly with Nvidia Hardware while this is great for Windows and Linux users it often leaves Mac users like me left sitting on these Sidelines after a failed attempt to locally fine-tune llama 2 on my local machine my impression was pulling this off is something that would take several hours of effort that was until I discovered the mlx python library mlx is a python Library developed by Apple's machine learning research team for efficiently running Matrix operations on Apple silicon so we see the documentation here it's inspired by Frameworks like pie torch Jacks and array file one of the notable differences with mlx is that it allows you to use the unified memory model of these M1 chips no longer do you have to worry about RAM and vram being separate things M1 chips just have a single memory so that means me with my Mac Mini M1 2020 with only 16 gigs of memory am capable of fine-tuning a large language model locally on my machine while mlx is a pretty low-level framework it's not going to have highlevel abstractions for loading and training models like hugging face for example there is this example implementation of Laura which is very readily hackable and adaptable to another use case which is exactly what I'm going to do here similar to the previous Cur video here I'm going to fine-tune a quantized version of mistol 7B instruct to respond to YouTube comments in my likeness however instead of using hugging face in Google collab here I'm going to use the mlx library and my local machine and again here are the specs of my machine it's a Mac Mini M1 from 2020 with only 16 GB of memory so by today's standards my machine is hilarious but despite that it's still good enough to implement ment this fine-tuning example so I've put together some example code and that's available on GitHub if you go to my YouTube blog repo go to the llm tab here we see all the different code and videos and blogs of this series so now there's a new one called kulur mlx so we click on that what I've done here is I have this notebook that walks through the example code and then I've taken all the scripts from that mlx example implementation so I've put them in the scripts folder and then here I've prepared data so this is data of my YouTube comments and I prepared it in this Json L format but we won't talk about this right now talk about it a little later okay so the first step in running this example is to go to the repo and clone it so I'll copy this Ur here I'll go over to my terminal let me zoom in a bit okay so we're going to clone the repo get clone might take a while unfortunately there's not a way to clone a specific subfolder of a GitHub repo if you you want to download some code from a repo you have to clone the entire thing go to the repo we just cloned and then the code is in the llms subdirectory and then it's Q mlx so here we see everything we've got our example code requirements file some scripts some data and a readme okay so I cloned the repo and I navigated to this folder so now let's create a python environment that allows us to run the code so here I'll create a new virtual environment called mlx DV and then I'll activate this environment so this how you do it in bash and zsh and I guess every Mac User uses this so this is how you'll activate the environment all right so now you can see we're in this mlx DV environment but the only thing in here is PIP so you see you do pip list all there is is PIP so let's install all the required libraries so we can just do the PIP install D requirements. text and this will install everything while that's happen happening we can look at what is in that so here we have mlx we have mlx LM so this is a library built on top of mlx specifically for large language models also we have the Transformers library and numpy and then finally since I wrote the code in a Jupiter notebook we'll install Jupiter lab in IPI widgets okay so we installed all the requirements and if you run into trouble in the installation steps so here are some important notes from mlx is documentation first and foremost you need the M series chip like that's the whole point of this video and this Library the second is that you are using a native python that's greater than or equal to 3.8 so I think here I'm using 3.12 if I just do this yeah so I'm using 3.2.2 then you have to have at least Mac OS 13.5 but they recommend that you have Mac OS 14 which is what I'm running on my machine right now so now that we've cloned the repo and we set up our environment let's run through the example code so to do that we'll do Jupiter lab okay so here we've got the example code video link and blog link coming soon cuz I'm making it right now and then we're going to zoom in because it's probably tiny on your screen all right so mlx fine tuning we're going to do some imports so we're going to import subprocess because all the example code that we're hacking runs from the command line and so subprocess allows us to run terminal commands through Python and then here I'm importing the mlx LM library to run inference on a model a little later so I defined some helper functions here they're not super important right now so I'll just come back to them as we encounter them in the code and then this is an optional step that example code from mlx comes with this convert. py script which is capable of taking any model from the hugging face Hub and converting it into the proper mlx format and additionally quantizing it so I actually did this for mistol 7B instruct version 0.2 and then there there's this argument that you can pass that will push the model to the mlx community so I guess that's worth calling out right now mlx has this page on hugging face here are several mlx compatible models many of which are quantized so there's Google Gemma mistol quen code Gemma llama 3 53 whisper llama 3.1 even so this page seems pretty active and basically any model that you would want to find tune is probably already available here and if it's not you can simply just go find the model that you want to use in mlx let's go to mistal AI I'll go to Mistral 7B instruct so this is the one I used given this hugging face model path we can convert it to the mlx format and quantize it using this convert. py script so there was this typo so I had to add this quantize flag so what this will do is grab the hugging face model convert to the mlx format and this quantize flag converts into 4bit quanti ization so I actually didn't run the command yet so you could run this command using the subprocess module but I found it's better to just print the command like this and copy paste it into the terminal because when you run these shell commands in a jupyter notebook you don't see the same like progress metrics that you would normally and this will actually save a mlx version of that model on your local machine that you can run for inference and fine-tuning since here we're going to use a model that's already available on the hugging face hub here's the model card for it we can just skip this quantized model step and so here what's happening is I'm going to build the prompt that is defined here so again here we're creating a YouTube comment responder what this model will do is you'll pass in a comment and it'll respond to that comment hopefully in my likeness so for this example I'm not just going to pass the raw comment into the model itself for inference I have actually constructed this prompt which is the same prompt that we saw in the previous Cur example to help the model generate better responses I created a Lambda function for this to find this instruction string and then did some string manipulation to incorporate the comment into it dynamically the result of that is this prompt Builder Lambda function which takes in the comment and so here we're just doing a very boring comment great content thank you and then we'll Define the max number of tokens and then here we're going to use the mlx LM library to do inference so the syntax looks very similar to hugging face which is pretty convenient if you're comfortable with hugging face already so here what we're going to do is we're going to load in the model so this is that same quantize model and then we're going to have the model generate a response so we pass in the model the tokenizer which was loaded automatically The Prompt that we defined using our prompt Builder the max number of tokens which I set to 140 and then I put verbos equal to True okay so we can take a look at the prompt here which is all this stuff I'm not going to read all this you can read it if you like but this is just to help nudge the model in the right direction then we have this please respond to the following comment and this is the comment that we spliced in using that prompt Builder Lambda function and then down here is the model's response so this is the raw quantized model no fine-tuning whatsoever and this is the response well first and foremost it put sha GPT in the wrong place this is supposed to be at the end of the response not at the beginning but it says thank you for your kind words I'm glad you found the content helpful and enjoyable if you any specific questions or topics you'd like me to cover in more detail please feel free to ask I would never respond to a comment like this this is something we've seen in the previous fine-tuning videos where the unfin tuned responses tend to be pretty verbose and when I respond to comments or really any kind of communication I try to keep it as concise as possible so this is very different than something I'd actually write in response to a comment so let's see how we can fine-tune this model to generate responses that sound a lot more like me here again we're going to use one of the scripts from that mlx examples repo and I'm going to construct the command in the same way so I put everything in a list and the reason I have it in a list is because that's how you can run these terminal commands in Python you'll pass it in as a list to the subprocess module but I realized that even with this fancy helper function that chat GPT wrote me to continuously print outputs from my terminal command it still wasn't printing the loss during training so I actually found an alternative strategy to be more helpful which is to just take this command variable here and to translate it into a string that I can copy and paste into my terminal and so this construct shell command is just a pretty simple helper function up here that's just doing some basic string manipulation to convert this list into a string what I'll do is I'll copy this string and then we can go over to terminal and then I'll paste the command here here okay so walking through this a little bit I'll try to zoom in even more hopefully that's legible we're running a python script so that's what this is the python script we're running is called lowra and it's in this scripts subdirectory and then we're just going to define the parameters for training so here we're specifying the model which is that 4-bit version of mistal 7B instruct we have this train flag because we're going to run training we're going to set the number of iterations so we're going to do 100 iterations that means it's going to run through 100 batches and the default batch is four next is steps per eval so this is the number of batches that the training will run through before Computing the validation loss so I set it to 10 which is the same as the number of steps per training loss evaluation the Val batches is the number of examples to include in the validation loss calculation setting it to ne1 goes through everything in the validation data set and here that's only 10 examples next we set the learning rate at 1 to the minus 5 which is actually the same as the default but I have it explicitly written here cuz I was playing around with this I probably ran this a dozen different times trying to find the best hyper parameters and then low R layers 16 which is also the default parameter but I went through a lot of iterations to just end up coming back to the default and then finally we use this test flag which computes the test loss at the end of training I guess before we run this it's worth talking about the data that we're using here we have three data sets so we have have this train. Json L test. Json L and valid Json L the way I make these data sets are here in this jupyter notebook also available on the GitHub what I'm doing here is I'm taking a CSV file of YouTube comments and responses which looks like this so I've got 70 comments these are real comments and 70 real responses from me and so this is the way I'm going to train the model to respond to comments in my likeness and way I do the train test validation split is I have 50 examples for training 10 examples for testing and 10 examples for validation I won't walk through this code cuz I did it in the Cur video and I did a similar thing for the open AI fine-tuning API video but if you're curious about how I'm doing the data preparation feel free to check this out we're using the same prompt in the training data as I used at inference in the example code we just saw very similar strategy the Json l format if you're familiar with python is essentially a list of dictionaries so a dictionary is a set of key value Pairs and a list is just a sequence a collection of elements Json L is just going to be a collection of these key value pairs where each key value pair each dictionary consists of one key and one value so super simple the key is text and the value is the example that you want to use to train or evaluate the language model notice that this contains the instructions essentially of the model this bit is the comment the real comment from the user and then it ends with this instruction token and then here's the real response from me so all of these are packed together to form the example and we have 70 of these and 50 are going toward training 10 are going for testing and 10 are going for validation so here I randomly select examples for testing and validation and then I just write everything to these Json L files so hopefully this example code is easy for you to follow and hack and adapt for your own use case and if you get stuck you know feel free to drop a comment or reach out I'm happy to help try to get you unstuck okay so that brief aside was the data preparation now let's go into training so I actually haven't done this while running OBS so my computer might blow up if I do this so I've got activity monitor and it's not looking great because OBS is already using a gig of memory and when I was running this the fine-tuning script was taking like 14 G gabt of memory and so I'm going to execute this script but if my computer blows up and I'm not able to post this video I'm so sorry all right here goes nothing nothing has blown up yet we'll just keep the activity monitor here so we can monitor the memory pressure so we see that the fine-tuning script is taking about 10 gbes of memory there's this other python script which I guess is the Jupiter notebook taking up 4 GB of memory M and then we got OBS here taking up 1 gab of memory so when I was doing this for real I was basically doing nothing else on my machine I was just allowing as much memory as possible to be dedicated to the fine-tuning script but here it seems like the mlx library is handling it pretty well you know it seems to just dynamically adapt to however much memory is available okay so it computed the V loss I don't think anything's going to blow up so that's great and kudos to Apple and their ml research team for writing a good software library but but I do think since less memory is being allocated to this compared to just not running anything else on my machine this is going to take a lot longer to run before when I was just allowing the training to run all by itself I didn't have the jupyter notebook running either this was hitting like 14 GB of memory 13 to 14 or something got kind of close to 13 there and it took about like 15 or 20 minutes to fine-tune the model with the hyperparameters shown here batch size of four and has 50 training examples so I'm not going to sit here for 20 minutes to wait for this to run cuz I've already done this and I can show you the finished product once again like those cooking shows where they show you how to prepare the food and put in the oven and then magically they had the lasagna that they made last night and they're going to eat it in front of us okay so here's me eating the lasagna we're going to quit out of this process killed that and then what we can do is to run inference with the fine tuned model so this doesn't have the adapter file so what I'll do is open up a new one of these we'll do this in private once training is done so we'll say 20 minutes goes by you go get a sandwich or something while this is running this adapters. npz file will appear in the repository these are the low weights learned during training once that is here we can continue with the Jupiter notebook and we'll use these adapters to run inference again so this will be our fine-tuned model so we're going to run it in a similar way but instead of printing a command and copy pasting it into the terminal here we can just run it in the notebook because there's really not much to see you can see that it failed cuz the adapters file wasn't there but now that it's here we can run this again so now it loaded the pre-trained model it passed in the prompt and the prompt is including the comment which is just that simple great content thank you and then we have the response from Sha GPT glad you enjoyed it smiley face sha GPT so this is much more aligned with something I would actually say to a short comment like this going back this long and poorly formatted comment is not what we want but just after 50 training examples we see that the model is noticeably responding in a different way and then we can even run this a few times to see what else it comes up with glad you liked it so I guess it'll probably keep generating responses that are similar to that yeah glad you're here happy to help okay yeah these are great these are things that I have said in comments so it's doing a good job but that's a really easy comment to respond to let's try a different comment so here's a harder one that at I think this is more recent I don't know this is not in the training data or anything so let's see how sha GPT mlx handles this one so the comment is I discovered your channel yesterday and I'm hooked great job it would be nice to see a video of fine-tuning shot GPT using hugging face I saw a video you running it on Google collap 7B any chance of doing a video on your laptop Mac or using hugging face spaces so that's exactly what this video is let's see what shot GPT thinks hi thanks for your kind words sure thing I'll do a video about fine-tuning HF version on Sha GPT on my Mac then has this YouTube link hope it helps sha GPT okay so let's see what this takes us to ah the video doesn't exist probably because I'm making the video it'd be really crazy if when I post this video this becomes the URL you know it's responding appropriately you know it's like thanking them it's pretty short it puts sha GPT at the end this sentence here doesn't make a whole lot of sense so I'll do a video about fine-tuning HF versions of Shaw GPD so hugging face versions of sha GPD on my Mac see what else it says so I guess this is a kind of hard question to respond to how would it know what I want to do glad to hear it glad you found the channel useful okay well this is a nice response but it doesn't answer the person's question so let's try another one hey glad you're enjoying the channel okay refuses to respond I guess another thing is we can check out the memory spikes during inference so let's do this one more time take a look at the memory pressure so we see that it kind of goes up a bit I guess it's opening up another python instance to run these subprocesses and it takes about 4 GB of memory yeah GL here so it's refusing to respond to this guy's comment I think I got lucky when I uploaded this because it had a really good response let's see glad you enjoyed it I'm looking forward to doing a fine-tuning video on my laptop I've got a Mac M1 mini that runs the latest version of the HF API so the great thing about this one is that it's got right that I have a M1 Mac Mini does run the latest version of the hugging face API but we didn't use the hugging face API but yeah I guess we did so we imported Transformers so Transformers is working under the hood so that was the example super simple but I will say there was one thing I forgot to mention which is that in this L.P file I went through like a dozen different sets of hyperparameters to try to get this thing working which is just the reality of machine learning machine learning is much more art than science or at least for now but I did want to point out one thing that I had to do so I had to go in here and kind of hack one set of hyper parameters okay here so adjusting the rank of these Lowa adapters is not something that this L.P file exposes as a command line argument so you can't just say oh I want to try rank four or rank eight or rank 16 or whatever from the command line I had to go in to the file and just manually change it I think it was eight originally and I changed it to four and this improved the training performance before I did this it was just kept overfitting and I tried a lot of different sets of hyper parameters but reducing the rank worked a lot better and this aligns with results from the low R paper if you've taken a look at and if you haven't check it out it's a really good read rank four rank eight seem to be that sweet spot for the results I guess I can pull it up real quick so in table six of the lowette paper you can see they were comparing what weights they were applying the adapters to and the different ranks of the adapters in the qar example on Google collab I just applied fine tuning to the query layer using rank eight but in this example I applied it to both the query and the value layers and I used rank four and you can see that at least in these examples the rank four rank eight is kind of like this inflection point in the performance like it kind of flattens out and actually starts to get a little worse as the rank gets too big okay so that brings us to the end of this walkthr the example code is again freely available on the GitHub repository shown here and I'll link it in the description below if you enjoyed this video or you have suggest sus for future content please let me know in the comment section below and as always thank you so much for your time and thanks for watching
y7xbHtFostg,2024-07-22T13:46:22.000000,Data Science Explained in 60 Seconds,data science explained in 60 seconds by a data scientist 12 years ago data science was the sexiest job of the century today it's still an in demand role offering pretty cushy salaries but what do they actually do the way I see it the fundamental role of a data scientist is to answer questions using data in my experience this consists of four Key activities first is getting a question to answer by working with clients and business stakeholders two is acquiring the relevant data this is the boring yet most important part third is analyzing the data once data are in a usable form data scientists use tools like Python and R to compute statistics and generate data visualizations and fourth is to build models this is probably what gave data science the sexy label data scientists can use data to construct predictive models to solve problems and support decision making
f4Gsbq10j1s,2024-07-10T13:00:45.000000,"How Much YouTube Paid Me for 1,000,000+ Views (no fluff)","$4,733 and5"
TwVl4aA5qtk,2024-07-05T18:06:10.000000,I Was Wrong About YouTube (what I learned),3 months ago I made a video about taking a step back from AI Consulting to focus more on YouTube my thinking was since most of my opportunities come through YouTube doubling my video output would double my business however after 3 months of posting YouTube videos every week it's safe to say that I was wrong in this video I'm going to share what happened last quarter along with my key learnings in case it is helpful to anyone on a similar journey and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoy this content please consider subscribing that's a great noost way you can support me in all the videos that I make in 2020 I started posting YouTube videos while getting my physics PhD as a grad student I was navigating an ocean of technical topics my videos were a way for me to learn by making content that would have been helpful to a past version of myself although the basic premise of the channel is the same since then something unexpected happened the channel got some traction and started to open up opportunities outside a regular 9-to-5 job this is mostly around teaching people Ai and helping businesses with projects while I post content on many platforms such as medium LinkedIn and X the vast majority of the opportunities I receive come through YouTube this brings me to 3 months ago when I faced a dilemma there was a tension between making videos and my Consulting business more specifically for me making videos requires long stretches of uninterrupted attention where even one 15minute call can kill an entire afternoon of productivity Consulting on the other hand is fundamentally different it consists of taking calls sending emails drafting proposals and contracts in other words the sort of thing that kills my creative flow balancing these opposite types of work was a struggle which eventually led me to the conclusion that I needed to pick one I ultimately went with YouTube because in the long run a successful YouTube channel gives me a lot more options than a successful consultancy so I increased my Consulting rates and stopped taking project-based work so I could focus on posting one YouTube video every single week since YouTube is the main way people find me my hypothesis was if I doubled my video output I would double my opportunities and thus my business this can be Quantified through metrics such as the number of paid calls booked the number of inquiries I received the number of new subscribers and monthly YouTube Revenue however after 3 months of posting every week on YouTube the results were not what I expected comparing q1 to Q2 my number of paid calls went down from 10 to 8 and inquiries dropped from 35 down to 29 on the YouTube side while the number of subscribers and revenue slightly went up my month-over-month metric are trending downward since April at this point you might be thinking what happened of course 3 months is a short period of time so the conclusions we can draw from this data are limited however there are still three important lessons I got from this the first is that it's not all about quantity the thing about YouTube is that one video can make up the bulk of a Channel's views for example my fine-tuning video makes up for about 22% of my total views this uneven distribution of Engagement supports focusing on making high quality content however quality is a loaded word here it can mean different things from person to person this is where the art of content creation comes in the way I see it creators develop an intuition for what quality means through trial and error and both trial and error are important here because that's the only way that we learn which is why the recent drop in my channel metrics is ultimately a good thing because it solidified the following two points the first is people want llm content I didn't post any llm videos in Q2 and these videos get more than 10 times as many views as my typical videos the second point is the power of doing Series in other words one-off videos tend to do worse than a series of related videos posted one after another this makes sense because when you watch a video from a particular channel YouTube YouTube will start recommending other videos from that channel and if you liked one video in a series odds are you'll like another another key learning is that maximizing content means nothing without a business strategy in other words if I'm trying to grow my business by growing my channel it's important that the content I put out supports my business goals I didn't spend enough time thinking this through and the bulk of my videos from last quarter were from my full stack data science Series where I showed how to implement a data science project from end to end while I learned a lot from this series and it seemed to help many trying to learn these skills ultimately these videos aren't something that a potential client would watch which naturally led to fewer paid calls and inquiries and the final learning from last quarter was I got out of touch I was spending too much time making content and not enough time talking to customers and other practitioners consequently I became disconnected from the industry and my audience another downside of strict maximizing content output is that I had less time to reflect and dwell on video ideas which to me seems to have diminished video quality reflecting on this experience I feel doubling down on YouTube was an overcorrection however I still believe a correction was needed my path was heading more and more toward growing a consultancy although this is a great business in a great way that technical folks can make money growing a firm like that doesn't sound like a whole lot of of fun to me what I love is pursuing my curiosity which is why I love learning doing research building projects and talking about these things but of course in the real world we have constraints such as rent and food and bills in other words at some point you've got to make some money while doing what you love and making money are sometimes at odds I don't think this is a necessary condition the way I see an entrepreneurship provides anyone the opportunity to create their dream job and ultimately their dream life all that to say I feel by narrowly focusing on this one video a week goal I lost track of this bigger picture I forgot the goal of all this was to make a living doing the things that I love which brings me to my three goals for this quarter first is lowering my content goal from one video a week to three videos a month I feel this will be a good balance of quantity while giving me time to ensure quality and work on other projects the second goal is to validate one new Revenue Source while making content and doing small Consulting engagements generate some Revenue it's hard to imagine how this can scale that's why I'll be dedicating more time to exploring other Revenue sources my current ideas include a referral business where I connect companies to a handpicked list of AI Dev shops doing AI trainings for companies and hosting a cohort-based AI course for professionals if you have any feedback on these or other suggestions please let me know in the comments below and the third and final goal is to help 30 companies use AI this can be through paid calls referrals or small Consulting engagements the main motivation of this goal is to stay more connected with industry use cases of AI my central strategy for hitting this goal is by lowering my consulting rate this is actually something I implemented 2 weeks ago and I've already seen an increase in bookings so that brings us to the end although entrepreneurship is a messy process I hope my failures and mistakes are somewhat helpful to those on a similar journey and to everyone that shared feedback and support on my last video I just wanted to say thank you the diversity of perspectives and input that I received has been very helpful in many different ways and as always thank you so much for your time and thanks for watching
suxHNXk5jp0,2024-07-01T19:36:39.000000,Why Businesses Should NOT use #AI,although every business is trying to use AI today it's not always the right call here are three reasons you should not use AI in your business one lack of technical Talent although using AI is easier now than ever before building custom projects is still technically demanding without the right talent for implementation in maintenance the chances of success are low two unnecessary cost and complexity for most business problems AI is unnecessary simple workflows can generate much of the value and third AI risk putting aside the existential AI risk debate there are clear impr present business risks for AI for example lack of explainability unexpected bias and uncertainty in Model Behavior
baxaZI_j71I,2024-06-19T21:57:56.000000,3 Reasons Businesses Should NOT Use AI,there are a lot of good reasons for businesses to use AI it's a powerful technology their customers are asking for it and they don't want to fall behind their competition however these points tend to overshadow the other side of the coin why one should not use AI over the past year I've consulted 65 smbs on using AI in their businesses through these conversations I've seen a few common problems that limit businesses in their ability to effectively use AI to create value to hopefully help you avoid these problems here I'm going to discuss three reasons why smbs should not use Ai and if you're new here welcome I'm Shaw I'm a data scientist turned entrepreneur and if you enjoy this content please consider subscribing that's a great no cost way you can support me in all the videos that I make the first Common problem for businesses is lack of AI infrastructure what I mean by this is the combination of two main things one one data infrastructure and two technical Talent data infrastructure makes it easy to access and use data relevant to your business this is important because it significantly reduces the barrier to entry for AI projects such as Rag and fine-tuning which are ways to tailor an AI system to a particular business context or use case another reason why existing data infrastructure is beneficial is because it provides additional opportunities for Value creation more more specifically AI can be used to enhance existing data workflows by introducing new ways to capture and structure data for example capturing information from customer reviews company documents and public web pages to augment current data sets and decision-making the second side of AI infrastructure is technical Talent although using AI is easier now than ever before building custom projects is still technically demanding this requires having the right people to support its development in your business generally speaking there are two options for this option one is to have an internal AI team the upside of this is it keeps costs relatively low compared to hiring outside contractors and the team can better integrate with other parts of the business but of course this comes with the downside that technical teams take time to develop thus are a long-term investment option two is to hire an external AI team in other words hiring a vendor in contrast to option one these teams can get started relatively quickly and with low initial investment however this comes with a larger cost in the long run and runs the risk of developing a dependence on a third-party vendor the second thing to consider is that AI often introduces unnecessary costs and complexity in business value is generated through solving problems the trouble with using AI to solve a particular problem is that it often comes with significant overhead namely you'll need to curate the data and the technical t talent to implement the solution however even if you have the data infrastructure and teams in place some AI Solutions might be overkill for instance one of the most common use cases clients approach me with is a chatbot that can interact with users and retrieve information from a knowledge base while this is a new and Powerful way to solve the search problem it is significantly more complex to engineer than the standard search interface and the marginal value generated may not justify this additional complexity the third and final reason to not use AI is AI risk this is something that can easily get eclipsed by all the excitement we're seeing these days surrounding AI risk is often a statement of ignorance and there's a lot we don't understand about how AI systems like chat gbt work under the hood while this creates many potential risks the two I want to focus on here are lack of explainability and hallucinations more powerful AI systems often come at the cost of less explainability this means we don't know exactly why a system generated a particular output which can pose significant risks if this output is being used to make business decisions or influence how customers are treated the second risk are hallucinations these are when generative AI systems generate false outputs that can potentially be harmful a recent example of this was an Air Canada chatbot provided false information to a customer about their bement policy leading the customer to sue the company and win about $800 while this single case is a negligible cost for the company it's not clear what the impact of future hallucinations might be of course there are many other AI risks such as prompt injection leaking proprietary data and unknown model biases however I'll save a deeper discussion of AI risks for a future video I don't want to give the impression that small to medium-sized businesses should give up on AI because of all these potential challenges in fact I believe the opportunity for smbs is even greater than for larger companies because they're still flexible enough to explore new ways of doing business which can help them replace the incumbents AI is reshuffling the deck which means the quote unquote little guys have the most to gain from it toward that end I want to share three recommendations for how smbs can approach using AI the first is to start with the problem not the technology there's a quirk of human psychology called The Hammer problem which says when you have a really nice Hammer like AI everything starts to look like a nail this leads us to go around and whack every problem with it while you might hit a few Nails in this process you might break a few things too that's why it's important to think critically about the problems you apply AI to ask yourself why do I want to use AI to solve this problem is there a simpler solution the second recommendation is to start with chat GPT often much of the value of an AI use case can be realized completely for free using chat GPT and other similar tools this is a great low stakes way to experiment with and validate use case ideas put another way if you can't realize the value using chat GPT then this might be a red flag for the feasibility of a more sophisticated solution however if chat GPT does provide a good Solution that's a good indicator that a more sophisticated solution might be worth pursuing the third and final recommendation is to keep it low stakes Beyond experimenting with ideas for free using Chad GP it's important to limit the costs and the risks of new AI applications this can be achieved in many ways such as validating new ideas with quick and lowcost poc's keeping a human in the loop of the AI system and applying it to areas where the cost of being wrong isn't significantly higher than the benefit of being right that brings us to the end this video is part of a larger series on AI for business so if there are any topics you'd like me to cover in future videos of this series please let me know in the comments and as always thank you so much for your time and thanks for watching
X8ZR6yFdg1Q,2024-06-17T14:12:45.000000,AI Explained in 60 Seconds #ai,here's AI explained in 60 seconds the definition of AI that I like is a computer's ability to solve problems and make decisions this can come in three different flavors flavor one traditional software development this is where we give computers stepbystep instructions to do something that we care about Flavor two is machine learning there are things that we care about but don't know how to give step-by-step instructions for it think of things like object detection or self-driving car s this is where machine learning comes in which allows us to program computers using examples third and final flavor is chat GPT and Friends machine learning requires a lot of examples to work well however modern tools like Chad gbt come out of the box with a ton of World Knowledge this allows us to use chat GPT to solve problems it wasn't explicitly trained to do like create images or build me a website
sxvyBxLVvKs,2024-06-13T23:44:59.000000,How to Communicate Effectively (as a Data Scientist),"let's talk about technical communication this is universally one of the biggest challenges that data scientists face however it also presents one of the greatest opportunities here I'm going to discuss the seven tips that have been most helpful to me in becoming a more effective Communicator you might be thinking Shaw I'm a data scientist my job is to code and build AIS and understand statistics why do I need to Be an Effective Communicator can I just leave that to the business folks of course this is a narrow perspective because in most business contexts the way data scientists create value is not through writing code or building AIS but rather through solving problems and the problems that they solve aren't their own problems but they are the problems of stakeholders or clients in order to effectively solve these problems the data scientist must be able to effectively communicate with the stakeholders this highlights the point that it doesn't matter how powerful your AI is or how impressive your analysis is if you can't effectively convey these things to these stakeholders they're never actually going to be used to solve the problem this presents a key bottleneck most of the time data scientists aren't limited by their technical ability but rather their ability to effectively communicate with stakeholders and clients and when I was working at a big Enterprise this was one of the biggest factors that would hold back data science scientists from advancing so if you want to make greater impact you want to drive more value and you want to get those promotions improving your communication is one of the best ways to do that as a data scientist some might think that communication is an innate skill meaning that it's either something you're good at or you're not good at and this of course is false communication like any other skill is something that needs to be developed through practice and I am living proof of that where five years ago I was an overly technical physics grad student who probably spent too much time in the lab but after 5 years of dedicated effort I now get invited to do public speaking events my YouTube videos have been viewed over 1 million times which is just a mindboggling number to me and my medium articles have been read over 300,000 times all that to say if this guy can do it you can too and so here I'm going to share the top seven communication tips that have helped me over these past 5 years the first is to use Stories the second is to use examples third use analogies the fourth is structuring information as numbered lists fifth is always following the principle of less is more six is to show rather than tell and the seventh and final one is to slow down so let's talk about these one by one the first tip is to use stories this this is something I picked up from a book called The storytellers secret there the author describes how our brains are wired for stories stories just make sense to us and they are one of the most powerful tools we can use to communicate information effectively and when I say story here I don't mean something like a news article or a novel but rather any three-part narrative some examples of that include status quo problem solution this is my go-to storytelling format and you'll see this throughout my medium articles my YouTube videos and Linkedin posts let's see a concrete example of this AI has taken the business World by storm while its potential is clear translating it into specific value generating business applications remains a challenge here I discuss five AI success stories to help guide its development in your business another structure I like is the what why and how and I actually used this to structure this talk I started with the high level technical communication I talked about why it mattered then I dove into the how which are these seven tips another structure I like is the what so what and what now so in a business context what this might look like is website traffic is down 25% this has led to a 150,000 Revenue drop the analytics team is investigating the root cause this is a natural way to structure information for instance imagine if we didn't use a story here and we said something like the analytics team is in investigating the root cause of website traffic being down 25% and revenue dropping $150,000 it doesn't really have the same flow and ring to it it kind of feels like a barrage of information however structuring it into this three-part narrative makes it a bit more digestible and ends the communication on a positive note the next tip is to use examples examples are powerful because they ground abstract ideas with concrete examples what this might look like in practice is you might get a ping from a stakeholder asking you what's a feature because maybe you mentioned it three times in an email to them to which you could say features are things we use to make predictions while this answers the question it's still a pretty abstract statement and so an easy way to make this more clear is to add something like for example the features of our customer churn model include age of account and number of logins in the past 90 days this allows the other side to connect this abstract idea to a specific example to which they might respond with a heart emoji a related idea to using examples is to use analogies analogies are powerful because they map the unfamiliar to The Familiar for example the word algorithm is unfamiliar to many people however the word recipe is very familiar almost everyone has followed a recipe before another example is a feature to which you could make the analogy to an ingredient in a recipe an analogy I used in my fine-tuning video video was comparing fine-tuning to turning a raw diamond into a diamond that you'd actually put on a diamond ring the next tip is to use numbered lists which is what I'm doing in listing out these seven communication tips and the power of numbered lists is that numbers tend to stick out to us especially when trying to navigate an ocean of words and language this is something that really hit me when I read the great book by the late Daniel Conan Thinking Fast and Slow where he he framed these two systems of the mind so what people might call the conscious or the subconscious or the automatic thinking system and the deliberate thinking system he simply called them system one and system 2 these labels really stuck with me and consequently I started using this strategy throughout my content creation some examples are in my fine-tuning content I talk about three ways to fine-tune in my llm intro I talked about three levels of using llms my causal Discovery video I talked about three tricks for causal Discovery when listing takeaways I'll often add a number to them like my four key takeaways or my three takeaways and in my causal inference blog I talked about the three gifts of causal inference and then these will be followed up by listings like we're seeing in this article where it's like tip one tip two tip three this way of structuring information tends to make it a bit more digestible for the audience the fifth tip is less is more which is the fundamental principle of all types of communication what really convinced me of the power of less is more is work again from Daniel Conan in his capacity Theory whose basic premise is our attention and ability to exert mental effort is limited applying that to a communication context your audience's attention is finite so it is important that you be very economical in how you spend your audience's attention do you want to spend it on small talk and fluff or do you want to spend it on key information and while you might think having less things on your slides or using less words in an email should take less time the exact opposite is usually the case and this is captured well by the famous quote from Mark Twain which goes I didn't have time to write a short letter so I wrote a long one instead a related idea to less is more is to show don't tell and the basic idea here is to use pictures over words at every opportunity to demonstrate this let's look at the fine-tuning analogy I made on a previous slide fine tuning is when we take an existing model and refine it for a particular use case this is like taking a raw diamond base model and transforming it into a diamond we can put on a diamond ring fine-tuned model while it's good we used an analogy this is a lot of work and mental effort our audience needs to expand to get this information so let's see what this could look like through images this is a slide from my fine-tuning video and it conveys the point much more concisely we have an ugly RW Diamond here and it becomes a very beautiful Diamond here and these things are labeled by base model and fine tune model the seventh and final tip is to slow down this was another tip that had a significant impact on my communication skills before I had a tendency to rush through talks typically because I was nervous and I didn't want to waste the audience's time but the thing about a rush talk is that from the audience's perspective it feels like getting blasted in the face with a fire hose and it doesn't leave anyone very happy it's painful to listen to and it's hard to digest the information on the other hand a well-paced talk is like a soothing stream that is easy to digest and leaves the audience like this the irony of it all is that a rush talk even if it takes less time is more draining than a well-paced talk that might be an extra 3 to 5 minutes which might leave the audience energized and excited to ask questions I'll also throw in a bonus tip which is to Know Thy audience knowing your audience allows you to effectively frame all aspects of your communication so what this might look like in practice is if I'm explaining a new AI project to someone in the sea Suite I'll use terms like AI however if I'm talking to a bi analyst who has a bit more hands-on experience with generative AI I might use the term llm while if I'm talking to a fellow data scientist I might be more specific and say the specific model that is being used in the project like llama 38b however this isn't just about using the right terminology when talking to your audience different audiences care about different things so if you're talking to someone in the sea Suite you often want to focus on the what and the why leave the conversation at a relatively high level focusing on the business impact if talking to a bi analyst you still want to discuss the what and the why however you might give more technical details and in addition to the business impact you might talk about the relevance of the project to their specific workflow finally if talking to a fellow data scientist you would talk about the what why but also the how so this might include all the nitty-gritty technical details you still want to talk about business impact because everyone wants to know the why they want to know how their effort fits into the bigger picture the key difference however is that you might be talking about the how the specific implementation steps for the project so that brings us to the end if you enjoyed this video and you want to learn more check out the blog posted on medium and as always thank you so much for your time and thanks for watching"
XQWhJsXu0sY,2024-06-07T18:32:43.000000,What Nature Can Teach Us About Business,if you want to build a great business you need to build great systems and who better to learn from than the master system builder mother nature in this video I'm going to discuss seven principles from nature that entrepreneurs can use to improve their businesses I'll give a highle description of each principle and demonstrate them through examples from both nature and business Nature has produced systems that have endured billions of years so it's natural for us to look to these systems as a source of wisdom and inspiration that's the motivation for this talk which is the culmination of three main things the first is my formal training I got a PhD in physics and here's a photo of me admiring the wisdom of nature the second are several books that talk about this idea from many different perspectives the third and final thing is putting these principles into practice through my entrepreneurial Ventures including things like content creation independent Consulting and running a community for entrepreneurs the first principle is diversity which is what underlies the resilience of many natural systems and so a great example of this are naturally occurring prairies which support a wide diversity of species including grains and Wheats that we might eat and the thing about these prairies is that they're incredibly resilient to storms droughts they don't really have weeds They Don't Really suffer from pests this is in contrast to monocultures in other words where every crop is exactly the same like we see in traditional farming while this is The Logical way to maximize the output of a particular crop it has a big problem which is that monocultures amplify the weaknesses and vulnerabilities of the prominent species for example if we have a wheat farm pests like the hessen fly can thrive in this environment because wheat is a food source for these insects however in a prairie since there's a wide diversity of species there's not the same opportunity for pests to proliferate we can say the Prairie is resilient while the wheat farm is fragile a business example of this from my personal experience comes from content creation where I post about a wide variety of topics including things about freelancing and data science large language models and other technical topics such as the wavelet transform the upside of posting about a lot of different things is that I can draw a wider audience to my content and Beyond just writing about many different topics on a single platform I also create content on multiple platforms so this is like a second layer of diversity and I feel the strategy of posting related content on both medium and YouTube has been one of the main drivers of my growth on both of the platforms however diversity doesn't come without its downside so an obvious one is that there's this tension between diversity and maximizing output so figuring out the right balance of diversity to make your system resilient while still maintaining the level of output that you need to see depends on the specifics of the situation the second principle is redundancy in other words having backups in case something fails an example of this from nature is in our bodies we have two lungs we have two arms we have two kidneys so if any one lung arm or kidney gets injured the overall system can still function in other words we can survive an example of this from the business world is when I worked at a large Enterprise I was one of many data scientists sitting on the data science team having this redundancy meant if I wanted to take a vacation or if I wanted to leave the company the overall system the overall team would be just fine because the work could be easily reallocated but like diversity redundancy has a downside which is that when you have redundant systems you incur a cost without much marginal value in other words the marginal value of having one data scientist versus no data scientists is a lot bigger than having two data scientists than one data scientist taking a step back our first two principles seem to have a little bit of tension principle one told us to have many different things while principal 2 said to have many of the same thing and each of them had some Associated downside but what if there was a way we could get the best of both worlds so that's possible through principle number three degeneracy and I like to think of this as a blend of diversity and redundancy so to see this let's define each of these words visually diversity means everything looks different for example if we have three boxes diversity would mean that each box looks different on the other hand redundancy looks something like this where each box looks exactly the same the way we can blend these things together is if we had three boxes where one was a blend of red and green another was a blend of green and blue and then the last one was a blend of blue and red this way of doing things allows us to have diversity because each of these boxes is slightly different and redundancy because each box has some overlap in their underlying color at the same time this is an idea I first saw from the book antifragile by Nim Talib where he described degeneracy as a functional redundancy in other words you have separate elements in your system that are different things but have this functional overlap an example of this from nature is on our face we have a nose and a mouth which are two different parts of the body that do many different things but they do have one shared function which is that you can can breathe through both your nose and your mouth so if your nose is stuffed up you can still breathe through your mouth or if you're eating you can still breathe in through your nose so an example of this from my business is posting on medium and YouTube while these are different things have a shared function which is that they both Drive traffic to my Consulting Services the next principle is recycling put another way in nature nothing is wasted a beautiful example of this are trees which generate oxygen which is what we breathe in and then we will exhale carbon dioxide which the tree breathes in and exhales oxygen and so on and so forth natural systems have this incredible ability to be resourceful and use whatever is available in the environment to promote itself or persist an example of this from my business is through my content creation on medium YouTube and Linkedin many people will reach out to me with a wide array of messages this isn't a comprehensive list but some of the most common ones are questions about AI people wanting me to build projects for them a recent one was a business reached out to me to create a custom course to train their team and then another common one is people reaching out because they want a technical co-founder and so the bottom of my funnel is only meant for one of these things which is the AI questions for any questions I can't answer over email I offer a paid service where I'll hop on a call with an individual or a company to talk through their questions or use case ideas however these other things are not services that I offer so the default response to all these inquiries might be no I don't do that have a good day however that would be incredibly wasteful so I always try to find a place to put all the different types of inquiries that I receive so for this build me a project inquiry I've put together a list of AI consultancies and Freelancers that I'll refer these leads to and if that client ends up working with that firm or consultant they'll often give me a referral fee so that's just an added opportunity for Revenue a similar thing with the custom course so while making this course from scratch isn't something that I do it is something one of my colleagues does so I could again prefer that lead to that company and then finally people reaching out to me about being a co-founder I'll just redirect them to my community the data entrepreneurs which is full of technical folks and resources the fifth principle is iteration and from my view this is what underlies the wisdom of all the natural systems that we see in other words The evolutionary process is capable of taking very simple elements like a soup of molecules and turning them into the rich complexity and diversity of life that we see today on Earth the power of this was captured well by Ray dolly in his book principles where he had this great line iteration beats intelligence and the example he gave for that was with Co where there was a time not too long ago where Co was rapidly mutating and it took an army of phds to keep up with this rapid pace of mutations so this Army of phds and doctors and medical researchers were struggling to keep up with a very stupid virus the virus has no intelligence but through this iterative process this evolutionary process this incredible complexity emerges in the business World we've known iteration is a really good idea that's why we have mantras from Silicon Valley like fail fast and move fast and break things because they figured out the key to getting an idea rolling quickly is to just iterate as fast as possible another place we see this is from from the famous Toyota production system and their principle of Kaizen in other words Perpetual Improvement where the idea is you're constantly looking at your process your system and looking for opportunities for improvement no matter what step or what version of it you are on and in the case of making this video I did a lot of iterations so I actually posted these ideas initially on LinkedIn and then made a Tik Tok about them and posted them on Instagram then I refined the ideas a bit and and post it again on LinkedIn and then post it again on Tik Tok and so on and so forth and at the same time I've been developing these ideas for probably the last 10 months or so where I've had many iterations in my notes app more than I can count and then once I iterated several times I posted it on medium and then now I'm making this video as yet another iteration of expressing these ideas principle number six is hierarchy which means systems have multiple levels of abstraction ction and complexity a concrete example of this from nature is we have these things called molecules and when you get enough molecules hanging out together they form a cell and then if you get a bunch of cells together they form tissues and if you get a bunch of tissues together they form organs and if you piece together a bunch of organs in a particular way you get an organism each of these structures are simultaneously an element of something bigger and the combination of many smaller elements but of course we can naturally continue this hierarchy into the business world because organisms like us get together and form teams teams Clump together to form departments like HR sales or whatever and departments make up companies needless to say hierarchies are ubiquitous in the natural world and business world what might not be so obvious however is why this is helpful or valuable in the book thinking in systems the author danella Meadow raises a really good point which is that hierarchy simplifies complexity in other words it allows one to abstract away the finer details of a system that don't influence its larger behavior for example you don't need to understand chemistry to understand human psychology and human behavior similar L the CEO of a company doesn't need to know and understand each individual employee to successfully run the company but this kind of cuts both ways while this is beneficial from a high level perspective if you do understand chemistry that doesn't really help you in predicting these larger scale systems this phenomenon is what scientists call emergence and it's captured well by phrases like more is different or a system is more than the sum of its parts this idea highlights the importance of not just the elements of a system but their connections and how they're interacting with one another this is something that Jeffrey West talks about in his book scale and underlies his conclusion that a company doesn't necessarily scale with headcount but rather scales with the quality of relationships between the individuals of the company and the various teams the seventh and Fin final principle is symmetry this is something we see all over the place in nature the left side of my face looks like the right side of my face today's weather is similar to yesterday's weather however a special type of symmetry I want to highlight here is what's called self similarity examples of this in nature include branches and trees specifically branches resemble the trees that they grow from another example comes from our lungs where our bronchi these Branch like structures here look like our broni these larger Branch like structures another example that I really like comes from Roma lettuce where if you take a big piece of Roma lettuce and you start stripping away the leaves and you get down to that small tail of it it looks a lot like just a big head of romae lettuce to demonstrate this if we take this Stripped Away Roma lettuce and just make it larger it looks a lot like the original Roma lettuce that we started with so self-similarity is everywhere in nature similar to hierarchy the value of this type of symmetry is that it greatly reduces the amount of information needed to both describe and generate a system we can actually take this one step further there's a special type of self-similarity where the elements of a system not only resemble the larger system but they are exactly identical and we call these types of systems fra an application of this idea I use most often in my business comes again from Nim talb in his book antifragile where he describes how we can use fractals in Risk allocation he describes this so-called barbell strategy which is a way to approach diversification if you have two options a safe bet versus a risky bet for me the safe bed is making content about AI the risky bed is making content about other things things that I find interesting like entrepreneurship and this video and so the barbell strategy says I should make 80% of my content about the safe stuff the stuff I know people would be interested in and I should make 20% of my content about the risky stuff the stuff that I'm not sure that people will like where fractals come in is if we want to have a finer resolution for our risky bets we can zoom in to this 20% here and apply this 8020 barbell strategy once again so so if we split this 20% into 80% and 20% that would have us a lot 16% to a less risky bet and 4% to a more risky bet what this might be is that 16% of my content is about my entrepreneurship Journey which people seem to like but not like as much as the AI and data science content while 4% will be more risky and will be my content like this or the content I've made on personal development and philosophy that brings us to the end if you enjoyed this video and you want to learn more check out the medium blog associated with this video this idea of drawing wisdom and inspiration from nature is such a deep and Rich topic there's no way I could capture its full richness in just one video or blog article that's why if this is something that you've also been thinking about and exploring I invite you to drop a comment below sharing your own insights and experience and as always thank you so much for your time time and thanks for watching
wJ794jLP2Tw,2024-05-30T15:41:30.000000,Automating Data Pipelines with Python & GitHub Actions [Code Walkthrough],this is the sixth video in the larger series on full stack data science in this video I'm going to be talking about automating data pipelines I'll start with a highle overview of how we can do that and then dive into a concrete example using GitHub actions which gives us a free way to automate workflows in our GitHub repos and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoy this content please consider subscribing that's great no cost way you can support me in all the videos that I make so I'll start with a story about a friend of mine back in grad school so it seemed about every Friday to cope with our existence of being physics grad students me and many other grad students would find themselves at a bar close to campus and there was one grad student in particular who would show up after doing research for several hours and he would be drinking a beer and he'd say something like technically I'm working right now because my code is running and of course we would always laugh about it but this is a sentiment that I find a lot of data scientists and other developers share which is this sense of satisfaction when some piece of software that they wrote is off and running all on its own while they are off doing something else that they enjoy doing like having a beer with fellow grad students so this is at least one motivation toward automating data pipelines and other aspects of the machine learning pipeline here I'm going to talk at a high level about two ways we can automate data pipelines the first way is via an orchestration tool which is something I mentioned in the previous video on data Engineering in this series this includes things like airflow dagster Mage and many more the second way is what I call python plus triggers this is Python scripts that run given particular Criterion such as it's a particular time of the day or a file appears in a directory let's dive into each of these approaches the first way an orchestration tool so I'll start with airflow because from the dozens of interviews I've had with data engineers and ml Engineers it seems that this has emerged as a standard tool for many building data and machine learning pipelines one of the biggest benefits of airfow is that it can handle very complicated workflows that include hundreds and even thousands of tasks this is a major reason why this has become an industry standard among data Engineers managing and Enterprise scale data pipelines one downside of airflow as someone who was trying to learn it specifically for this series and this project that I've been working on is that it can be pretty complicated to set up and maintain these workflows and of course that comes with a steep learning curve these challenges of getting set up with airflow have created a market for airflow rappers such as prefect dagster Mage and Astro the upside of these wrappers is that you can tap into the power of airflow with potentially simpler setup in maintenance however one downside of these airflow wrappers is that this ease of use is often coupled with managed or paid services for these rappers however to my knowledge most of these have open-source versions if you want to go the self-managed route regardless of what orchestration tool you want to use for many machine learning applications where the there's a relatively simple data pipeline these tools may be Overkill and may over complicate your machine learning system which motivates the second approach to automating data pipelines using python plus triggers you can say that this is the old fashion way because before we had these orchestration tools if you wanted to build these data and machine learning workflows you'd have to build them from scratch so to make this a bit more concrete let's look at a specific example say we wanted to implement a very simple ETL pipeline consisting of one data source and one target database what this might look like is we pull data from a web Source we'll run an extraction process via a python file call it extract. py we'll transform the data in some way in a python script called transform. piy then we'll load it into our database using another script called load. piy so this is a simple ETL pipeline which is something I discussed in the previous video video in this series on data engineering given these three Python scripts there's nothing stopping us from creating yet another script that consolidates all these steps we can call that script ET L.P so now instead of running three scripts consecutively we can just run one script although this has streamlined the process this is still not automated because we still have to manually go to the command line and type python space L.P this is where the idea of a trigger comes in so let's say we wanted to run this ETL pipeline every single day one thing we could do is run a KRON job KRON is a command line tool which allows you to schedule the execution of some process so let's say we wanted to run our ET L.P file everyday at midnight we would type something like this into the command line and then that would get saved in our system as a Cron job and every day at midnight This would run but of course if we do this this would be running the Cron job on our local machine which may not be something that we want to do some Alternatives could be doing this process on a server that we manage or spinning up a cloud resource that runs this process but both of these options come with a bit of overhead in maintenance for setting up these compute resources so it would be great if we could set up this trigger to execute our ETL pipeline without the extra effort of setting up these compute resources this is where GitHub actions are super helpful gith GitHub actions is github's builtin cicd platform cicd stands for continuous integration and continuous delivery the typical schematic of cicd looks something like this where I suppose this is supposed to be some kind of infinite Loop of integrating new updates into your software system and delivering those updates in real time hearing this you might be thinking sha what does continually deploying code have to do with data pipelines well although data may not play a role in traditional software development when it comes to building machine Learning Systems data play a central role in the software development put another way when it comes to machine learning we use data to write programs to write algorithms which is manifested as a machine learning model so if we're talking about continuously integrating and delivering a machine learning application that will require us to continually integrate and up update the data that we feed into that system two key benefits of GitHub actions is that firstly the compute to run these workflows is provided for free for public repositories this is great for poor developers like me who just want to put out example code like for this video or for those who are building proof of Concepts or building projects for their portfolios and of course there are paid versions for Enterprises and businesses and the second thing is that we don't have to worry about setting up compute environments whether that's an on premise server or a cloud resource all the setup happens via writing a. yaml file with that highle overview let's walk through a concrete example of automating an ETL pipeline to turn YouTube video transcripts into text embeddings if you're unfamiliar with text embeddings I have a whole video where I talk about that but understanding what they are is not necessary for this example here the steps we're going to walk through are one we're going to create our ETL python script two we're going to create a new GitHub repo three we're going to write our workflow via the yaml file four we're going to add repo secrets to our GitHub repo this will be necessary to allow the GitHub action to automatically push code to the repo and also to make API calls to the YouTube API without exposing my secret API key and then finally we'll just push and commit all our code to the repo let's start with the first one creating our ETL python script and so here I have a fresh Visual Studio code I created a new folder and now I'm going to create a new file I'm going to call it data pipeline. pi and I've already pre-written the code here so I'll just paste it over and explain it one by one the first thing I'm going to do is some imports the first line here is we're going to import a set of functions from another python script called functions. piy which we'll write in a second and then we'll import the time module and the date time module the reason we want time and date time is that it'll allow us to print when this python script is run and how long each of the following steps took to run toward that end first I'm going to print the time that the pipeline is starting to run we can do that using the datetime module next I'll start importing the different steps of our data pipeline so the first step is the extraction the first thing it's going to do is extract the video IDs of every single video on my YouTube channel that whole process is baked into this get video IDs function which is going to be in this functions. piy script that will create in a second this is something I walked through in the previous video of the series on data engineering some other fanciness that's happening here is that I'm just capturing the time just before and just after this step is is run so I can print how long it took to run this function and this is helpful for debugging purposes and observability once we have the video IDs extracted we can use a python library to extract the transcript of each video given its video ID using similar process here where this whole get video transcripts process is abstracted as this function and we're capturing how long it took to run that step next I have this transform data function which is just doing some data cleaning so ensuring that the data have the proper types and handling any special character strings and then finally step four we're going to take the titles and the transcripts from all the YouTube videos and make the text embeddings with these files in place we'll go ahead and create another file and this will be the functions do Pi file and here we will put all the different steps that we just put into our data pipeline there's a lot here actually more functions than are used in the data pipelines script I'm not going to walk through this because I've talked about it in previous videos but of course this code is freely available on the GitHub so you can check that out if you're interested one thing I'll point out is that at each of these key steps in the data pipeline we're not outputting any data structures or data frames as intermediate steps the data are actually being saved to this data directory this is actually something we'll have to create so we'll create a folder called Data so that the files have somewhere to go one last thing you'll notice is that we're importing a ton of libraries here so I'll go ahead and create another file called requirements. text where we can put all the different requirements for this data pipe line polers is what I'm using to handle all the data frames YouTube transcript API is the python library that allows us to pull the YouTube transcripts given a video's video ID I'm using the sentence Transformers library to create the embeddings and then I'm using the requests library to make API calls to the YouTube API all right now that we've written all the code for our ETL pipeline let's create a fresh GitHub repo we can push the code to we can create a GitHub repo from the command line or from the web interface I'll do it from the web to do that you just go to your repositories Tab and you click new it will give it a name I'll call it data pipeline demo and I'll just say demo data pipeline via GitHub actions we'll keep it as a public repo one benefit of doing it as a public repo is that GitHub won't charge you for the compute costs of the GitHub actions I'll add a readme file I'll also add a giit ignore using the python template and then I'll choose a license I'll just do Apache 2.0 all right so we now have our repo and then what we'll want to do is we'll want to clone our repo open up our terminal to whatever folder we want to store this repo in and then we'll do get clone and then we'll download this newly created repo we can see we have the license and the read me now that we have our repo we can go ahead and write our yaml file which will Define the work flow that will automate the execution of our data pipeline okay so the next thing we want to do is add all the code we just wrote to this new repo we created so the code we wrote earlier I stored it in this data pipeline demo temp folder so what I'm going to do is just copy and paste that over we go back here hit LS we see that all those files are here and then what we want to do is create a new folder we'll call it GitHub and then in that GitHub folders we'll create a new folder called workflows the reason we do this is that GitHub will look in this GitHub workflows subdirectory for all the workflows that we want to run as GitHub actions with that directory created we can open up our GitHub repo locally so it's called Data pipeline demo we'll open that up and we can see all the code is here and our GitHub workflows folders here so what we wanted do is create a new file in this workflows folder we'll call it data pipeline. yml and here is where we'll Define our GitHub action there are a few key elements of a GitHub action the first is its name which we Define like this and if you're not familiar with yaml files they're essentially python dictionaries so they consist of these key value pairs that can additionally be nested so the simplest of these when it comes to GitHub actions is this one we wrote here which is the name of the workflow which I'm just calling data pipeline workflow the next element of the workflow is the trigger so this will Define when this workflow is run that's specified by this on syntax and from here we have a few different options we can make it so that this workflow runs anytime new code is pushed to the repository we just do that by writing this another thing we can do is have the option to manually trigger this workflow using this option which is workflow dispatch so we'll see later that setting this option a button will appear under our GitHub actions tab which will allow us to manually run this workflow but the one that we probably care most about is this option which allows us to schedule our workflow as a Cron job which is what we saw in an earlier slide to make this Aon job we simply type cron here and then we specify the arguments for the Cron job what I want is for this workflow to run every night at 12:35 a.m. so the Syntax for that looks like this and if you're not familiar with the syntax of running a Cron job there's a great website called cron tab. Guru it has a guide for scheduling Cron job so this is what we wrote in our emo file it's saying at 0035 it's going to run and it actually tells you the next time that this job would run alternatively we could do all all asteris which means this is going to run every minute however when it comes to GitHub actions 5 minutes is the fastest that it can run so you would write every fifth minute like this but I tried this and even if you specify to run every 5 minutes it still won't run that fast it'll run closer to every 15 minutes so there are some limitations on how quickly you can run a workflow using GitHub actions but here we don't want to do anything crazy and just running it once a day is fine because I'm posting videos every week so running this workflow every day is more than sufficient now that we have the name of our workflow and we' specified when it's going to run the next thing we want to do is Define the workflow itself workflows consist of jobs which have names so the names of the jobs are specified like this and then what we can do is specify what system this specific job will run on so we'll say auntu latest so this will just run on the latest version of Ubuntu which is a Linux operating system and then jobs will consist of steps which we can specify like this here we're going to have a handful of steps to run this whole workflow the first step we'll call it checkout repo content and we can actually make use of pre-built GitHub actions provided by GitHub this one is called checkout version 4 what this step of the workflow is doing is pulling all the code from our GitHub repo and if you're curious about this specific action we can Google it and then there's a repo on GitHub that has a lot more information about this action you can check this out if you like it's at github.com actions checkout then I'll do one more thing here and I'll add this with thing and I'm going to specify a token what I'm doing here is giving this workflow access to one of the repository Secrets which will give it read and write access since it's a public repo it doesn't need any special permissions to pull the code onto this auntu instance at spun up but later in the workflow we're going to push code to this repo which will require special access and so that's the reason I'm adding this token to give the action the proper permissions and we'll create this token in a sec and I'll show you how to make it a repository Secret okay so that's step one of our job all we did was create a fresh auntu instance and then pull the code from our repo next thing we're going to do is set up python I'll call this step of the workflow set up python I'll use another another pre-built action called setup python version 5 and similarly if you're curious about this you can just Ty it into Google and then you can pull up the repository for setup Python and it'll show you the basic use usage and have more information on that but it does exactly what it sounds like it does then we can add this with thing and specify the python version here I'll use 3.9 I'll also add this option which will cach the libraries that we install with Pip so it doesn't have to install these libraries from scratch every single time it runs the workflow it can just install it once and then it can reuse those libraries from Cache that second step we set up python now we'll do another step and we'll install the dependencies here we're going to use a different command so we'll use this run thing this is essentially like running a command from the command line so we'll just do pip install requirements. text this is just as if we opened up our terminal and typed this in the command line and the reason we can do that is because we've just installed python on the machine so now we can run pip on it with the libraries and installed we can now run the data pipeline so I'll Define another step call it run data pipeline I'll need to do one thing here which is import a environment variable called YouTube API key and this will be another repository secret that will Define later and what this is is my YouTube API key which is needed to run this function here the get video IDs so once we've done that we can now run our data pipeline so we do that like this so we'll use the Run command again and we just type python data pipeline. piy just like we were typing this into the command line now we've run this python script on this machine that just got spun up and then what we can do next is see if any changes were made to the git repository after running the data pipeline give it an ID and then we'll run another command I'll use this vertical bar syntax which will allow me to import mult multiple lines of commands but we can go through this one by one What's Happening Here is that first we're configuring the GitHub account then we're adding all the local changes what this line is doing is that it's checking the difference between the staged changes and the last commit and this quiet option will set the exit status of the command to zero if there are no changes and we'll set it to one if there are changes so if the exit status is one we'll create a new variable called changes and that'll be equal to true and then we'll store this in the GitHub environment what that allows us to do is do one last step we'll call it commit and push if changes what we can do is basically have an if statement so if changes so this environment variable that we just created equals true we'll run this command so again we'll use this vertical line to do a multi-line command and we'll commit the staged changes and then we'll push it to the repository this is where we need the special permissions from the personal access token that we defined earlier and then one last thing we need to do is add a file to the data folder because since it's empty GitHub won't actually push any empty folders to the repo so we should just have a file in here and then we can just say something like data go here and with that we've created our workflow in this data pipeline. yaml file we have our data pipeline here we have all the functions that make it up here we have our requirements file with all the libraries that are needed to run our data Pipeline and we have a data folder in which we can store all the data produced from our data pipeline in this example we can get away with storing the data on GitHub because it's super small it's not going to be more than 100 megabytes or so but if you're working with data sets much bigger than that talking about gigabytes or even more in that case you probably want to store that in a remote data store and those are changes you would just make in your data pipeline itself for example instead of reading and writing to a local directory you would read and write to an S3 bucket or to Google Drive or to a database so that'll just depend on your specific use case before we can push all these local changes to our GitHub repository we need to create two secret variables that are available to to the GitHub actions in order for this pipeline to run successfully and so to create a repository secret we'll go over to settings we'll scroll down to secrets and variables and we'll click actions we'll see a screen like this and we'll see this repository Secrets section this will allow us to create repository secrets that will be accessible to our GitHub actions the first one we want to create was this personal access token but we need to actually create it personal access token for this and so in order to do that I'll click on my profile here scroll down to settings open that in new tab and then I'll scroll down to developer settings click on personal access tokens and then I'll click on tokens classic and you can see that I've already created some personal access tokens but what I'll do is create a new one it asks for a note so I'll call this data pipeline demo P expiration we'll just leave at 30 days and then we can select the scope so we only need this repo scope here and the reason is we just want our GitHub action to be able to push code to our public repo it has read and write access and then we can actually leave everything else unchecked to do that we'll just hit generate token and then this token will appear you shouldn't share this token with anyone I'm sharing it with you because I'm going to delete it right after this demo but we can copy that and then we'll come back over here and we'll paste that into our secret now this personal access ACC token will be accessible as an environment variable to our data workflow we just hit add secret and then I'll do a similar thing for my YouTube API key this I will not share but if you're importing your own YouTube API key or any kind of API key it's important to just paste the RW string and not put quotations around the API key I'll just add that and hit add secret now we have two repository Secrets here the personal access token and the YouTube API key so now with the secrets in place we can commit and push all our changes to the GitHub repo and watch the workflow run at our local changes we'll commit our local changes first push and then we can push all the local changes to our before we do that let's add one thing to our git ignore file if you're on Mac you'll notice that your GitHub repos will always add this DS store thing I don't want that so I'm going to include that in the git ignore file and we'll do get push now we pushed it to the repo and I guess I didn't properly remove this so I'll just go ahead and manually delete this so we see all our code has been pushed to the repo we can see that this little pending dot is appearing so if we click that we can see that our workflow is running and so to watch that we can click on the actions tab if we click on this and go to run data pipeline we can see that run data pipeline was the name of our job in the data pipeline. yo file and then these were all the steps that we defined so we'd setup job check out repo content set up python install dependencies and then we have run data pipeline check for changes commit and push if there are changes and then these steps are automatically generated from the pre-built actions of setup Python and the checkout action so we'll wait for these dependencies to install all right so the dependencies install took about 2 minutes now it's going to run the data pipeline so this will actually take longer than the dependenc because what's happening is we're making the API calls to the YouTube API to grab all the video IDs and there's another step of grabbing all the transcripts for each YouTube video and then finally we have to generate the text embeddings for those transcripts and the title of those YouTube videos so this might take a few minutes we can see that everything we printed in our data pipeline is showing up here and then we're checking for changes and they word changes so it's pushing the code but it failed and it's probably because I deleted that DS store after this workflow got kicked off this is a great opportunity to actually go to our workflow and use the manual trigger so this was that dispatch workflow option that we created so we can actually run this manually like this and so once we click that we can see that now it's running again now we go through that whole process all over again all right so this time we see the data pipeline ran successfully and it committed and pushed the changes so we can see that it added these three files and then it's going to going to just do some post setup of python and then post checkout of repo content while that's finishing up we go to our data folder and we can see that the data from our workflow is here now these files can be used in some Downstream task Okay so we've successfully automated the data pipeline the next natural step is to integrate this automated data pipeline into the final machine learning application I actually do that in this repo here which I'll also Link in the description below so we can just click that and we can see that we have our workflows folder here and we have our data pipeline. yamof file so this is the same thing that we just wrote and then we have our data pipeline here and so these were the files that we defined earlier and the rest of this is similar to what we saw in the previous video of this series where we created a search API endpoint using fast API and Docker however here instead of deploying this API endpoint to AWS I deployed it on Google Cloud platform specifically using the cloud run service and the reason I did that is because they have a free tier also they support continuous deployment so anytime A change is made to this GitHub repo it'll redeploy the API running on Google cloud and then the front end is publicly available and I'm hosting it on hugging face I'll also put this in the description below but this is now live so you can go and see the fruits of all the labor of this video series the first run is going to be slow because it has to wake up the container on Google Cloud but eventually this spins up search results so if I type in llms it'll bring up all the videos on llms I can do something else like data freelancing and then it shouldn't take as long for the second run cuz the container's already awake and we get the results for data freelancing and then we can of course do this series on full stack data science and see the search results came out much faster the container's now awake you can see all the different videos relevant to full stack data science well that brings us to the end of the series we've come a long way talking about the four hats of a full stack data scientist being that of a project manager a data engineer a data scientist and an ml engineer if you're curious to learn more about fullstack data science or the details behind this semantic search app check out the other videos in the series and as always thank you so much for your time and thanks for watching
pJ_nCklQ65w,2024-05-18T15:24:22.000000,"How to Deploy ML Solutions with FastAPI, Docker, & AWS",this is the fifth video in a larger series on full stack data science in the previous video I walked through the development of a modelbased Search tool for my YouTube videos here I'm going to discuss how we can take this tool and deploy it into a production environment I'll start with an overview of key Concepts and then dive into the example code and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoy this video please consider subscribing that's a great no cost way you can support me in all the videos that I make when we think of machine learning we probably think neural networks or other models that allow us to make predictions although these are a core part of the field a machine learning model on its own isn't something that provides a whole lot of value in virtually all situations in order for a machine learning model to provide value it needs to be deployed into the real world I Define this deployment process as taking a machine learning model and turning it into a machine learning solution we start by developing the model which consists of taking data passing it into a machine learning algorithm and obtaining a model from that training process deployment can look a lot of different ways it could simply be making predictions available to programmers and other developers it could be using the model to power a website or a mobile application and then finally it could be embedding the model into a larger business process or piece of software but the key Point here is that the model that comes out of the training algorithm and is sitting on your laptop doesn't provide a whole lot of value however the model when integrated into a website into a piece of software or made available to end users through an API is something that provides value natural question is how can we deploy these Solutions while there are countless ways to do this in this video I'm going to talk about a simple three-step strategy for deployment that is popular among data scientists and machine learning Engineers the first step is to create an API in other words we create an interface for programs to communicate and interact with our model what this looks like is we take our model and we wrap it in this API which is represented by this box here and then people can send requests to the API and receive responses from it and so in the case of a model the request will be the inputs of the model and the response will be the outputs two popular libraries for doing this in Python are flask and fast API the next step is to take the API and put it in a container here container is a technical word referring to a Docker container which is a lightweight wrapper around a piece of software that captures all its dependencies and makes it super portable so you can easily run that piece of software across multiple machines and then finally we deploy the solution and since we put everything into a container now it's super easy to run that container on someone else's computer some server you manage or most commonly into the cloud The Big Three Cloud providers of course are AWS Azure and gcp so with this high level overview let's see what this looks like in code here I'm going to walk through how we can use fast API Docker and AWS to deploy this semantic Search tool that I developed in the previous video this video is going to be pretty Hands-On so I'm not going to talk too much about fast API Docker or AWS from a conceptual point of view but if those are things you're interested in let me know in the comments and then I'll make some follow-up videos specifically about those tools okay so here we're going to create a search API with fast API then we're going to create a Docker image for that API then we'll push that image to the docker Hub and then finally we'll use that Docker image to deploy a container on aws's elastic container service so let's start with the first one creating the search API with fast API which is this python library that does all these great things apparently and using it to write this example here it was super easy for me to learn and they have this great tutorial for those just getting started I walked through this to make this example and it probably took me like an hour or something to do it so super easy to learn especially if you've been coding in Python for a while anyway coming back to the code first thing we want to do is we're going to make a file called main.py and we're going to import some libraries so we'll import fast API to create the API and then the rest of these libraries are so we can Implement our search function like we did in the previous video so we use polers to import data about all my YouTube videos we use the sentence Transformers library to compute text embeddings we use psyit learn to compute the distance between a users query in all the videos on my channel then I have this other file called functions. piy that has this return search results function which I Define here and I'm not going to go into the details CU it's not critical for the deployment process but essentially what it does is that it takes in a user's query and then it'll spit out the top search results for that query coming back to the main script the first thing we do is I'll Define the embedding model that we're going to use from the sentence Transformers Library so by default the library will download the model when it's Run for the first time but here to avoid that I just save the model locally so what that looks like is here we have the main python file that we were just looking at and then I have this folder called data and in it I have this folder which contains all the files for this embedding model and then we have this parquet file which has all the data about my YouTube videos we can then load the model from file like this we can load the video index using this line of code and this is the same way we did it in the previous video and then we can import the Manhattan distance from sklearn which I did like this and again since I talked about this at length in the previous video I'm not going to get into the details of how the Search tool works here but you can check out that video if you're interested so everything we just did had nothing to do with the API this was just the implementation of that search function to create the API we create this object called app and and it's this fast API object and then we can simply create these API operations here I'm strictly defining these get requests which allows users to send requests to the API and receive back responses the other most common one is a put request which is often used to send data to an API and load it in the back end for example if we wanted to update this parquet file in some way we could use a put request to do that anyway here I Define three operations and that's done using this syntax here where we have this decorator What's Happening Here is we're saying that we wanted to find this get request at this end point here for the API and it's going to operate based on this python function this is a common practice where you have the root end point be a health check so it doesn't take in any input parameters but anytime someone calls this m point they'll receive back this string response of health check okay and a similar thing here so I created another endpoint called info which just gives some information about the API so it doesn't take any inputs but it returns back the name which I called YT search and I have a description for it which is search API for shots Levy's YouTube videos this one's not completely necessary but you can imagine if you have multiple users using this API and maybe you have multiple apis having an info endpoint can be helpful but the one we care most about is this search endpoint What's Happening Here is we're defining this search function that takes in a query from the get request and then it'll pass it into this function return search result indexes defined in this functions. piy file it'll pass it in as well as the video index the embedding model and the distance metric and then we can use the output of this function to return the search results and so a lot of fanciness here maybe it's not super easy to read but what's happening is we use the select method to pick out the title and video Columns of the data frame the video index we use this collect method because we didn't actually load the data frame into memory because we used scan paret instead of read paret and then once we load this in we pick out the indexes from this search result and then finally we convert that data frame to a dictionary that dictionary will have two Fields one corresponding to the title and the other field corresponding to the video IDs and it'll have up to five search results for each field that's the code to make the API was super easy it's great for me as someone who's very comfortable with python and knows very little about a lot of other programming languages especially ones that have to do with web development but now we can run this API on my local machine and we can interact with it so the way that looks is make sure we're in the right direction we see we have this app folder and then we can go into app and we see that we have our main.py file to run that we can do fast API Dev main.py all right so now it's running running on this port 8000 had to make a couple of changes to the main file so I had to add this app in front of functions and then I had to remove app from these paths here because it was running from a different directory than a previous version of the code but that should be working now now we'll see that we have this little URL here which we can copy and then I have a notebook here that allows us to test the API the URL is already here and it's at Port 8000 by default and then we want to talk to the search endpoint of the API that we created so we can actually run this you have this query called text embeddings simply explain and then we can pass that into our API and then we can see we get a response so it took about 1 second so that's actually pretty long long but maybe if I run it again it'll be faster yeah so maybe that first one is just slow but run it a second time 76 milliseconds and then we can see the search results here just kind of taking a step back the response in its raw form looks like this so it's just a text in the Json format which is basically a dictionary and then we can use this Json library to convert that text into a proper python dictionary and then we can access the different fields of it like this so these are all the titles from the top five search results and then we can also look at the video IDs so that looks like that now we've confirmed the API is working locally so coming back to the slides the next thing we want to do is to create a Docker image for the API the steps to make a Docker image from a fast API API is available on their documentation and it's a few simple steps we'll create a app directory so a folder called app we'll create an empty init.py file and then we'll create our main.py file we've actually already done this if we go back we see that the app directory already exists and we have the main.py file and we already have the init.py file taking one step out of that directory we see that this app folder is in another folder with a few other files so we have this requirements. text file which is shown here this is just your typical requirements file that you might have for any kind of python code here you can see we have all the different libraries we used in the main.py file so we have fast API polers sentence Transformers psychic learn and numpy we also have this Docker file which is essentially the instructions for creating the docker image this consists of a few key steps we start by importing a base image there are hundreds of thousands of Docker images available on the docker Hub the one we're importing here is the official python image version 3.10 and so we can see that on the docker Hub here it's an official image so I guess it's by Docker it's called Python and then they're all these tags so these are all different versions of this image we did 3.10 which I guess is going to be this one the next thing we're going to do is change the working directory you know imagine you just installed Linux on a machine or something so the working directory is just going to start as the root and then we can change the working directory to this folder called code next we can copy in the requirements file into the docker image so we take the requirements file from our local directory here and put it onto the images directory this code directory here and then once we've moved the requirements file onto the image we'll install all the requirements we do that with this line of code here and then I have this line of code to add this code app to the python path this might not be necessary because we actually changed this main.py file so I'm going to actually try to comment this out and see if it still works next we're going to add this app directory to the image so we're going to move it from our local machine to this code subdirectory on the docker image finally we Define a command that will be run automatically whenever the container is spun up to build the docker image we run docker build we specify the tag so we'll give it a name I'll call it YT search image test and then oh I forgot this we got to specify where the docker file is so it's going to be in the current directory so we do that and now it's building the docker image okay so now the image is done building you can see it took about maybe a minute to run the run times are here the longest was installing all the python libraries and so now if we go over to the docker desktop app we see that this image is here under the images tab so I have a previous version of the image but the one we just created is called YT search image- test and then we can actually run this image the way to do that is we can go let me clear it out so we do Docker run and then we specify the container name I'll put YT search container test and then we specify the port we want it to run at do 8080 yep cuz that's what I put here in the docker file finally we'll specify the image here's the code Docker run- d-- name of the docker container and then we specify the port and then specify the image so we can run that that's not the right image name it is YT search image test now the container is running locally we can actually see that if we go to our image here and it says in use this is the container that is using that image alternatively we can just go to this containers Tab and we can see all the containers saved locally here we can see that the container stopped running and that means something went wrong and we can see that the folder with the model in it is not a local folder so it didn't run because the model folder wasn't on the python path we could add the data subdirectory to the python path but alternatively we can just go back to the main.py file and add app to these directory names here and the reason we need this is that we Define the working directory as code all the code is going to run relative to this directory here that means if the python script is looking for this model path you have to put app here because it's running from code alternatively you could add this python path thing here but I don't want to do that to make this Docker file more simple now let's try to run it again so we'll build the image so that was nice and quick and then we'll run the container so now the container is running click this and indeed it is running successfully so we can click on it and we can see that it's running at this URL here we can test this if we go over to the super notebook we test the API locally so now let's test the docker container running locally so this should be the correct path so yep and so it's the same thing we have the URL and then we have the endpoint name we're going to use the search operation we'll Define a query and then we'll make a API call it ran slower because it's essentially talking to a different machine but we can see that the API response is the same we can similarly call the info endpoint or we can call the base endpoint as well we can see those generate different responses now that we've created the docker image next we're going to push the image to the docker Hub the reason we want to do this is that once the image is on the docker Hub it makes it easy to deploy to any cloud service that you like so here specifically we'll be using aws's elastic container service but dockerhub integrates with other Cloud providers so not just AWS but also gcp I'm sure it also connects with Azure even though that's not something I've checked to push the image to the docker Hub the first thing we want to do is create a new repository so I already have one called YT search but let's go ahead and create a new one from scratch so we'll call this one YT search demo and then we'll say demo of deploy deploying semantic search for YouTube videos and then we'll leave it as public and we'll hit create reposit doesn't have a category so let's just do that I'll call it machine learning AI so that's it the repository is made now what we can do is we can go back to our terminal we can actually list out the docker images like this we can see that these are all the images that I have saved locally what we want to do is push this one to the Docker hub first thing we need to do is Tag the image so it matches the repository name on the docker Hub essentially we're going to create a new image and this is going to have the same name as that repository we just created if we go back here we see that repo is called shahin which is my dockerhub username and then the name of the repo we just created YT search- demo and then the next thing we need to put is the name of the local image so here we had YT search image- test so I actually have it backwards we need to put the local image name first so YT search image- test and then we need to put the dockerhub repo name we've now created a new image which we can see here called shahin t/t search- demo and now we can just push it to the docker Hub so that's really easy so Docker push shahen T YT search demo and then we can add a tag to it as well but that's not not necessary so it's using the default tag of latest and now we can see that it's pushing up to the docker Hub so now it's done running if we go back to the docker Hub and hit refresh we see now the image is here now that we've pushed the image to the dockerhub the last step is we can now deploy a container on AWS using their elastic container service the way to do that is we can go to our AWS account if you don't have a AWS account you'll need to make one for this tutorial and then we can go to elastic container service so we can just type in ECS and it should pop up once we do that we'll see we come to a screen like this we can see that I already have a cluster running but let's start one from scratch the first thing we can do is go over to task definitions and click this create new task definition I'll call this one YT search demo we'll scroll down to infrastructure requirements we'll select AWS fargate as opposed to Amazon ec2 instances and the upside of fargate is that you don't have to worry about managing the infrastructure yourself that's all handled behind the scenes and you can just worry about getting your container running and using it as a surfice the next important thing is selecting the operating system and architecture this will depend on the system that you're running for Mac they use Arm 64 so that's the architecture and then Linux is the operating system of our image next we can go to the task size I'll leave it at one CPU but I'll actually bump down the memory to 2 gb and then task roll you can actually leave this as none if this is your first time running it and it'll automatically create this task roll called ECS task execution rle but since that already exists for me I'll go ahead and click that now we're going to specify the container details so I'll call this YT search container demo and then here we'll put the URL of the image which we grab from the docker Hub so I'll grab this and then I'll add the tag of latest and we'll leave it as a essential container port number we'll leave at 80 we'll leave all this as the same we'll leave all this stuff as default we won't add any environment variables we won't add any environment files and then logging leave that all as the default and then we have a bunch of these optional things that we can set like a health check startup dependency ordering container type timeouts so on and so forth we can also configure the storage so there's ephemeral storage so just like shortterm I'll just leave this as the default of 21 we can also add external storage using this add volumes thing which is good if you wanted to talk to some external data source and there's this monitoring Tab and tags tab but not going to touch any of that just going to keep it super simple here and then I'm going to hit create all right so now the task definition has been successfully created now we can go here and we'll see we have this new task definition now we can go over to clusters and we'll hit create cluster I'll call this one YT search cluster demo again we'll use AWS fargate for the infrastructure and then we won't touch the monitoring in the tags hit create now it's spinning up the cluster so this might take a bit so now the cluster has been created we can click on this so we see that the cluster is running but there's nothing running on it there are a few things we can do we can create services or we can create tasks services are good for the we service kind of like this API we're creating a task is better for something that's more of a batch process that runs like once at a predictable time increment but here we'll create a service so to do that we'll click services and then click this create button we're going to use the existing cluster the Wht search cluster demo click on launch type and we'll leave that as fargate and latest we'll make the application type a service we'll specify the family of the task definition and then we can give a service name call it YouTube search API demo we'll leave the service type as replica we'll have the desired tasks as one deployment options we'll leave those as default deployment failure detection leave that as default we won't do service connect I'm not sure what that is service Discovery networking we'll actually leave all this the same we'll use an existing Security Group and then we can enable load balancing if we like but I won't do that here service auto scaling we can automatically increase the number of containers that are running or decrease the number of containers that are running again we can configure the data and what not but we're not going to touch any of that and we'll just hit create so now it's deploying the search API so the API has been successfully deployed it took like 5 minutes or something but now if we scroll down and we click this YouTube search API demo something like this will pop up and we can go over to tasks and we can click this task here and we can see that it'll have a public IP address so what we can do is copy this public IP and then I have a another piece of code here and we'll just paste in the public IP and then we'll make an API call so just 100 milliseconds to make the API call it's actually faster making the API call to AWS than locally which is pretty interesting so this ran just fine here but one thing I had to do yesterday to get this working was go to the YouTube search API demo click on configuration and networking and then go down to security groups that'll open this VPC dashboard thing and I had to add this rule that allowed all inbound traffic from my IP address specifically so if you do that it'll you know have some default Security Group and then you'll hit edit inbound rules and then you can add a additional rule that allows all inbound traffic from my IP you can also have custom IPS which you specify one by one you can have any IP version 4 or any IP version 6 so it wasn't working for me but once I added this inbound rule it was working just fine now that the API is deployed on AWS it makes it a lot easier to integrate this functionality this Search tool into a wide range of applications to demonstrate that I'm going to spin up a gradio user interface that can talk to the API I'll just run this whole thing and this is essentially the same thing that I walked through in the previous video of the series so if you're curious about the details be sure to check that out but now we can see that this user interface got spun up we can search something like full stack data science and we see that search results are coming up this is the great thing about running the core functionality on AWS now we just have this lightweight front end that can interact with the API and return search results through a web interface so you can see that the other videos in this series are popping up in the search results we can search other things like finetuning language models and I had a typo but it doesn't matter and we can see all the content on fine tuning and large language models pops up and I'll just call out that all the code I walked through is freely available on GitHub so if you go to my YouTube blog repository and the full stack data science subfolder you'll see that all this code is available in this ml engineering folder and then you can check out other videos in this series and all the medium articles associated with this series this was supposed to be the last video of this series but then I got a comment from cool worship 6704 on my video on building the data pipeline for this project and they were asking how would you automate this entire process and so that's a really good question and it wasn't something I originally was going to cover but since you have this question here I assume other people have the same question and so just to recap what we did here is we took the Search tool wrapped it in an API put that into a Docker container and deployed it onto AWS so now users and applications can interact with the Search tool but one limitation of how I coded things here is that the video index the videos that are available in the search API is static it's a snapshot from a couple of weeks ago when I made the video on making data pipelines so the OB vious Next Step here would be to create another container service that automates the whole data Pipeline on some sort of time Cadence whether it's every night or every week or whatever it might be and then feed the results of that process and update the search API so that new videos will be populated in the search tool so that's going to be the focus of the next video of this series so that brings us to the end this video was a lot more Hands-On than a lot of my other content I'm experimenting with new new format so let me know what you thought in the comment section below if you want me to dig deeper into any of the tools or Technologies discussed in this video let me know and I can make follow-up videos on those topics and as always thank you so much for your time and thanks for watching
6qCrvlHRhcM,2024-05-11T15:00:08.000000,How to Build ML Solutions (w/ Python Code Walkthrough),this is the fourth video in a larger series on full stack data science in the previous video of the series I discussed how we can make data pipelines for machine learning projects here I'll discuss the next stage in the ml pipeline which is how we can use data to build AI Solutions I'll start with a highlevel overview and then dive into a Hands-On example with python code and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoyed this content please consider subscribing that's a great no cost way you can support me in all the videos that I make although we can draw parallels between traditional software development and machine learning development there are several key differences that are important to keep in mind the first and most fundamental is that in traditional software development the rules and the logic that make up the program are explicitly written into the computer by the programmer however when it comes to machine learning computers aren't told what to do explicitly but rather the rules or the instructions of the program are learned from data directly while this allows us to build ml solutions for things we could never write traditional software for such as text generation or autonomous driving this indirect way of programming computers gives rise to a few other key differences for one the behavior of traditional software systems are typically predictable in other words given any input for a traditional Software System you can typically know what the output is going to be on the other hand the behavior of machine Learning Systems is a bit more unpredictable you don't always know how the system will react to particular edge cases no matter how many tests you come up with to evaluate your system there will always be examples that you can't take into consideration because there are an infinite number of them another key difference is that traditional software systems are usually interpretable meaning you can usually have an intuitive understanding of how a software system took any given input and generated a specific output on the other hand machine learning systems are often uninterpretable or at least they're not interpretable in the same way that traditional software systems are so even though a machine Learning System can often generate better performance than a traditional Software System that often comes at the cost of interpretability and then finally traditional software development typically has a linear development cycle or at least a clear development cycle in other words projects can progress in a predictable manner on the other hand developing machine Learning Systems is often iterative and progress might be made in a nonlinear type of way while these differences create several Downstream consequences and how we should think about machine learning development as opposed to traditional software development the main thing I want to focus on here is the role of experimentation the way I see it this is what makes data science closer to something a scientist might do rather than an engineer more specifically scientists typically have hypotheses that they'll test against experiments while Engineers are typically implementing a given design of course it's not always this black and white in practice but experimenting with multiple potential Solutions is a key role of a data scientist so what this typically looks like is represented by this flow chart here so what this is representing is that we have the real world which is full of things that are happening some things we care about some things we don't care about what we do when we want to build ml Solutions is we collect data about some of the things that we care about in the real world and then we make that data available so that we can develop a machine learning solution with it once we have a candidate solution we can evaluate the efficacy or the value of that solution typically this results in a set of feedback loops so you might evaluate a solution see that the performance isn't so great so you go back and you tweak some parameters and you evaluate it again and then you tweak some more parameters and you keep going in this feedback loop and of course this Loop might even be automated you may exhaustively search a bunch of different parameters and still not get the results that you want so you decide to go back and change the data set that you're using for your solution tion development and perhaps repeat this whole process finally you might realize that the data that you have available isn't sufficient to develop Your solution so you go back to the real world and you re-evaluate the data that you need this is why developing ml Solutions is iterative and often nonlinear because you might go through hundreds of iterations of your solution before finally realizing that you weren't collecting sufficient data and then once you grab one key variable for example and pass it into your model you find that you finally get the performance that you need and the value is generated so to make this a bit more concrete let's look at a specific example let's say we wanted to develop a semantic search system this is something I've talked about in a couple previous videos of the series including the one on Rag and the one on text embeddings but if you're not familiar with semantic search the basic idea is that we start with a set of documents and then we take these documents and we generate numerical representations of them which we call text embeddings then what we can do is develop a Search tool where a user can type in a query we can generate a numerical representation of this query and then we can evaluate which documents are closest to the user's query and return them as search results and it's called semantic search because rather than using specific keywords in the user's query the meaning of the query and the meaning of the documents are captured by these numerical representations since I have a video All About text edings I won't go into the details here but I'll link that video in case you want to learn more while this might seem like a pretty straightforward idea take documents generate text embeddings and then do some kind of similarity score between the query and all the different documents there are several design choices that come up When developing this system them for example given documents documents have a lot of text in them so what text do we want to use for example if these are blog articles do we just want to use the title do we just want to use the first paragraph of the blog do we want to use the entire blog another one is should we chunk the text if we're talking about a Blog it could include a lot of different information where one paragraph is relevant to a potential users's query but the rest of the document is irrelevant this may result in our semantic surge to be a crude approximation of the underlying information in the documents another question is should we summarize the text you have a long document maybe you want to summarize it just so you capture the key information before passing it into an embedding model but of course there's more what embedding model do you want to choose there are several readily available models both open source models and closed Source models also should we embed multiple parts of a document so if you have an article again do you want to embed the title and the B body of the document separately and then maybe combine them in some way and then talking about the Search tool like how do you want to measure the distance between a query and all the different documents how should we filter results you have millions of documents it might be a good idea to narrow down the candidates before applying the semantic search because it's a bit more computationally expensive and then should we use meta tags you want to add tags to documents to help with this filtering process so all that to say there are countless design choices that come up When developing any machine learning solution and even everything I discussed here is far from an exhaustive list so to make this even more concrete let's look at a real world example of building a semantic search system here I'm going to walk through a project that I'm currently building to perform semantic search over all of my YouTube videos and this project has been the focus of this larger Series where in the previous video we built the data pipeline for this project we started with the data source which was the YouTube API we saw how we can build a data pipeline for this project I extracted information about all my YouTube videos from the YouTube API I did some light Transformations and then I loaded them into a data store specifically a paret file in this video I'm going to walk through the experimentation piece of building this semantic Search tool so we're going to take that paret file file which includes things like the video's ID its title and transcript we're going to generate text embeddings and then we'll build a Search tool with a user interface and here there are a few design choices that I will experiment with specifically whether we should base the search on the video's title its transcripts or both picking an embedding model from three op- Source options and then finally defining the metric or how we're going to define the similarity between the query and all the different videos and there will be five options of that looking through this if we have three options time three options Time 5 options these are 45 different options for this semantic search system and of course these aren't things that we're going to hardcode one by one I'll show how we can automatically generate all of these Solutions and objectively compare them to one another using an evaluation metric with that highle overview of what we're going to do I'm going to jump into the code which is available on the GitHub linked here and I'll also put it in the description and comment section below so before jumping into the code let's just see what the final product looks like by the end of this we'll have a user interface like this where we can type in a query and then it'll spit out responses the formatting doesn't look great cuz it's just a PC but we can see if I type in something like llm it'll return a bunch of videos from my channel as well as links to them so that's pretty cool and then we can search something else what are fat tails and then we go we get all my videos on fat tailedness let's see how can I build a semantic search system all right so this is the perfect video to return cuz I literally walk through it in this video we'll come back to this and play around with it a bit more but anyway I'm going to walk through three different notebooks all available on the GitHub repository the first one is going to be the experimentation piece where we're going to Loop through all all 45 different options and compare them all to each other using an evaluation metric once we figured out which of the 45 options is best we'll create a video index based on that configuration and then finally we'll write the search function and create the user interface starting from the top first I import polers which helps us handle the data structures and polers if you're unfamiliar is basically like pandas but it's much faster and is gaining popularity rapidly the project was a good excuse for me to try out polers and so far I've enjoyed the experience then we import sentence Transformers which has a handful of open-source text embedding models we can use and then we import some distance metrics from sklearn the distance metrics will allow us to evaluate how similar a user's query is to each video in the data set we'll import numpy to work with the matrices that we get from the search function and then I import M plot lib which I may or may not use but this is a great thing to have whenever you're doing any sort of experimentation of machine learning models so you can plot things like histograms and Scatter Plots to compare the performance of different solutions first we load the data like any other machine learning project the way I do it here is I have two data sets one is a data set of the transcripts saved in video- transcripts. par it's a data set containing all of my YouTube videos and YouTube shorts so has all my video IDs the dates they were posted the title of the content and the transcript this is just the head we can also look at the shape so I have 83 videos very small data set by ml standards but it took a long time to make those 83 videos next we have this evaluation data set which consists of two columns one is example query and the other is the ground truth video associated with that query the point of this evaluation data set is to give us a way to objectively compare multiple potential solutions to one another so whether you're training a model from scratch or you're using a model off the shelf like we're doing in this example you need to have an evaluation data set so you can effectively compare multiple candidate Solutions together we can also look at the shape of this data set and so we see we have 64 examples next I'm doing some data preparation what I'm doing here is I'm going to l Loop through each title and transcript in the original data frame so each of these titles and each of these transcripts and I'm going to Loop through three different embedding models available in the sentence Transformers Library so two different columns with three different models gives us six possible configurations in this chunk of code I Loop through every possible combination so you'll have the title with these three models and then you'll have the transcript with these three models so six possible combinations I'll Loop through each one and generate the embedding so what that looks like is a nested for Loop so I have a for Loop for the model name and I have a for Loop for the column names I'm going to store everything in a dictionary so I initialize that here and now just walking through this code first we Define the embedding model that we want to use we set model equal to sentence Transformers model name and then once we have the model we can generate an embedding for a particular column here I Define a key so we have a unique identifier for each element in the dictionary and then in this line of code I'll use the model to generate the text embeddings for every piece of text in that column for example if we're encoding the title this will take the title column of the data frame convert it to a list and then pass it into this encode function and spit out a array of all the embeddings finally we'll store the key name and embedding array in in the dictionary so the key name is just going to be a unique ID it'll be the model name with the column name and then we'll have the embedding array for that combination if we look at the embedding array that's going to be 83 by 768 so we have 83 videos and then the text embedding has 768 Dimensions so that's where this number comes from and of course each embedding model will be different another thing we can look at is this text embedding dictionary view of that we'll see that we have the model Name appended by the column that we're embedding and then we'll have a numpy array with all the numbers associated with each text embedding so if we look at this one specifically we see it's a numpy array and then we can look at its shape and then we see this one is 83x 384 notice that different embedding models will have different embedding Dimensions so this one is actually smaller than the other one which would have been this small model yeah so this model has 768 while the other one has 364 or whatever it was I already forgot going back to this time function this is really handy when it comes to doing these experiments because it'll automatically spit out the time it took to run this line of code here this is helpful because it can allow us to get a rough idea of the computational cost of each of these configurations so we can see that generating embeddings for the transcripts tends to take longer than for just the titles with this case being an exception maybe there's some kind of startup cost with running the first one and then these models tend to have different costs associated with them and the reason is that they actually get bigger and bigger another thing I'll share is that if we go to the sentence Transformers documentation they have a handful of pre-trained models here let's see all mini LM six yeah okay so this is one that we're using it's actually the smallest one and we can see that it's 80 megab while the largest one that we're using multi QA mpet the largest one we're using is more than five times as large at 420 megabytes so these are all important things to take into consideration not just the performance of the solution but the computational cost associated with it because that plays a role as well and then another thing going back this code might be difficult to read or seem a little complicated because we have these nested for loops and we don't really know the model names and column names they're stored in this list here some may have the inclination to want to hardcode all of these things for example just taking this line of code of defining the model name and then this line of code of generating the embedding array and then copy pasting something like this we'll take the model name embedding array doing this and then tweaking it and then repeating that for this and then so on and so forth while in a sense this might be simpler when it comes to doing experimentation across multiple potential Solutions this is an absolute nightmare because say you take this to your team or you read an article talking about how great this other model is if you wanted to go back and change your code it's a lot to keep track of cuz now you got to change it here and then maybe two cells down you use the model name again and then you got to think about keeping track of this and then if you're copy pasting inputs like this you're bound to make a typo and then it's going to cause issues down the line that is the number one reason why I could not recommend enough to write your code something like this where you have somewhere where you basically Define all the different options that you're trying to play with and then just let the code run its magic below and print out all the results that you need to see manually going in and tweaking code blocks here is going to inevitably lead to errors and this is just something I learned the hard way in grad school where I would train a model present it to the research group and they're like oh that's amazing but what if you tweaked this and what if you tried this and then I'm like oh okay so I'd go back but then my code wasn't written like this a lot of manual tweaking and then I would mess things up and things would stop running then I would finally get it working and take it back to the group and then they would come up with some other suggestions and so writing it this way allows you to iterate much faster and helps you avoid a lot of headaches that was a bit of a lecture there but it's super important next block of code basically doing the same thing but instead of embedding the titles and the transcripts for each YouTube video doing it for each of the queries in the evaluation data set this code is a bit simpler since we don't have to iterate through the colum names but it's exactly the same then we move on to evaluating the different search methods here I Define a handful of functions which we can just skip for now and I'll return back to them as we come across them in the code but here I'm doing a similar thing as before I'm listing all the different ways we can evaluate the similarity between the query and a particular video here I list three different distance metrics from pyit learn then two different similarity metrics from the sentence Transformers Library we're going to evaluate all possible combinations of model columns to embed and distance metrics or similarity scores so again this is 45 different combinations even if you could have hardcoded the last six combinations do not hardc code 45 different configurations just write the four Loops similar situation here we're going to Loop through the models here I'm grabbing the text embeddings for all 64 queries in the evaluation data set so I stored them all in this query embedding dick if we look at this thing we see it's a numpy array and then we'll have a row for each query and then we'll have a column for each embedding Dimension then we're going to Loop through all the text columns and we're going to pull the text embeddings for that particular column first we'll start with the title this is going to pull the text embeddings of the titles for every one of the videos looking at that this will also a numpy array but we see that the number of rows is 83 because I have 83 videos and then finally we have a third for Loop because we're going to Loop through each of the distance metrics this will get us this disc object which we can use to compute pairwise distances for all the videos and all the queries so this final thing will be an array of distances we can look at the shape notice that there 83 rows for corresponding to 83 videos and 64 columns corresponding to 64 queries in the evaluation data set each element of this array will be the distance between the E video and the J query for example if we looked at the very first element this would be the distance between the first query in our evaluation data set and the first video in our video index we're going to use this ARG sort function from numpy to sort each of the columns and so if we go back to the disc array we have 83 rows and 64 columns so if we sort each column we're going to rank the videos from smallest distance to largest distance for each of the 64 queries since it's ARG sword instead of returning the ordered values themselves it's going to return the index of the values in ascending order next I Define a method name and this is essentially like we did before where we had a unique name for each combination of model and column but here we're going to combine the model name the column name and the distance name so each of the 45 configurations for this Search tool has a unique name so here I use a function that I defined called evaluate true rankings which evaluates the ranking of the ground truth in other words for a given query we have 83 possible videos to return but only one ground truth in the evaluation data set what this function does is that it returns earns the ranking of the ground Truth for each of the 64 queries that function is defined here and I won't walk through this because I feel like that might get too far into the weeds but if you're curious the code is available on GitHub but we can look at the shape of this thing we can see that it's essentially a one-dimensional array with a ranking value for each of the 64 queries we can see for the first query the ground truth was in the third position for the second query the ground truth was in the zeroth position so it was the number one ranking and so on and so forth and so what I do here is I convert this whole thing to a list and then I append it to the method name so basically a Val list is just going to be one giant list of all the rankings with the first element being the method name and then I store that in another list called eval results so this eval results will be a list of lists where each element is a list corresponding to a particular configuration so we can't use shape cuz it's a list but we'll look at the length and we see yes there are 45 elements 45 elements for the 45 possible combinations this is where a little hard coding comes in because the distance metrics are from pyit learn while the similarity scores I'm importing from the sentence Transformers Library so so since the syntax is a bit different I have to write a different script for that of course this part is copy pasted essentially so I could have been a bit more clever in how I wrote this code but in this specific case I thought it was easier to just leave it how it is like this the one thing I did here which people might come after me for is I dynamically defined this line of code using this syntax and then I executed that command using the exact function command is just a string which looks like a piece of python code so we're defining this distance array as the minus of the similarity score between the embedding array and query embedding and the reason I put minus is that since this is a similarity score it's the inverse of a distance score so in other words if two things are close together a distance score will be small but a similarity score will be large so instead of changing this ARG sort to go the other direction I just add this minus sign so it reverses the order and then the code will be exactly the same we'll sort the indexes like before we'll Define a method name we'll extract the ranking of the ground Truth for each of the queries and then we'll store it in the eval list and then store that list in the eval results list and then here I basically do the same exact thing but it's a little different because I'm embedding the titles and the transcripts while before I was embedding either the tit or the transcripts here I embed both and then it's a lot of the same stuff but here's the key difference when I do the pairwise distance I compute the distance between the title embedding and the query embedding as well as the transcript embedding and the query embedding and then I add those two distance arrays to each other and then repeat the same process so we've seen this chunk of code for a third time now so that's a good indication I should have wrote a function to do this but here we are then I do a similar thing for the similarity scores and so this is the downside of automatically generating code and running it is it's kind of hard to read this line here so we can just run it and take a clear look at it here we Define the distance array as the minus of the similarity score between the title embedding and the query embedding minus the similarity score between the transcript embedding and the query embedding again we have to do that because this similarity score will either be the cosine similarity or the DOT score and we have to add the minus sign to turn the similarity metric into a distance metric and again magnitudes don't matter it's just the ranking that matters which we get from this block of code here which we've now seen a fourth time so this definitely should have been a function and then just some fanciness happening here so maybe this is why I didn't do it as a function because title transcript changes as well and then we have to adjust the similarity name to make the method name come out good I changed the underscore and doore and cosign similarity to a hyphen to make this a little easier to read after that arduous process we've generated 45 different configurations of this Search tool and so everything is stored in this list called eval results which should have 45 Elements which it does but all this information in a list is kind of hard to access so let's store it in a data frame to make it easier to make sense of so to do that I Define a schema for the data frame I do this programmatically where our data frame is going to end up having six 5 columns where the First Column will correspond to the method name which we generated programmatically and the rest of the columns will correspond to the rank of that particular query for that particular method so now you can imagine we're going to have 45 rows in this data frame for each configuration and then we'll have a column corresponding to each query and then the element of the data frame will be the ranking of the ground truth search result for that query using that method so to make that a bit more concrete it looks something like this we have the method names in this column here we have the ranking of the ground Truth for every single query in the evaluation data set as columns so with this first method here let's just print the name so we can see what it is for this first method it's using the model all mini LM L6 B2 it's embedding the title and it uses the ukian distance between the query and the title embedding to rank the search result and then using that method the ground truth was the zero search result so that indicates perfect performance using this metric then we repeat that for every single query and every single search method next I'm going to create two summary statistics so specifically the mean rank of the ground Truth for a particular method and then if the ground truth result appears in the top three results or is the number one result so so this gives us three summary statistics which I add to the results data frame with these two lines of code and then I'll create a new data frame called DF summary that just includes the summary statistics and doesn't have the more granular performance metrics shown here we can look at this summary data frame from three different perspectives first we can rank it by the mean ranking of the ground truth and so this first method had the best performance along this evaluation strategy where the ground truth was usually either the zeroth or the first search result so this method was using all mini LM L6 V2 which was our smallest model it was using both the titles text embeddings and the transcripts text embeddings and it used the Manhattan distance metric and so a Manhattan distance instead of ukian distance which is like the direct path between two points on a graph the Manhattan distance travels along a particular particular axis so distances are computed along grids so the shortest path along a particular grid one thing that is kind of expected is that title and transcript combined together has the best performance and we can actually see that a lot of these results have both the title in the transcript as text embeddings but what's somewhat surprising is that the smallest model had the best performance as opposed to a bigger embedding model two other views is instead of ranking by the mean ground truth ranking we can rank it by the number of top one search results we can see that actually four methods had the ground truth in the number one result and so this was again using this smallest model but these didn't include the transcript they just included the title which is interesting and then they all Ed different distance measures so this one used ninian distance this one was the cosine similarity this one was the doore essentially all three of these methods are equivalent which is very interesting finally we can look at this summary table according to the number of times the ground truth appeared in the top three so again we get three methods that had similar performance but these were all different than what we saw before so now what seems to perform best is our second largest model which is multi QA distill bird poost V1 where they all embedded both the title and the transcript but then used different similarity scores so notice that there was no one method that dominated all others for instance this first method outperformed this fifth method in terms of the number in the top three but this method did better than this method in terms of number in top one similarly even though this method outperformed this method down here in terms of number of top one this method down here outperformed the method up top in terms of the average ranking of the ground truth so this is kind of where the art comes in and you often synthesize this information in your own head to pick out the best strategy and of course you can make this more objective where you give particular weights to each of these evaluation scores so maybe the average ranking of the ground truth is the most important evaluation metric you want to use you'll give this ranking more weight as opposed to this ranking another thing you might do is give more weight to a smaller model as opposed to a bigger model like this one multi QA m pet base. V1 due to the computational cost and the storage cost of a larger model so in this specific situation I went with this model here for two main reasons one I feel the average ranking of the ground truth is a good evaluation metric to base things on and it did pretty well in terms of this number in top three evaluation metric where it was basically in second place and a lot of times you don't need the number one search result to be on the MTH as long as the first few have what the user is looking for that's typically a good user experience at least that's just a hypothesis to be tested and so through this whole experimentation process came to the conclusion that this is the best method to use we'll move over to the next notebook where we're going to create the video index and so this is pretty simple so we read in our data frame this the same data frame we saw before all we're going to do now is embed the titles and the transcripts so we can implement this specific method that's pretty similar to what we saw in the previous notebook where we're going to Loop through both the title and the transcript columns we're going to generate embeddings we're going to store these embeddings in a temporary data frame it's going to have 83 rows for the 83 columns and then we're going to have 384 columns for each of the embedding dimensions and then what I do is I concatenate the original data frame that we imported here with this temporary embedding data frame so that happens for both the title and the transcript and the and result of that is that our original data frame went from 83 rows and four columns to 83 rows and 772 columns if we print the head it looks something like this where we have a bunch of new columns corresponding to the title embedding and the transcript embedding then we simply can save this to file so I'll save this as a parket file called video index so this is the final data store or database we can use in a production system and it's hilariously small the final file is like less than 1 Megabyte no need for any kind of fancy database or data warehouse to store this information this is small enough it can just be stored in the project file for the final system okay and now moving on to the last notebook we're going to implement the search function and generate a user interface for it so here importing a lot of the same stuff as before now instead of doing the read paret I'm doing scan paret so what this does is instead of loading this data frame into memory or into the python environment it's going to create a lazy frame object which is what they call it in polers that allows us to manipulate the data frame so to speak without loading it into memory and then when we want to load it into memory we can call a specific method called collect to do that so this isn't totally necessary here because the data set is super small but this is very handy when the size of your data set is larger than the amount of memory you have on your system but it also just keeps things lightweight you're not carrying around this bulky data frame throughout all your different operations that's what's happening here it's that video index we created in the previous notebook we're defining the model name and then we're going to load it in and then we're going to import that distance metric so in principle all the stuff will be loaded ahead of time so that these are ready to go when the user goes to use the search function now I'm going to define the search function so it's super simple we'll write a function called return search results that takes in a user query and spits out the indexes of the search results in our data frame what that looks like is if we type in the query llm it'll spit back out the indexes and then we can display the results using this line of code here we'll have the video ID and then the title of the first result is llms explained in 60 Seconds then we have how to build an llm from scratch how to prove llms with rag practical introduction to large language models and video on fine-tuning so this return search results kind of does everything we need looking under the hood this is similar to what we saw in the experimentation code where we're generating and embedding for the query but here we don't have to worry about the 64 queries in the evaluation data set we just have one query coming from a user then we can compute the pairwise distance between the Ty embeddings which are stored in these columns and the query embedding and the pairwise distances between the transcript embeddings and the query embedding then we'll add those together then I Define a couple of search parameters specifically I'm going to define a distance threshold so I will only return results that have a distance of 40 or less away from the query and then of those results I'll only return the top five I Implement that in these two lines of code here where I first find all the arguments that are less than the threshold and then of those distances below the threshold I will sort them and return their arguments or their indexes and then finally I'll take these sorted indexes below the threshold and return the top five that's what's returned here that allows us to print results in this way but of course this isn't a very intuitive user interface users aren't using jup your notebook or coding in Python so it's helpful to develop a guey or a graphical user interface to interact with this functionality I do that using gradio so I defined a few functions which I'll hide to keep things simple but basically with gradio you can spin up these user interfaces in a very simple way so this is what it looks like and then if we type in the same thing we see the same search results as before it looks kind of wonky cuz I'm so zoomed in but let's open it up in a new tab and search the same thing okay so that looks a little better so we can see the same results that we saw in the Jupiter notebook lm's explained in 60 seconds how to build an llm from scratch how to improve llms with rag introduction to llms and the fine-tuning video essentially What's Happening Here is instead of displaying the results like this displaying the results in a user interface briefly going through the gradio code gradio is pretty intuitive where it just creates the interface in like this top down manner you can create this demo as a series of so-called blocks in gradio and then each line will be a block here the first block is the title which is a markdown object so that's what this thing is here then below this markdown title we'll have a row which will consist of a text box which will take in the user's query and we'll have a button where when the user clicks the button it'll run this search results function which I defined here and we'll talk about in a second but going back to the interface we can see we have the text box where the user can type in their query and then a clickable search button looking under the hood to this search results function it's calling a pseudo search API so in production this would be living in the cloud or some server you have available but basically the API the pseudo API will take a query and will spit back a result and the pseudo API looks like this essentially what it's doing is it's running that same function we saw before this return search results and instead of returning the results as a data frame it's going to return it as a dictionary we have a dictionary with two key value pairs the first key is title with a list of titles from the top five search results and the second key are the video IDs for those top five search results and the reason I put it in a dictionary form is that when you're making these API calls the responses typically come in a Json format which are essentially python dictionaries so I did that to mimic a API call once we have the response we'll basically write code to format the response in the user interface so I guess I'll take a step back and go back to the user interface so again this was the row we saw with the text box and the search button but then what I do is I will generate five more rows corresponding to five top search results what that looks like is we have this output list and I'll append an HTML object and a markdown object to it what that corresponds to is that this is our HTML object and then this is our markdown object and each of these items this HTML block this markdown block this HTML block this markdown block these are all organized in this output list so when we refresh the page these are all empty like they've just been initialized so that's what's happening in this first PA but whenever the user types something into the text box and hits search or they type something into the text box and just hit enter that's what this line of code is corresponding to it'll run this search results function it'll update this output list and so the first thing that it'll do is look at the number of responses that it receives because if there are less than five search results it needs to be able to handle that case and so let's say there are three search results what's going to happen is it's going to Loop through those three search results generating the HTML block and the markdown block for them and appending those to the output list but then for the two remaining slots it's going to make invisible HTML and markdown blocks for those results an example of that might be if we just type in a bunch of mess okay well that was really crazy so let's try something like okay so when I type in I lost my dog not really relevant to anything on my YouTube channel but there are still search results you notice that there aren't five it only return two results and the remaining three are invisible and then in that other case where we just have a bunch of craziness and nothing matches the search criteria it'll just say no results try rephrasing your query then that's handled as a special case in this if statement here so here we really got into the weeds of experimentation and what it looks like to develop a machine learning solution while this does build out a lot of the core functionalities of the machine learning project what we did here is not something suitable for a production system or something that you'll be able to use in the real world which brings me to the next video in this series where I'll talk about what I call phase three of any machine learning project this is where we deploy our ml solution into the real world so in the next video I'm going to walk through three main things first developing a real API not just a Pudo API that can access this search function second containerizing the search function and its API to make that functionality much more portable and then finally deploying that container of code onto AWS so that brings us to the end if you enjoyed this video and you want to learn more be sure to check out other videos in this series on full stack data science and as always thank you so much for your time and thanks for watching
OnIQrDiTtRM,2024-05-03T12:40:43.000000,How to Build Data Pipelines for ML Projects (w/ Python Code),when you think of machine learning fancy algorithms and techniques probably come to mind while these things definitely play a role the most important part of an ml solution are the data used to develop it in this video I will discuss the most critical data engineering skills for building ml Solutions end to endend and walk through a concrete example with python code and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoyed this content please consider subscribing that's a great no cost way you can support me in all the content that I make data engineering is all about making data readily available for analytics and ml use cases so at a high level what this looks like is we have raw data that comes from the real world and data engineering is taking that data and making it use case ready so that Downstream users can create spreadsheets from it build dashboards or even train machine learning models while this consists of a wide range of skills such as data modeling creating schema for databases and even managing distributed systems for large scale data sets in the context of building machine learning projects end to end this comes down to one key skill building data pipelines basically a data pipeline gets data from point A to point B so if this is point a and this is point B a data pipeline is what connects these two things together NE necessary components of a data pipeline are an extraction process the data need to be pulled from The Source in some way another necessary component is a loading process the data need to be loaded to the Final Destination also called a data sync so the simplest data pipeline would just consist of extracting data from point A and directly loading it to point B however for a lot of machine learning applications it makes sense to have a step in between the extraction and the loading processes and to transform the data in some way these three steps of extract transform and load are the three key elements of any data Pipeline and these days there are two prevailing paradigms for combining these three elements these two pipelines are ETL which stands for extract transform and load which is what we saw on the previous slide and extract load and transform which I'll talk about in a minute starting with ETL data are extracted from a source transformed in some way and then loaded and typically the data are loaded into a database or a data warehouse this makes it easy for a business analyst or a data scientist to query the data for their particular use case although this is great because when the data are loaded they're basically ready to go this type of data pipeline has two key limitations one ETL pipelines typically work best for data that can be represented in rows and and columns for example if you're working with image data or text data in order to store it in a database it needs to be reconfigured or processed in some way another limitation is that ETL processes can only serve a relatively narrow set of use cases this is because any transformation you do on a data set will naturally constrain the types of use cases that it can be used for this is why the second type of pipeline is becoming more popular which is elt so in this type of pipeline the data are extracted and loaded with minimal adjustments typically into a data warehouse or data Lake and then any sort of Transformations will be done on a use Case by use case basis an upside of this is that elt processes can support all data formats this includes tabular data like in the ETL process but also text documents or PDFs as well as images can all be stored in a data Lake another upside is that since minimal adjustments are made to the raw data this gives more flexibility in the types of Transformations that can be done for Downstream tasks this gives more use case flexibility for elt pipelines while elt is becoming more and more common across Enterprises when we're talking about full stack data science and building machine learning projects end to endend this ETL pipeline may be a natural choice for most projects so let's go deeper into each key component of a data pipeline starting with the extract process which is acquiring data from its source these days most data sources for most businesses are managed by Third parties this is great because this allows us to access data through apis so an API is an application program interface it's essentially a way to interact with an application using Code what this might look like say we have an e-commerce business that uses HubSpot as their CRM Shopify to host their website and to make their sales and then they promote their business on Facebook and Instagram if we wanted to pull data from all these different data sources we could use the respective apis hubs swots API shopify's API and meta's different apis this would allow us to get the lead data sales data and social media data to build some kind of customer loyalty model or build a dashboard or anything anything like that however there's situations where the data we want to access aren't available via apis in these situations we will have to develop custom extraction processes a few examples of this could be scraping public web pages of course you've got to be careful with this you don't want to break any copyright laws or break the terms of use for a platform that you're using another example is pulling documents from a file system whether that's an internal file system or an external one or sensor data this is something I did a lot in grad school we were using environmental and physiological sensors and had to write a lot of code to pull that data next we have the transform step which is translating data into a useful form a big part of this is when we extract data from a raw Source it is often either semi-structured or unstructured semi-structured data are things like text in a Json format or a CSV file Json is very popular because often when you're working with a apis the responses that you receive from them will be in this Json format or you're working with unstructured data let's say you want to build a language model or fine-tune a language model on company documents this can involve extracting text from docx files or PDF files translating semi-structured and unstructured data into a structured form so basically something you can put into a database is a major part of the transform step however transform involves much more than just making unstructured and sem structured data structured this could be managing data types and ranges of specific variables could also be duplicating your data and imputing missing values another common task is handling special characters and special values and finally feature engineering so preparing your data so you can feed it into a machine learning model finally we have the load step which is about making data available for machine learning training or inference while the details of the load process will depend on the specific use case here I'm going to walk through a handful of storage solutions and when it might make sense to use each of them the simplest way to load your data is to save it in the same directory as your machine learning project Loosely speaking this is appropriate when the data are of megabyte scale maybe come from a few sources and only have the one use case of that specific project that you're working on another option is to use any cloud storage solutions such as S3 Google Drive Dropbox one drive the list goes on these are great because they give you a very easy way to load and access your data and often come built in with redundancy and version tracking however as your data start to scale and your use cases start to scale simple storage solutions might start to fall apart so in those cases you may want to start using a database so popular database Solutions are my SQL and postris sequel but as your data scales and your organization scales at some point you may want to transition to a data warehouse which is a more modern and scalable storage solution the key difference between data warehouses and databases is that data warehouses have distributed infrastructure so they can really scale to the Moon here I put terabyte scale but most data warehouses can support even petabyte scale data and then finally if you have an overwhelming amount of data to manage and essentially endless use cases you may want to consider a data Lake which uses the more modern elt pipeline I described earlier so once we've defined our extract transform and load processes the next step is to bring it all together while simple data pipelines can live in a couple of Python scripts as your pipelines become more and more sophisticated this raises the need for orchestration tools the main idea behind an orchestration tool is to represent your data pipeline as a directed ayylic graph or dag for short and all that means is we will represent our tasks like extract transform load as so-called nodes and then we'll connect the tasks together with arrows which represent the dependencies so a very simple data pipeline might look like this where you have some trigger so maybe every day at midnight this trigger will go off an extraction process will be kicked off the data will be transformed in some way and then it'll be loaded somewhere while traditionally you may have used a combination of python and command line tools these days there are a lot of tools for orchestrating data pipelines such as airflow which is a very popular one dagster Mage and many more another key part of orchestrating these data pipelines is observability which basically gives you visibility into your data pipelines because often these are automatic processes just happening on their own no humans are involved so being able to monitor the performance and the status of your data pipelines is very important and even sending automated alerts if things start to go wrong so I believe all these tools come built in with some observability functions so this has really never been easier to implement okay so with a basic understanding of data pipelines let's walk through a concrete example here I'm going to build upon the case study from the previous video and walk through building a data pipeline to get video transcripts from from all of my YouTube videos so to do this we'll start by importing some libraries the requests Library allows us to make API calls the Json Library allows us to work with text in a Json format polers is basically a faster version of pandas this line here is importing my YouTube API key from a external file and then finally I'm importing this YouTube transcript API Library which will allow us to grab the transcripts programmatically we're going to start with the extract process so first I Define my channel ID then I'll Define the URL for YouTube's search API so it's pretty simple just googleapis.com youout version 3 search next I initialize a page token which we'll see why this is important in the next slide and then I will initialize a list to store data for each of the videos this is a pretty crazy piece of code here but I'll explain it line by line Let's ignore the W Loop for now the first step is we're going to define the parameters for the API call we're going to load all of these into a dictionary the things we need to send to the API in order to get a response are the YouTube API key the channel ID what sort of search results we want to receive so snippet includes just a bunch of data about a particular video and ID I believe is just the video ID order is how we want the results to be ordered so I ordered it by date the most recent video will be first next we have Max results which sets the maximum number of search results to receive back and then finally we have a page token the way you can think about this is that making this API call is just as if we went to the YouTube website and made a search through the user interface what's different here is that instead of using the UI we're doing the search programmatically just like when you do a search you have a certain number of results and then if you want to see more results you'll have to scroll down or go to the next page to see those results that's exactly what's happening here which is why we need to define the page token so any set of search results will be split across multiple pages and you'll need to look at each page one by one to find all these search results and if you're thinking why can't we just have all the search results on one page so we don't have to go through Page by Page the reason is the maximum number of results per page that is allowed through YouTube's API is 50 this is why I initialized that that page token To None because when we first do that search result we don't know what that page token will be so it'll default to the very first one and then we can Loop through each page one by one in this while loop so once we've defined these parameters we can make the get request we use the requests library to do that all we need to do is specify the URL and this dictionary of parameters and then I have this function get video records which takes the API response and creates a list of dictionaries where each item in the list corresponds to a specific video so I will add this list to the existing video record list then I'll try to grab the next Pages token that exists in the response so response. text will return the text of the API response and there's this next page token item which we can extract however if a next page doesn't exist meaning we've reached the end of the search result page this line of code will fail so it'll jump to this accept thing and the page token will be set as zero so if we go back up to the W Loop if we got a page token we'll go through this again if we didn't get a page token this will kill the wall Loop and go on to the next thing the next thing here is this get video records that's actually a user defined function defined in this way this is a lot of what extracting is all about you have this raw text from an API and you got to extract the information that you actually care about that's what I'm doing in this function here so it'll take in the API response it'll initialize a list so we can store data for each video walking through this what I'm doing here is looping through each item of the API response since our search is anything to do with my channel the search will return videos shorts and Community posts and since we don't really care about Community posts we want to make sure we skip the extraction for each of those items but unfortunately YouTube # video is the same for both YouTube videos and YouTube shorts so you'll need to find another way to separate out shorts if that's something you want to do and then we can just extract the relevant data so I initialize this dictionary then I add information element by element so I get the video ID which we can access like this the date time the video was posted which is available here and then the title of the video which we can grab like this then we can append the dictionary to this list that we initialized earlier and then just keep going through every single item in this API response so all we've done through this whole process is we've extracted video IDs the date it was posted and its title we haven't actually gotten the transcripts yet so in order to do that we got to keep going what I do is I take this video record list and I store it into a poers data frame so this just makes it a little easier to access all the information so we'll have the video ID as a column the date time as a column column and the title as a column in order to grab the transcripts we will need the video ID we'll see in the next slide how we can Loop through each of the video IDs and extract the transcripts for each video this is what the code looks like so again I initialize a list to store the text of each transcript here I Define an index I for each row in the data frame so what I do here is I try to extract the transcript using this YouTube transcript API this Library makes it super easy to grab the transcript of any video all I have to do is provide the video ID and it'll return back a transcript the transcript actually comes in I believe a dictionary format so I wrote another userdefined function to extract just the text from that transcript and then I set it equal to transcript text however if the library is not able to extract a transcript this chunk of code will fail and then we'll just set the transcript text as na so this happens when there's no language on a video so basically there's no talking in a video whatsoever and I have a few videos that don't have any talking in it and then finally we append the transcript text to the transcript text list so we can doubleclick into the extract text function to see what that looks like and it's actually super simple so it'll take in a list okay so the transcript actually comes in a list format and the extract text Will map it to a string so I kind of do everything in one line of code here I take the I element of this transcript list and then extract the text from it so I think each element of this list is a dictionary where one of the keys is the text so you can think of it as like each line of the transcript is stored in a dictionary and then text will be one of the fields of that dictionary and so we're just extracting every single line and just joining it into a massive piece of text next we can add the text from all the transcripts to our data frame and so we can that with this line of code here and then that'll look like this so now we have a new column says transcript all the text from each video is here okay so that was the extract process definitely a lot of coding but now we can move on to transforming so transforming will involve a little bit of exploratory data analysis or Eda for short there are a few things that I do for this specific use case the first thing is I check for duplicate values so basically I want to check if they're no identical rows cuz that wouldn't be a good thing we don't want repeating videos and also I want to make sure there are no duplicates on the column level Beyond just transforming data into a usable form this is just a good thing to check to ensure data quality the way I do this is I just simply print the shape of the data frame print the number of unique rows and then print the number of unique elements in each column we see that the data frame has 84 rows and four columns it has 84 unique rows which is good that means there are no repeating rows we have 84 unique video ID date times and titles which is good because there shouldn't be two videos with the same ID I've never posted two videos in one day and then there should be no two videos with the same title but when we go to transcript we see there are only 82 unique transcripts and this is actually expected because I have three videos that don't have any talking in them so for these videos I set the transcript as na in the script earlier another thing is we want to check the data types looking at our data frame we see everything is a string while this this is appropriate for the video ID title and transcript it's not appropriate for datetime this should be saved as a datetime data type and this is super easy and polers just do it in one line of code we just update this datetime column as a pl. Time object and then we can print the head again and we can see that it changed the format of this column the last thing I'll do here for the transform step is handling special characters this took a bit of like manual skimming but after a few minutes I found a few characters strings that weren't correct specifically some of the apostrophes were represented as this string of text here some of the Amper stands were represented like this and then finally this isn't really a special character but more like a quality control thing the automatic captions don't know who I am so whenever I say Shaw it comes out like this so I just corrected the spelling and then we can Loop through each of these special strings and replace them in the title and transcript Columns of the data frame this is the code to do that finally we can load the data set and since this data is hilariously small it's about 200 kilobytes for the entire data frame and this is only for a proof of concept it's more than sufficient to just load this data into the directory of the project that I'm working on and just do that with one line of code where we save this data frame as a parquet file using the right parquet method I could have also used CSV but par is actually a compressed file format so the park file is actually a third of the size of the equivalent CSV file so in the next video of the series we'll move on to what I called phase two of a machine learning project what this will consist of is taking the paret files that we created in this example and generating text embeddings from them and the reason we want to do this is if you saw in the previous videos of this series we can use these text embeddings to create a semantic search system over my YouTube video this will require a bit of iteration and experimentation to see which embedding models will provide the best performance for this specific use case and then once we've picked out the embedding model we can create this search function this whole process is going to be the focus of the next video that brings us to the end I hope you got some value out of this content if you enjoyed it and you want to learn more check out the medium blog Linked In the description below and as always thank you so much for your time and thanks for watching
eayzAZltV9U,2024-04-29T13:54:55.000000,4 Lessons from AI Consulting #freelancing,are four things I've learned from AI Consulting one trust is more important than anything else in my experience what differentiates clients from prospects is the belief that I could solve their problem and that I was on their side two don't skip the discovery phase when providing Technical Services it's easy to jump right into the coding the problem with this however is that you can spend a lot of time and money solving the wrong problem three find your number one sales Channel although there are several ways to get clients for example upwork referrals conference speaking content creation ads and more I and others that I've met in the space get most of their leads through one key sales Channel fourth and finally it's not real until the money's in the bank this is a lesson I've learned over and over again from Great Discovery calls that ended in nothing
03x2oYg9oME,2024-04-25T15:16:00.000000,How to Manage Data Science Projects,this video is part of a larger series on full stack data science in the previous video of this series I introduced this idea of a full stack data scientist and described the four hats that it involves in this video I'm going to dive into the first of these four hats which is that of a project manager I'll start by introducing a five-step project management framework specifically for data science and then I'll walk through a concrete example of the project manager role in implementing this framework and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoy this content please consider subscribing that's a great no cost way you can support me in all the videos that I make before diving into the framework I think it's helpful to take a step back and ask ourselves why do we need project management while there surely cases where project management feels more like red tape and constraints as opposed to something that pushes a project forward the way I see it just just a little bit of planning and structure can go a very long way when it comes to implementing projects that involve a lot of moving pieces here are a few reasons for that first project management involves all of the planning and scoping for a project just like how an architect will draw out a blueprint for a building before it's actually built project managers will draw out the blueprint of a project before it is implemented next data science projects often involve bringing together multiple pieces this includes data sets compute Technologies and of course this involves people these could be individual contributors they could be your stakeholders or other people involved in making the project happen and then finally project management helps keep projects on time and on budget so while this video is specifically about data science projects everything that I said here is not something unique to data science projects in fact these three things are going to be relevant to project management in any domain however there are some unique considerations for data science projects which brings up a five-step project management framework that I like to use When approaching a model build or the development of some other datadriven solution the framework is shown here where it's broken down into five phases starting from phase zero which is what I call the problem definition and scoping phase phase one is the data acquisition exploration and preparation we can also think of this is all the data engineering work next is phase two which is the solution development this could be training a model or it could be doing some kind of analytics this will typically involve a data scientist or maybe even a data analyst next phase three is the solution deployment so this is actually taking the solution and putting it into the real world so that it can actually have an impact and then finally phase four is the evaluation and documentation step let's dive into each of these phases a bit more deeply starting with phase zero the problem definition and scoping phase this will involve formulating the business problem basically asking ourselves what problem are we actually trying to solve next is designing a solution to that problem any given problem will have countless ways it can actually be solved so it's important to make a decision on the best way one can solve the problem at hand finally defining the project requirements and road map so in other words what do we need to make this project happen and what will the implementation look like next we have phase one which is the data acquisition exploration and preparation so this might involve looking at internal datab bases looking to third-party data vendors or any other data source evaluating available data also means asking the question do we have sufficient information to solve the problem at hand another key step of this phase is acquiring and exploring the data so this includes things like exploratory data analysis where one is just trying to get a sense of the information that can be gleaned from the data set and then finally the development of data pipelines so this is your extract transform and load pipelines or your extract load and transform pipelines next we have phase two which is the solution development so this might be the part most people are excited about which is the model development so developing Ai and Data Solutions but it's not just about training a model or building some other analytic solution it's also about evaluating that solution in terms of its validity and the value that it generates and this evaluation process will often involve iterating with stakeholders next we have phase three which is solution deployment so the basic idea here is to integrate the solution into the real world business context whether that's automating a particular workflow or making information available to stakeholders via a dashboard maybe updating a widget on a website so this can take many different forms additionally for some sort of intervention you know you're updating a business process you also may need to implement a solution monitoring pipeline it's not just about making a change and walking away but making a change and consistently evaluating the efficacy of that change and then finally phase four is the evaluation and documentation step so this involves assessing project outcomes and comparing them against the expectations from the outset of the project this is also delivering technical documentation and user guides and then finally doing a perspective reflecting back on the project looking at things that went well looking for opportunities for improvement considering the Future Works and the limitations of the current project and starting to talk about the obvious next steps and of course we have these feedback loops in this five-step framework some key ones are shown here where you may come to phase one and you might be evaluating the available data and you may find that we thought we could solve this problem we could build this model with the data we have internally available but upon further investigation that's not the case and we'll have to figure something else out so that'll require going back to phase zero and asking ourselves okay how can we get this data do we look to a data vendor do we try to get it from publicly available sources another key feedback loop is between phases 2 in phase one so say during model development there seems to be some kind of bias in the model and then upon further investigation you find that a particular exception wasn't properly handled in the transform phase or the data pre processing step so that'll require going back to phase 1 updating the data preparation Pipeline and then returning back to phase 2 to train the model another feedback loop is from Phase 2 to phase zero so the data may be properly pre-processed and there's nothing wrong with it but still upon training the model the predictive performance may not be as good as the existing solution so this might require going back to phase zero and reassessing the solution design and asking is there a better way we can solve this problem the final feedback loop is if everything goes according to plan value is generated but of course no project is ever complete there are always opportunities for improvement and then You' return back to phase zero to start building a broader solution so while the project manager is ultimately responsible for the successful implementation of the project and ensuring that each of these phases happens the key contribution of the project manager is this phase zero because if this phase zero is is not done properly it's going to cause problems in every subsequent phase of the project so this brings us to the key role of the project manager which is this phase zero the problem definition and scoping and as we saw on the previous slide this consists of three key steps the first is the problem diagnosis it's often the role of the project manager to collaborate with stakeholders and project owners to get a clear understanding of the problem that they're trying to solve and it's critical to get this step right because if you think about it if you don't diagnose the problem properly you can spend a lot of time and effort solving the wrong problem which no one wants the stakeholder doesn't want that and the team doesn't want that so being able to facilitate these conversations with stakeholders is a key skill of any project manager while I won't go too deeply into what this might look like I do have an entire video dedicated to this topic which I'll link up on the screen here so once you have a clear idea of what problem you're trying to solve then comes the task of identifying the best way to solve that problem so this requires a project manager to bring together a lot of different information they have to bring together the business context the priorities of the stakeholder they have to consider the available Technologies the available resources the available data the available budget the available timeline and they're also might be competing priorities and beyond that any given problem will have a wide range of potential Solutions and varying degrees of complexity and scope so at the end of the day the project manager has to synthesize this information to help the stakeholder or the project owner make the BET of what's the best way to solve the problem and then finally is the implementation plan once the solution has been defined there's still the matter of the details of what this thing's actually going to look like and how it's going to be built and even still any given solution can have a wide range of potential implementations so to make this more concrete I'm going to walk through a concrete case study the point of this is to one perform the phase zero for this project that I'm building but the second is to walk through step by step what it looks like being a full stack data scientist and so here this is me putting on the project manager hat for this project starting with a bit of background I make content on YouTube and write articles on medium what this looks like is I'll make content about whatever is interesting to me whatever I'm curious about or things that I've personally experienced since I am just talking about whatever's interesting to me it might be difficult for people to navigate all the different pieces of content I make across these two platforms which brings up the problem potentially I talk about too many topics across too many platforms so to kind of give a flavor for this I'll talk about things from topological data analysis AI for business causal inference how much I made writing on medium how to get a data sign job more philosophical things like what it means to be antifragile more personal things like my struggles with anxiety and now to this series of becoming a full stack data scientist so someone who's seeing this content for the first time or is trying to navigate this very diverse landscape of content might have trouble finding the content that's most relevant to them which brings up the proposed solution which is a semantic search function for my YouTube videos what this might look like is a page where users can type in a natural language query or a question and then the web page will return search results of YouTube videos relevant to that query so this only addresses half of the problem I mentioned on the previous slide which is I make too many topics on too many channels this only solves that first problem of too many topics so the hypothesis is if these topics are more easily searchable and they're easier to navigate then more people will engage with the content and then more people will share the content and that will promote the growth of the YouTube channel and so you might be thinking sha why are you just solving half the problem why not solve the whole problem that brings up this broader question of should I build a POC or a proof of concept I know there are a lot of different opinions on poc's and some people love them some people hate them I personally believe that poc's are very valuable when it comes to building machine learning projects you can do all sorts of interesting things with machine learning but of course of all these interesting things you can do only a small subset of them actually generate value so the first reason why I believe in Po's is that they help you validate the idea I can put together this proof of concept version of the web page I can make it available to some people in my audience I can make it available to some people that I know and they can provide me feedback which will help in making this decision of is this something worth pursuing or should I just cut my losses and move on to something else the second reason is often with data science projects efforts tend to stack on top of each other so if I properly build this proof of concept for YouTube videos it should only be a marginal effort to include my medium articles into the same interface now that we have a clear picture of what the solution will look like how do we actually make that happen that brings us to the implementation plan the first part of which are the project requirements so this includes the roles the data the compute infrastructure and the Technologies needed to implement the project here I list off the roles of the full stack data scientist so these are the four hats mentioned in the previous video the first is the project manager then we have the data engineer data scientist and ml engineer and what's important to note even if you are a full stack data scientist maybe you still want to bring in a data engineer or a data scientist to help you build the project or maybe you want multiple data Engineers or maybe you want to bring in someone to do the data science and the ml engineering piece while you just do the project manager and data engineering piece so it's important to note that roles do not equal people multiple people can do multiple roles and a single role can be done by multiple people the next is the data requirements so here I put an evaluation data set which will consist of 50 query video pairs that will help evaluate the quality of the search so in other words I need a data set I can use to evaluate the performance of the search function that I built so what that'll look like is a list of 50 queries and an associate ated YouTube video ID with that query for infrastructure I'm going to use AWS light sale which makes it super easy to deploy Docker containers and then finally Technologies I included all that I could think of at least at this point so I'm going to use Python for basically everything I'm going to use the YouTube API to pull in some data there's a python library that downloads the automatically generated captions of a YouTube video going to use pandas or polers to handle the data structures the sentence Transformers library to generate text embeddings fast API to make an API Docker to containerize scripts and then gradio to spin up a front end and then finally I'm going to put everything in a GitHub repo and have the documentation be part of that repo next we have the project road map so this consists of a few different things these are the project Milestones which I map to phases 1 through four of the five-step project management framework discussed earlier then we have a task description so what are the tasks that make up this Milestone or this phase assigning a role to each task and assigning a due date to each task you can add more things here like a more detailed description or acceptance criteria but I would say that these four elements of Milestone task roll and due date are the bare minimum you need to make a proper project roadmap I'll just kind of briefly fly through these I don't want to spend too much time reading it but you can always pause the video and read through these if you like phase one is all the data engineering stuff so this is extracting the transcripts and saving them as a paret file phase two will consist of exploring multiple embedding models and testing the search function using the evaluation data set and then once that's done creating a video index that is searchable phase three will consist of building an API for this search function containerizing it testing it locally building a simple gradio UI and then deploying it on AWS SP war will consist of creating the documentation and doing a project retrospective so future videos of this series will walk through each of these steps and I haven't built this you're seeing this thing live you're seeing this thing raw so I'm sure there's going to be some changes here things are going to come up that I didn't expect and I'm going to have to come back to phase zero maybe I fall behind on the due date since this is an independent project I don't have a manager I don't have a broader team holding me accountable but committing to these due dates on this video and committing to posting a video every single week on YouTube is helping give me structure to this project and keeping me motivated so everyone's comments from the previous video saying I'm excited for the next video of this series were actually very helpful to me to sit down and be productive and work on this project the next video of the series is going to walk through phase one of this project so everything I described in the previous slide so at a high level this will consist of building a data pipeline more specifically an extract load and transform pipeline so what that'll look like is we'll start by extracting video captions from from YouTube using python then we'll load these video captions into a paret file and then we'll transform these captions into text embeddings so this is our data pipeline that I'll discuss in the next video since I am using an elt extract load and transform Paradigm here technically this will actually be in Phase 2 elt sounds better than e so that's why I put it all here okay so that brings us to the end I hope you got some value out of this video like I mentioned in the previous video this whole series is part of my own own personal learning process so if you have any questions or suggestions on this project please drop those in the comment section below those are very valuable to me and as always thank you so much for your time and thanks for watching
O5i_mMUM94c,2024-04-19T14:05:54.000000,How I’d learned #datascience (if I had to start over) ￼,here's how I'd learn data science if I had to start over I start by making a YouTube playlist with high level introductions to data science and putting together a list of Articles to read on the subject next I'd set up 10 interviews with data scientists to learn from people that are actually doing it I'd ask basic questions like what is data science how did you get into it and do you have any advice for me next I'd do a project learning from others can only get you so far eventually you need to get your hands on the keyboard if I was super green I'd start with a basic project using data from kaggle however if I'm starting with some adjacent experience I try to grab some data from The Real World and solve a real problem then finally I teach someone what I learned by making a YouTube video about the project or writing an article
xm9devSQEqU,2024-04-18T15:59:02.000000,4 Skills You Need to Be a Full-Stack Data Scientist,although it is common to delegate different parts of the machine learning workflow to specialized roles there are many situations which require individuals who can manage and Implement ml Solutions end to end I call these individuals full stack data scientists in this video I will introduce full stack data science and discuss its four hats and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoy this content please consider subscribing that's a great no cost way you can support me in all the videos that I make starting with the basic question what is a fullstack data scientist so the way I'll Define it here is that a full stack data scientist is someone who can manage and Implement an ml solution from end to end in other words they have a sufficient understanding of the entire ml workflow which gives them a unique ability to bring ml solutions to reality so typical ml workflow might look something like this you'll start by diagnosing the business problem and designing an ml solution to that problem next with a design in mind you'll move on to sourcing and preparing the data for solution development then you'll develop the solution so in other words you'll train a machine learning model and then finally you will deploy Your solution so you'll integrate that machine learning model into existing workflows or into a product given the rise of specialized roles for each aspect of this machine learning workflow this idea of a full stag data scientist might seem a bit outdated and this was my thinking when I was working as a data scientist at a large Enterprise where we had a data engineering team and an ml engineering team and I was sitting on the data science team however over time the value of learning the entire Tex stack has become more and more obvious to me the spark for this realization and change in perspective for me happened around last year where I was interviewing top data science Freelancers on upwork one of the key takeaways from these interviews was that data science skills alone provide no value while this might sound like a provocative statement think of it like this if I'm a freelancer I'm probably going to be talking with small to medium-sized businesses and most of the times these businesses don't have a data science function that's the whole reason they're hiring a freelancer and so they often don't have the data infrastructure to provide the foundation for training machine learning models that means if I want to come in as a data scientist and train a machine learning model I need to be able to extract the data prepare the data and make it available for training but it doesn't stop there once the model is trained it needs to be integrated into their existing workflows and again they probably don't have a machine learning engineer on staff that can do this work so in order for the value to be realized that's something I would need to do as a freelancer the data science skills the model training piece of the ml workflow is sandwiched in between the data engineering piece and the ml engineering piece while it is an important part of the workflow it can't even happen if the data aren't available for model training and it can't provide any impact if it's not implemented into the real world however freelance isn't the only context where knowing the full Tech stack is valuable even if you're not a freelancer working with small to mediumsized businesses say if you're a full-time employee at one of these companies they are often in the early stages of their data maturity and AI maturity so you might be the only resource or part of a small team of resources that are responsible for implementing the AI strategy of the company another situation might be you work at a large Enterprise but you're embedded in a team where you are the lone AI contributor so in that situation you may not have a ton of support on the data engineering side or the ml engineering side to implement machine learning Solutions and then finally if you're a Founder that wants to build a machine learning product you're going to need skills from all aspects of the Tex deack because often you're the only person in your company and it's on you to build the product from end to end so that brings us to what I like to call the four hats of a full stack data scientist and each of these corresponds to key parts of the machine learning workflow so hat one is the project man manager so that diagnosing problems and designing Solutions piece of the workflow hat two is the data engineer so the sourcing and preparing of the data hat three is the data scientist so training the machine learning model and hat four is the ml engineer which consists of deploying the ml solution starting with hat one the project manager the way I see it the key role of a project manager is to answer three questions what why and how more specifically what are we building why are we building it and how are we going to build it while this might sound simple enough it's not uncommon for people to gloss over or skip this step entirely perhaps especially for technical folks who really want to dive into the implementation and building the model so it might be like this Meme here the technical folks are more excited about coding than necessarily doing this project management work but the reason it's important is that if you skip over this step you run the risk of spending a lot of time money solving the wrong problem and even if you are solving the right problem you may solve it in an unnecessarily complex and expensive way all that to say taking some time at the outset of any project to stop and think about the problem you're trying to solve and the solution that you want to build can save you a lot of time and wasted effort so the way I see it the key skills involved with the project manager hat are communication and managing relationships the reason for this first one is that as data scientist s or full stack data scientists you're probably not going to be solving your own problems more often than not you're solving other people's problems and what this typically looks like is you're talking with stakeholders to better understand their problem and talk through potential Solutions the next key skill is the ability to diagnose problems and design Solutions diagnose problems comes down to finding the root cause for why something is going wrong and then designing Solutions isn't just about automatically throwing AI at problem but thinking through the value and the costs of each potential solution in making your decision and then the final key skill is being able to estimate project timelines costs and defining requirements and again while this work may not seem as exciting as the technical stuff the coding the implementation Etc doing this step right can save you a lot of headaches down the line next we have hat two which is that of a data engineer so in the context of full stack data science what data engineering is all about is making data readily available for model development or inference data Engineering in this context has one key difference than what we might call traditional data engineering at a large Enterprise at a large Enterprise the bulk of the data engineering work is often optimizing data architectures to support a wide range of business use cases on the other hand in the context of full stack data science the work is typically more product Focus focused while having some understanding of how to design flexible databases is important the type of data engineering work you would do in this full stat context is more concerned with building data pipelines so this will be creating ETL processes which stands for extract transform and load as well as data monitoring so giving visibility to data flowing through your pipeline and all the data related to your machine learning product some of the key skills that come up here are are much more technical than what we saw on the previous slide so python has really become a standard among data Engineers python can be used for a wide range of tasks such as the extract process so scraping web pages or working with apis transforming the data so this could be things like d duplication exception handling feature engineering knowing SQL is a must especially if you're loading this data into a database which is going to be queried in some Downstream task also a basic understanding of command line interface tools while there are a lot of gooey based applications for data engineering being able to use command line tools allows you to automate and scale processes a bit more easily next we have building data pipelines so this could be things like ETL or elt again ETL is extract transform and load while elt is extract load and transform these will depend on the details of your use case that I'll talk about in a later video but common tools for building data pipelines are airflow which is an orchestration tool and Docker which is a containerization tool and then finally while you can definitely have your own servers and compute to acquire and store your data these days it's common to implement these data pipelines and data stores on some sort of cloud platform The Big Three are AWS gcp or Azure next we have Hat number three which is the data scientist hat my definition of a data scientist is someone who leverages regularities in data to drive impact and since computers are much better than us at finding regularities and patterns in data what this often boils down to is training a machine learning model what this typically looks like is you start with the real world which consists of things that you care about what we do is we'll collect data about those specific things that we care about and then we'll use that data to train a model and the model can be used to make a prediction such as the probability that someone will buy our product based on their demographics or their behavior or something else it could be the probability that they don't pay back their credit card bill based on their credit score and other things and so on and so forth there are countless applications of machine learning models but the role of the data scientist doesn't just stop with training the model what's just as important if not more important is how one evaluates the model so defining performance metrics that are meaningful and in the best case scenario the performance metrics you use to evaluate the model can be tied back to key business performance metrics so that there's a clear mapping between model performance and business impact and the nature of training models is very experimental and iterative so what often happens is that you'll go through this whole workflow you'll evaluate the model and you'll learn something and that'll create a feedback loop for the data scientist to update the algorithm used to train the model or the specific hyperparameters used or the specific features used in the model or the data scientist might realize there are inconsistencies in the data which requires to kind of go back to the data engineering step of the workflow and make some changes in how the data are transformed or they may realize that the data aren't sufficient so one has to go back and collect even more data and finally you may evaluate the model and realize that some of of your assumptions about how the real world processes work were flawed for example at the project outset you may have assumed a particular variable was a key driver in something that you cared about but then upon training your model you may realize that that specific variable doesn't have the predictive performance that you initially hoped so this requires you to kind of go back to the drawing board and rethink your assumptions about the specific use case this very iterative and experimental whole process makes data science in many ways more art than science which is one of the reasons I like doing it so much flexus one's creativity but it also introduces a fair amount of uncertainty into the model development process some of the key skills of the data scientist had is probably first and foremost python some common data science libraries include pandas and poers this provides data structures for working with data and ways to manipulate data there's sklearn which is a popular machine learn learning library it has several machine learning algorithms readily available and then finally there are deep learning libraries like tensor flow or pytorch which allow you to train neural networks another key skill is exploratory data analysis even before you train the model there's usually a step in between here where one looks at the data looking at distributions looking how different variables track with one another if there any missing values if there any duplicates double checking the quality of the Data before training a model on it and then finally model development and everything that goes into that one of the most important things is experiment tracking because there are so many knobs in dials that go into training a model such as the algorithm you choose the hyperparameters for the algorithm the train test split of the data the features used in your model the performance of the model and keeping track of all this information is very valuable when you're trying to discover the best performing model the fourth and final hat is that of the ml engineer and what this consists of is turning your machine learning model into a machine learning solution what I mean by that is a model in of itself provides very little value it's probably going to be implemented in Python so it's going to require you to run a python script and provide a particular data and then it'll spit something out and that output may not be inherently meaningful by itself so taking that model and embedding it into an existing workflow or a broader software solution is a critical part of this process so a very common way of deploying machine learning models is as follows you'll take your model and you'll containerize it which means you'll create a modular version of the model that can be deployed in many different contexts and this is typically done using Docker however the model just sitting in this container still doesn't provide a whole lot of value so a simple way to allow this container to talk to external applications or external workflows is by adding an API to it what this allows is that if you have some AI app let's say this is an internal website that allows employees to look up specific information users can write in a query into the user interface this will get sent to the machine learning model and then the API will spit back a response this is a very common and simple design you just have a container container version of the model and you slap an API on top of it two popular tools for doing this are Docker for the containerization bit and then fast API which is a python library that allows you to create apis for Python scripts however some use cases may not be so simple and require more sophisticated Solutions you know maybe something like this where you have your production model with the API SLA on top of it but you also want to do consistent model monitoring and you want to do some automated model retraining say every single month so if you want to do the retraining you're going to have to ingest data in an automated way do the ETL process to put it into a database maybe you also want to do some data monitoring so you slap that on top of the database as well this process data gets passed to a model retraining module which then gets pushed to the production model after some automated checks or something like that so for these more sophisticated Solutions you're probably going to want to use an orchestr ation tool like airflow which provides a abstract way to connect these different pieces of software together so the key skills for the ml engineering had is to containerize scripts using Docker and to build apis using perhaps fast API another key skill is orchestrating multiple data and machine learning processes together so connecting data and ml pipelines and a popular tool for that these days is airflow and then again while you could Implement all these Solutions on your local machine or some local hardware it's common practice these days to deploy these Solutions in the cloud while I've been doing data science for 5 years I would say I am just at the beginning of my journey toward becoming a full stack data scientist and while it might seem like this daunting and overwhelming task of learning the full Tex stack the way I think about it is that it's not about learning everything it's not about learning every single detail and skill involved in the machine learning workflow but rather it's about learning anything necessary to implement your particular solution and so the way I see it the best way to become a full stack data scientist is taking a more bottomup approach as opposed to a top- down approach as problems arise learn just enough to solve that problem and move on to the next thing on this journey of becoming a full stack data scientist here are three principles that I'm personally following the first is to have a reason to learn new skills there are many ways one can do this I'm personally building out my own projects and products both as a way to learn and as a way to solve specific problems that come up for me however there are other ways Beyond personal projects you know freelancing is a great opportunity instead of solving your own problems you're solving other people's problems which are going to require you to learn all aspects of the tech stack and indeed most of the Freelancers I know have skills across the entirety of the text de the second is to learn just enough to be dangerous this goes to this idea of not worrying about learning every single little detail but learning whatever is necessary to solve the problem in front of you then finally to keep things as simple as possible there are countless tools Technologies libraries Frameworks Solutions best practices for doing machine learning these days and it's easy to get so caught up in the best PR practices and what's scalable that you end up over complicating the project so in my view Simplicity is the best guide for building machine learning Solutions this video is part of a larger Series in upcoming videos I will Implement a machine learning project end to endend walking through each of the four hats discussed here so specifically I'm going to build a semantic search system that allows people to search across all of my YouTube videos I'll walk through each hat where I'll have a video for each one of these I'll do the project manager hat walking through AI project management estimating time and costs defining requirements I'll do the data engineering stuff which is walking through the data acquisition building the data Pipeline and creating the data store then hat three I'll walk through the solution development the experimentation phase and then evaluating the solution finally I'll do a video on the ml engineering so deploying the solution the container process and building an API so that brings us to the end I hope you got some value from this video this video and the others in this series are all part of my own personal learning process toward that end if you feel like anything's missing or you have suggestions for future content I invite you to drop those in the comment section below those are very valuable to me personally and as always thank you so much for your time and thanks for watching
Z6CmuVEi7QY,2024-04-11T10:00:27.000000,How I'd Learn Data Science (if I started over),when I was first learning data science it was easy to get overwhelmed by the mountain of buzzwords and technical details and hard to know exactly where to start while the number of buzzwords has only seemed to increase since I started I now have a much clearer view of the space and its essential elements in this video I'll answer one of the most common questions I receive which is how would you learn data science if you had to start over today I can honestly say this is the exact approach I would use because I'm currently foll following it to learn data engineering and ml engineering and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoyed this content please consider subscribing that's a great no cost way you can support me in all the videos that I make I've been doing data science for the past 5 years in a few different contexts including research freelance and corporate one of the biggest challenges of learning data science is that it consists of a constantly changing ocean of Technologies the best way I found to cope with this constant change is to just get really good at learning and the way I see it there are four ways we can learn the first way is through consuming content this includes reading books taking courses watching YouTube videos second is through mentorship and learning from those who are ahead of you third is by doing so getting your hands on the keyboard and the fourth is by teaching when it comes to learning a comp completely new subject or field I combined these four methods into a four-step strategy here I'm going to walk through what that might look like for data science step by step the first step is to consume content this is especially important if I know absolutely nothing about the field I'm always amazed by the amount of clarity I can gain from Reading one good article or watching One Good YouTube video on a subject unfortunately what makes a good resource will vary from person to person for example I prefer to read revieww papers and other scientific articles when available however if you would have given me a paper to read when I was an undergrad I probably wouldn't have read it and even if I did I probably would not have gotten much out of it so it's important to find which modalities work best for you to get started I'd go to Google Scholar and search something like data science review here I'd be mainly looking for titles that sound interesting and relevant and looking at the number of citations each article has received to be clear this isn't meant to be a literature review so I'll typically pick out one to three articles that seem relevant next I'd go to YouTube and search something like what is data science similar to Google Scholar I'd evaluate videos based on their title and number of views so while doing these searches it's important to point out that I'm not actually reading these papers or watching these videos I'm just getting all these resources together to consume them later to actually consume the content I'll block time on my calendar specific spefically for learning this is important because it increases the probability that I'll actually do it I personally like to do my reading first thing in the morning while watching YouTube videos is something I can really do anytime either way I'll have my notes app open while consuming the content to take notes on key things that stand out to me and ensuring I site my sources in case I want to come back to them later or share it with anyone else although I recommend you do this kind of search for yourself I've linked a few resources in the description below in case it's a helpful jumping off point after getting a basic grounding of data science through consuming content I start reaching out to professional data scientists to see if they'd be willing to talk to me about their experience learning from those ahead of me is one of the greatest hacks I've learned for growth this is something that I did when first exploring entrepreneurship in the data space and something I'm doing currently in learning data engineering and ml engineering I found that interviewing 10 people is a good number because it gives a nice diversity of perspectives to help give a broad view of the field but of course getting 10 Busy professionals to talk to you is much easier said than done here's how I would do it I'd go to LinkedIn and search for data scientists that I'm directly connected with then I'd shoot them a DM saying something like hey hope things are going well I'm currently doing interviews with data scientists as part of an effort to expand my technical data skills would you be willing to do a call sometime this month if the person doesn't respond within a few days I'd follow up with something like hey I wanted to follow up did you get a chance to look at my previous message doing this recently for data and ml Engineers more than 50% of people responded to my messages of course this will vary from person to person but if we assume that 50% of the people you reach out to respond and of those 50% another 50% agree to talk to you that means you need to reach out to 40 data scientists in order to talk to 10 of them I'm personally connected to hundreds of data scientists on LinkedIn so this would be no problem for me however if I was only connected to say 20 data scientists I'd take the following approach I'd add more personalization to that initial reach out and be sure to follow up a third time for those who don't respond to the first two messages this should increase the number of people that actually respond to my request if this doesn't get the 10 interviews I'd expand my Outreach to data scientists I have at least one mutual connection with reaching out to strangers naturally has a lower success rate so personal iation and follow-ups are even more important for these contexts when actually getting on the calls with these professionals I'd ask basic questions like what is data science how did you get started what kind of projects are you working on these days what does your Tech stack look like and do you have any advice for me I'd be sure to take notes on these calls so I can compare different interviewees perspectives and do any deep dives into specific tools or resources that they mention in other words i' jump back to Step One to to fill in any gaps that may have come up during the interviews consuming content and learning from others can only get you so far eventually you need to get your hands on the keyboard and start doing this is where doing a data science project comes in if I was completely new to programming and analytics I would start with a very basic project like creating a simple data visualization what this might look like is installing python on my machine downloading an interesting data set from kaggle loading it in with pandas or polers processing the data in some way whether that's cleaning the data or doing some sort of analytics and then finally visualizing the data with matte plot lip if I was starting with some adjacent experience in say python or statistics I'd try to grab data from The Real World and solve a meaningful problem some examples of this might be creating a model to automatically create chapters for my YouTube videos predicting the click-through rate of a video based on the thumbnail and the title or building a web app that allows people to do semantic search over all my YouTube videos and blogs I find that doing projects to solve real world problems are not only more rewarding but much more instructive than purely academic ones as I make progress on the project I'd likely be jumping back to steps one and two as needed so to either read up on a specific approach or technology or to ask people with more experience for guidance in case I get stuck once I've completed my first project i' move on to the final step of teaching in my opinion teaching is the ultimate way to learn and the main reason why I make YouTube videos when explaining something to others I'm forced to distill my understanding into clear and concise language in this process I find that I'll stumble upon gaps in my understanding that I didn't realize were there this provides me the opportunity to bridge these gaps and learn more the three teaching modalities that I found most helpful are making YouTube videos writing medium articles and giving presentations I really like the first two methods cuz I can do them anywhere at any time the third method is also great because I get the added pressure of talking to a live audience and taking their questions but of course this requires a bit more work to make happen so to keep things simple I'd make a slide deck outlining my project and key learnings and then make a YouTube video of me walking through it although many will agree that teaching is a great way to learn most won't follow through on this they might be thinking what if I say the wrong thing what are people going to say about me what if I embarrass myself the best strategy I found for overcoming these fears is focusing on my own personal development and growth in other words it doesn't matter what other people think as long as I'm learning and for me these fears actually serve as F to help make sure that I really understand what I'm sharing and in the inevitable case that I do make a mistake that's beneficial too because those those public mistakes tend to stick much better than the private ones while this four-step strategy is simple putting it into practice may present some unexpected challenges to help with that here are three key habits I find helpful for learning first is to make room for it like I mentioned earlier I find it critical to block time for learning to ensure that it actually gets done for me this has helped develop this habit for continual learning the second is be willing to look dumb one of the biggest mistakes I made early in my educational career was an unwillingness to look dumb what this typically looked like was staying quiet and not asking questions when someone was telling me something I didn't really understand when I overcame this fear in grad school it unlocked a whole new level of learning and progress and third pursue curiosity again data science is an ocean of Technologies even if you spend every waking moment of everyday learning you still wouldn't come close to learning everything that's why I find it much more productive to pursue topics that peique my interest this is what makes data science fun even if the topics are technically challenging at first so that brings us to the end I hope this video was helpful to you in some way if you have any data science questions feel free to share them in the comments below or feel free to set up some office hours using the link in the description and as always thank you so much for your time and thanks for watching
INlCLmWlojY,2024-04-04T18:45:00.000000,I Was Wrong About AI Consulting (what I learned),"last year I quit my corporate data science job to pursue entrepreneurship full-time my plan was to sell data science Services as a way to fund the development of a product I could build a business around while this made a lot of sense on paper pursuing this path over the last N9 months has made me realize this plan was flawed in this video I'm going to share my experience and some key Lessons Learned in case it is helpful to anyone on a similar journey and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoy this content please consider subscribing that's a great no cost way you can support me in all the videos that I make right out of grad school I went to work as a data scientist at Toyota this was in many ways my dream job and an incredible learning experience for me however after about 6 months in the role that initial excitement and learning curve began to flatten out and I slowly began to realize that the role was no longer aligned with my longer term goal of running my own business so after about a year in that role I decided to pass on a senior data scientist promotion and tank my income from over 10K a month down to basically zero since I had done some freelance work in grad school and had grown a small audience on YouTube my plan was to bring these things together and leverage my content to sell consulting services and to my surprise it worked over the next 8 months I took 36 Discovery calls of these 36 calls two of them turned into contracts and last month one of these contracts turned into an even bigger opport opportunity of over $25,000 where I was sitting in a project manager role and not doing any of the coding myself while it may sound like things were going great something was off this was similar to what I felt facing the promotion at Toyota it was a great opportunity on paper but something about it didn't feel aligned with my long-term goals so I made the tough decision to pass that opportunity off to another consultant looking back it's clear that my expectations of Consulting didn't match reality when I started this journey I saw Consulting as an easy way I could make cash while I explored other Ventures however after pursuing it as my main source of income as opposed to a side hustle like I did in grad school it became obvious that running a Consulting business wasn't as simple as I expected not just because of the technical challenges of building AI projects but also selling yourself nurturing leads working with subcontractors and the list goes on and on in fact most of the work were these non-technical aspects of the job with the biggest piece being the sales process as I've learned there are many unique challenges in selling AI Services three of which are as follows one for most businesses AI is a nice to have rather than a musthave so a lot of times it's not the client's number one priority two building AI projects requires a lot of experimentation and iteration which introduces a lot more uncertainty than the traditional software development process and reduces the perceived value of your offer and three since these are typically High ticket contracts they often require multiple touch points with the client before they close and I found this extra time commitment difficult to manage as a solo operator although I was learning a lot Consulting was taking up much more of my time and attention than I had anticipated so much so that my content output began to slow down and I virtually had no time to work on my own projects which was supposedly the main goal of all this this experience led me to take a step back and reminded me of some advice I had received from a successful product entrepreneur about a week after quitting my job I had asked him if Consulting was a good stepping stone to product development to which he immediately responded no the advice he gave was simple if you want to build a product then build a product looking back it's kind of funny that it took me 9 months to realize what he had told me 9 days into this journey but here's what I didn't fully appreciate building a product is hard building a consultancy is hard building a brand is hard entrepreneur ship is just hard the trick at least in my opinion is to pursue the hard thing that gets you fired up and that you find fulfilling and after trying it for 9 months I realized that selling AI projects to clients didn't get me as fired up as some of the other things I was working on that's why last quarter I removed the discovery call option from my website and passed that first major contract off to another consultant although building a consultancy wasn't for me I still believe it's a great business for those who enjoy it it also taught me a ton about sales marketing and working with customers which are universally applicable skills and Entrepreneurship if I had to boil it down here are my four key takeaways from this experience first is trust is more important than anything else for me what differentiated clients from prospects was the belief that I could solve their problem and that I was on their side through a lot of trial and error I eventually landed on the following approach be curious be transparent and be yourself more specifically be curious about the client's problem and where they're coming from be transparent about the limits of my skills and knowledge and to just be myself not trying to put up a front and pretend to be something that I'm not the second takeaway was not to skip the discovery when providing Technical Services like data science it's easy to dive head first into the coding the problem with this is that people end up spending a lot of time and energy solving the wrong problem that's why at the outset of every project it's critical to put on your project management hat so you can understand the business problem and fully scope a proposed solution the third takeaway is to find your one sales Channel although there are countless ways you can get clients upwork Fiverr cold Outreach LinkedIn content creation speaking at conferences referrals and the list goes on and on I and most of the people that I've interacted with in the space have just one main lead source for me my main source was my YouTube channel Channel and my funnel looks something like this someone would watch a YouTube video book a discovery call after the discovery call we would do a paid Discovery phase where the goal was to get a clear understanding of the client's problem and to scope out the project requirements and goals following the paid Discovery is building a proof of concept and then after the POC building an MVP the fourth and final takeaway is that it's not real until the money's in your bank account this is a lesson I had to learn over and over again and maybe I still haven't learned it there were many times I would have a great discovery call or multiple calls with prospects and it seemed like they were ready to move forward but then days and weeks would go by and I wouldn't hear from them and so while there's always excitement in sales I had to adopt this mindset to avoid going on these weekly emotional roller coaster rides at this point you might be thinking sha if you're not selling your data sign skills how are you going to make money while contract work has great short-term earning potential it is not my only Revenue source there are three other ways I've generated Revenue these past 8 months this includes revenue from my YouTube channel my medium blog and ad hoc paid Consulting calls which have generated a total of $766 38 although this isn't enough to pay the bills there's another thing here that's worth taking into consideration since quitting my job my YouTube channel has grown from 2,000 subscribers to 18,000 subscribers along with that my revenue from YouTube went from $100 in the first 3 months to 1,600 in these past 3 months which brings me to my new plan post one YouTube video a week while this might sound like an overly simplistic and also super risky plan here's my reasoning one YouTube is actually working for me two it allows me to focus on one thing three making one video a week gives me a clear quantifiable goal I can use to structure all of my efforts for instance here's a list of things that can go into making a YouTube video reading papers writing medium articles writing code examples talking to people conducting interviews building projects workshopping content ideas on other social media platforms and probably a lot more now here's a list of things that can result from making a YouTube video learning a new skill or topic getting more paid calls more speaking gigs more inbound leads more people joining the data entrepreneurs more content from my other channels and growing my audience nevertheless committing to one thing is scary especially something unpredictable like YouTube however the longer I spend on this journey the more I realize that commitment and focus are necessary ingredients for Success because this is the only way that every ounce of your effort can go in the same direction and to quote a fellow entrepreneur and friend Michael Lynn if you're doing less and less that means you're going in the right direction and indeed this feels like the right direction at least for now 9 months into this entrepreneurship journey I have three Reflections that are top of mind the first is I could have a very successful Consulting business and I could have a very successful YouTube channel but I can't have both I have to pick one and personally I just like making YouTube videos more the second is a subtle mindset shift which is instead of asking yourself will this thing work ask yourself how could I make this thing work it may seem like a subtle shift but this is the mindset that I'm adopting this quarter in making YouTube my main focus and the third and final mindset is to trust yourself trust that you'll figure it out trust that if you're backed into a corner your survival Instinct will kick in and you will solve the problem thanks for watching to the end I hope you got some value out of this if you have any specific questions about my journey feel free to drop them in the comment section below and as always thank you so much for your time and thanks for watching"
sNa_uiqSlJo,2024-03-29T15:57:34.000000,"Text Embeddings, Classification, and Semantic Search (w/ Python Code)","although there's a ton of excitement these days around llms and AI agents I'm more excited about the recent Innovations in text embeddings we saw these in the previous video of this series where we use text embeddings to implement a rag system to improve in llm in this video I'll discuss text embeddings in Greater detail and share two simple yet high value use cases namely text classification and semantic search and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoyed this content please consider subscribing that's a great no cost way to support me in all the videos that I make there's a fundamental challenge in trying to analyze text put one way text isn't computable meaning we can't do math with text in the same way that we do with numbers for example say you go to a networking event and you're talking with other Professionals in the data space if you wanted to summarize the typical height of every person at the networking event this is something that would be pretty straightforward all you would need to do is measure the heights of everyone at the networking event and then you can summarize these Heights using something like an average and this is something that you can easily compute using a calculator Excel or your favorite programming language however if you have the job descriptions of everyone at this networking event summarizing everyone's roles wouldn't be as straightforward because there's no cell function or mathematical operator that allows you to put in text and generate a summary and so this is where text embeddings come in and put simply text embeddings translate words into numbers so if we have those same job descriptions from the networking event translating these descriptions into text embeddings would look something like this for every single description here we would map it to a set of number numbers but these aren't just any set of numbers these are numbers that capture the inherent meaning of the text in other words text embeddings translate words into meaningful numbers and one way we can see this is take all those numbers from the previous slide and use them to visualize all the job descriptions and so that would look something like this where the numbers that we generated in the text embedding Define the location of each person's job description we can see the data analyst in retail with 5 years of experience is somewhere here and they are located next to a freelance data visualization specialist with four years of experience however these people are relatively far away from this guy who is a data architect with 15 years of experience and so this is the way that text embeddings capture the meaning of the underlying text namely job descriptions that are similar will be located close together while job descrip descriptions that are very different will be located far away from each other from this view the bi analyst in Hospitality early career is very different than the data architect with 15 years of experience so if you've used Chad GPT in the past the thing that might come to mind is sha why should I care about these text embeddings and translating text into numbers can't I just pass all my text to chat GPT and have it figure out what I'm trying to do and the answer is yes there are many tasks that don't require these text embeddings and they're much better suited for chat GPT for example if we wanted to summarize the eight job descriptions from the networking event it would actually be super simple to pass all these to chat GPT and generate a summary however if you have a use case that's not just some oneoff low stakes task but it's something you're trying to integrate into your product or a website or some broader production level Software System then you'd probably want to at least consider using text embeddings before running off and building an AI agent and the reason is if we're talking about AI assistance it's really the early days for this type of technology and there are still a lot of things that we don't understand about using these large language models in this way another downside of using an AI assistant for your use case is that there's a major computational cost associated with running these Large Scale Models another downside is the inherent security risks that come with building llm based applications and so there's a nice list of top 10 llm security risks at reference number three which I recommend you check out for example these are things like prompt injection which are maliciously crafted prompts which get the llm to expose private or confidential data or disrupting some expected decision-making pipeline finally responses from these llm based systems might be unpredictable and prone to hallucinations the system generates fictitious or unhelpful information on the other hand text embeddings have been around for decades and there are several past example applications that we can look to to help guide future applications and use cases another upside of using text embeddings is that they have a much lower computational cost and thus Financial cost than using an entire large language model another is that there are far fewer security risks associated with using embedding model compared to a large language model that users can access through a chat interface finally the responses are more predictable because you know every word or piece of text Will generate a deterministic set of numbers so there's no Randomness in this process now that we have a basic understanding of what text embeddings are and why we should care about them let's see how we can use them for a practical use case the first use case I'm going to talk about is text classification which is the process of assigning a label to a piece of text for example if we were to take all the people from our networking event and their Associated job descriptions a classification task could be trying to determine which people are data analysts and which people are not data analysts based on their respective job descriptions so that might be pretty simple here because anything to the left of this blue line we could label as a data analyst and anything to the right we could label as not a data analyst however text classification is a broadly applicable use case some other situations might be classifying emails as fishing attacks versus not fishing attacks or classifying credit applications as fraudulent or not fraudulent and the list goes on and on and on with this high level understanding let's see what text classification with text embeddings looks like in code so in this example I'm going to take a data set of resumés and try to classify them as either data scientist or not data scientist based on the content of each resume the code in the data set for this example is freely available on the GitHub repository linked here you you to steal this code as a jumping off point for your own use case so the first thing we're going to do is import some helpful libraries here we're going to use open AI text embeddings to use open AI embedding models we'll need to use their their API which requires a secret key if you're unfamiliar with the open aai API or how to get a secret key I walk through that step by step in a previous video of the series next I'll import pandas numai and matte plot lip so we'll use pandas to structure the data we'll use numpy to do math and we'll use map plot lib to visualize things along the way and then finally we will import some things from s learn specifically their random Force classifier which is just a machine learning technique for doing classification and then the ROC Au score which is a way to evaluate a classification model's performance here we're going to import our data from a CSV file and the CSV file is available on the GitHub repo just one note here the data used in this example and the next example were actually synthetically generated using GPT 3.5 turbo although I don't like using synthetic data in these code examples I went with the fake data set here to avoid any privacy issues in using real people's resumés nevertheless this example is instructive in how you can apply text classification to text embeddings next we're going to generate our embeddings so here I'm going to define a function to do this for us basically what this function will do is take the text and my open AI secret key and it'll spit out the text embedding object the way we do that is we set up communication with the open a API and then we just make an API call by passing at the text and specifying which model we want to use this is one of open ai's newer embedding models and it has about 1,500 Dimensions so each piece of text is going to be translated into about 1,500 numbers and then they have another option which has about 3,000 numbers and that's called text embedding 3 large and then the function will return the API call response and then here for every resume in our data set we can generate the embeddings in one line of code then we can extract these text embeddings and store them in a list in this line of code here then we'll store all the text embeddings into a new pandas data frame called DF train so here I'm automatically generating column names so these column names will be embedding underscore 0 embedding underscore 1 embedding underscore 2 all the way up to embedding score like 15 35 or something however many embedding Dimensions there are then we can create the pandas data frame in one line of code so just by passing in this list of text embeddings and the column names and then I'm going to add one more variable to this data frame which is a true false a Boolean variable which indicates whether that row is associated with a data scientist's resume or not a data scientist's resume and then we can split our variables into a set of predictors so X will be all the text embeddings for every one of our resumés y will just be the true false is data sign scientist is not data scientist variable and then we'll pass this to the random Force classifier to train a model so just in one line of code we train our classifier we can print the accuracy and Au value of the model applied to the training data so the output here is one indicating perfect performance of the model on the training data which is super suspicious so let's evaluate the model on a different data set here we're going to load in a testing data set so this is also available on the GitHub repository here we're just repeating the same steps as we did for the training data we'll read it in as a pandas data frame we'll generate the embeddings in one line of code we'll extract the embeddings as a list then we'll create a new pandas data frame for the testing data and then we'll create a Target variable of is data scientist for the test data and then we'll split it into a set of predictors and the target variable with that we can simply evaluate the model on this new data set with the two lines of code and we see the model still has very good performance on the training data however before we kind of celebrate the model is still likely overfitting here and there are two reasons for that first is we have 1,537 predictors but only 100 records in these situations where you have a ton of predictors and very few examples it's not hard for the model to memorize the training data and overfit to it secondly the training data and testing data were generated in identical ways using GP PT 3.5 turbo and so this is one of the downsides of generating synthetic data is that they're going to be artifacts in the data generating process which may create similarities between our training data and testing data that won't be relevant to resumés from The Real World so while there are many ways we can solve the overfitting problem in this situation the best thing would be to one get many more examples of résumés so 100 RS isn't going to cut it probably want close to 1,000 or 10,000 résumés and then second if you want to use this model on real world resumés you should train it on real world resumés moving on to the second use case of semantic search the big idea here is that semantic search returns results based on the meaning of a user's query in contrast to keyword search which looks to match specific words or phrases in the query to a set of search documents or web pages and so the way this works in the context of text embeddings is that say we have a query like I need someone to build my data infrastructure if we were to take everyone from our networking event once again and embed their job descriptions in this concept space here the way we can do semantic search is we can embed the user query into this same space and if we were to do that the query might sit somewhere over here then if we wanted to return search results we could look at the three nearest job descriptions and return them in our search results and for this particular query keyword search may not work so well because there's no mention of the typical role that would be able to build a data infrastructure namely a data engineer however semantic search is able to connect this task of building a data infrastructure to things like data engineer or building data pipelines or ETL process or building a data model and other related skills and activities to building a data infrastructure which may not share the same exact wording here we're going to use text embeddings to implement semantic search in exactly the same way as we saw in the example on the previous slide so we'll use the same data set as in use case one and what we'll do is we'll take a user query such as I need someone to build my data infrastructure and return resumés in our data set which match that user's query again we're going to import some handy libraries we have numpy and pandas and next we import sentence Transformers which is actually an open-source library with many embedding models that you can use completely for free so no need to get an API key or pay to make API calls to open ai's API next again we're going to import things from Psychic learn namely this PCA function function which allows us to do dimensionality reduction and if you're unfamiliar with PCA I actually talked about this in an old video so I'll link that on the screen so you can check it out if you're interested and then we'll import this distance metric function which will allow us to measure the distance between the query and all the different resumés in our database finally we'll import matplot lib to visualize some things okay so here we'll read in the data in the same exact way as before so we'll read in this rumore train.csv file one thing we'll do quickly is that this CSV file has a column called rooll and then there are a handful of different roles but there's one of them that's a sentence it's not a specific role we'll just relabel all those elements as other next we're going to generate the embeddings so the sentence Transformer Library makes this super easy so all we do is import the model and then we can use this do encode method to pass in all the resumés and generate embeddings for all of them and so there are several embedding models to choose from in the sentence Transformers Library here I use this one all mini LM L6 V2 which was specifically designed to do this semantic search task if you're trying to do text classification or some other NLP task I would check out their pre-trained models in their library and try to tailor the model for your specific use case next we can visualize all the different resumés and roles in this embedding space in the following way what's Happening Here here is that I used PCA to reduce the dimensionality of the embedding space from 384 down to 2 so it can fit on this 2D plot here and since this is a lot of code to do the data visualization I don't want to show it here but it's available on the GitHub for anyone who's interested here we can see that even though we're flattening this embedding space from 384 Dimensions down to two we can see that the different roles such as data scientist data engineer machine learn learning engineer AI consultant data entrepreneur and other they tend to be localized pretty well in this visualization so this is a good sign because if we have a query that aligns with a data scientist it will likely be close to a lot of data scientists that can fulfill that role next we can Define our query so we'll use that same I need someone to build out my data infrastructure and then we can incode it in the same exact ways we encoded the resumés using this do encode method that'll generate a one dimension numpy array called query embedding then we can compute the nearest neighbors and so the way we do that is first I'll Define this dist object which is a distance metric and then we can compute the pairwise distances between the query embedding and all the resume embeddings in this embedding array that we defined earlier all these hairwise distances will be stored in discore Array which is a numpy array of the distance between the query and every single resume in the data set and then we can use this ARG sort function from numpy to sort all of the distances in ascending order note that this isn't sorting the disc array it's actually doing two things it's sorting the disc array in ascending order and then returning the index of each of those elements so the first element is not going to be the smallest distance value itself it's going to be the index of the smallest distance value that's all we need to do the semantic search so what that'll look like is if we want to print the roles of the top 10 results we can go back to our original DF resume data frame look at the roll column and then we can just return the first 10 elements in this disc array sorted array that'll look like this so the query I need someone to build out my data infrastructure returns a ton of data engineers which is a good sign because those are the people that will be able to do that another thing we can do is to look at the resume of the top result and so we can do that in this way we go back to our DF res data frame look at the resumé column and then return the resumé that corresponds to the first element in this Idis array sorted array and so the resume looks like this just reading through this highly skilled and experienced data engineer with a strong background in designing implementing and maintaining data pipelines proficient in data modeling ETL processes and data warehousing Adept at working with large data sents and optimizing data workflows to improve efficiency and so notice that nowhere here does it say built data infrastructure but it does say all these things that go into building data infrastructure designing implementing maintaining data pipelines data modeling ETL processes data warehousing scalable data pipelines optimiz ETL processes data architectur there's so many buzzwords and so much jargon in this space and just matching keywords together may not be super helpful but when you use text embeddings which captures the underlying meaning of the text it tends to give good results visualizing these queries in the embedding space it'll look something like this so that same one I need someone to build out my data infrastructure we see that the query is pretty close to all the data Engineers which is a good sign another one is project manager for AI feature development so this one tends to be closer to the machine learning engineers and the AI Consultants and it's kind of like on the border between the two which makes sense because you're going to need an ml engineer for the feature development but you probably want an AI consultant for the project management side of the project and so probably a mix of those two skills will be best and then finally a data engineer with Apache airflow experience pretty close to the data Engineers but it also seems close to this other category which is just randomly generated resumés okay so before celebrating I want to kind of zoom into this particular query here data engineer with Apache airflow experience for this simple semantic search example it seems to struggle with specific search requirements such as I want a data engineer but I specifically want them to know Apachi airflow and so when I pass this into the semantic search system only one of the top five results had airf flow listed on their resume and of the top five it was the third one that had it even though there were three other resumés in the data set that had Apache airflow experience and so this example illustrates that semantic search isn't better than keyword search in all situations each of them has their pros and cons and so if you want to build a robust search system you'll likely want to employ both keyword-based search and semantic search to get the best of both worlds which brings us to a few additional strategies for improving a search system first is hybrid search and so this is exactly what I just mentioned which is bringing together keyword-based search and semantic search while there are many ways we can bring these two approaches together a simple method would be given a user's query apply a keyword based search to it so you'll filter down the search results based on specific words in the query such as data engineer or Apache airflow and then from those results you'd apply the semantic Sur search another option is to use a ranker and so a ranker is a special type of model which takes in a query in a document for example and it spits out a similarity score notice that this is an alternative way to compute the similarity or the difference between two pieces of text common way to use a ranker is let's say you use your semantic search system and you return the top 25 search results from the semantic search you can use the ranker to take those 25 search results and compare them with the query to do an additional ranking and empirically this seems to have pretty good results especially when integrated into a rag system and then finally you can fine-tune an embedding model for a specific domain or use case one of the downsides to these embedding models is that they tend to be trained on a large Corpus of text so while this makes them very very good at general purpose tasks they may fall short in specific domains where there may be heavy use of Jaron for example if I wanted to further improve this semantic search system navigating all the jargon of data engineering ml engineering and data science I could Additionally fine-tune the embeddings on this type of text so if you enjoyed this video and you want to learn more check out the blog published in towards data science and although this is a member only story you can access it completely for free using the friend Link in the description below while this does bring us to the end of the llm series at least for now if there's anything that wasn't included in the series please feel free to drop that in the comment section below and perhaps the series will be resuscitated once again in a few months and as always thank you so much for your time and thanks for watching"
Ylz779Op9Pw,2024-03-18T17:32:21.000000,How to Improve LLMs with RAG (Overview + Python Code),this video is part of a larger series on using large language models in practice in the previous video of this series we saw how we can efficiently fine-tune a large language model to respond to YouTube comments in my likeness while the fine-tune model did a really good job at capturing my style when responding to most YouTube comments it didn't do so well when responding to technical questions which required more Niche and specialized knowledge in this video I'll discuss how we can improve llm based systems using retrieval augmented generation or rag for short I'll start with a highle overview of rag before diving into a concrete example with code and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoy this video please consider subscribing that's a great no cost way you can support me in all the content that I make a fundamental feature of large language models is the ability to compress World Knowledge the way this works is you take a huge slice of the world's knowledge through more books and documents than anyone could ever read in their lifetime and you use it to train a large language model and what happens in this training process is that all the knowledge and Concepts and theories and events that have happened in the world that are represented in the text of the training data they get represented and stored in the model's weights so essentially what has happened is we've compressed all that information into a single language model well this has led to some of the biggest AI Innovations the world has ever seen there are two key limitations for compressing knowledge in this way so the first limitation is that the knowledge that is compressed in a large language model is static which means that it doesn't get updated as new events happen and new information becomes available and for anyone that's used chat GPT and tried to ask it a question about current events probably have seen a message like this as my last update in January 2022 I don't have access to real-time information so I can't provide specific events from February 2024 the second limitation is that these large language models are trained on a massive Corpus of text the result of that is that they're really good at general knowledge but they tend to fall short when it comes to more Niche and specialized information mainly because the specialized information wasn't very prominent in their training data and so when I asked chbt how old I was it said that there was no widely available information about shahim tab's age he might be a private individual or not widely known in public domains one way we can mitigate both of these limitations is using retrieval augmented generation or rag for short starting with the basic question what is rag this is where we augment an existing large language model using a specialized and mutable knowledge base base so basically we can have a knowledge base that contains domain specific information that is updatable where we can add and remove information as needed the typical way we'll use a large language model is we'll pass it a prompt and it will spit out a response this basic usage will rely on the internal knowledge of the model in generating the response based on the prompt if we want to add rag into the mix it would look something like this so instead of starting with a prompt we'll start with say a user query which gets passed into a rag module and what the rag module does is that it connects to a specialized knowledge base and it will grab pieces of information which are relevant to the user's query and create a prompt that we can pass into the large language model and so notice that we're not fundamentally changing how using the large language model it's still prompt in and response out the only thing we're doing is augmenting this whole workflow using this rag module which instead of passing in a user query or prompt directly to the model we just have this pre-processing step to ensure that the proper context and information is included in the prompt one question you might have is why do we have to build out this rag module can't we just fine-tune the large language model using specialized knowledge so that we can just use it in the standard way and the answer to that question is yes so you can definitely fine-tune a large language model with specialized knowledge to teach it that information so to speak however empirically fine-tuning a model seems to be a less effective way of giving it specialized knowledge and if you want to read more about that you can check out Source number one Linked In the description below with this basic understanding of what rag is let's take a deeper look into this rag module to see how it actually works the rag module consists of two key elements first is the Retriever and second is the knowledge base so the way these two things work together is that a user query will come in it'll get pass to the retriever which takes the query and searches the knowledge base for Relevant pieces of information it then extracts that relevant information and uses it to Output a prompt the way this retrieval step typically works is using so-called text embeddings before we talk about how we can use text embeddings to do search let's talk about what they are exactly put simply text embeddings are numbers that represent the meaning of some given text so let's say we have a collection of words like tree lotus flower daisy the sun Saturn Jupiter basketball baseball satellite spaceship text embeddings are a set of numbers assoc associated with each word and concept that we're seeing here but they're not just any set of numbers they actually capture the meaning of the underlying text such that if we are to plot them on an XY AIS similar concepts are going to be close together while Concepts that are very different from each other are going to be spaced far away here we see plants tend to be located close together celestial bodies tend to be close together these Sports balls tend to be close together and things that you typically see in space tend to be close together and notice that the balls are closer to celestial bodies than they are to say plants because perhaps balls look more like celestial bodies than they do plants and trees so the way we can use this for search is say each of these items is a piece of information in our knowledge base you know we have some description of this tree a description of this lotus flower the description of Jupiter and so on and so forth what we can do is represent each item in our knowledge base as a point in this embedding space and then we can represent the user's query as another point in this embedding space and then to do search we simply just look at the items in the knowledge base that are closest to the query and return them as search results that's all I'll say about text embeddings and text embedding base search for now this is actually a a pretty rich and deep topic which I don't want to get into in this video but I'll talk about in the next video of this series next let's talk about the knowledge base say you have a stack of documents that you want to provide to the large language model so you can do some kind of question answering or search over those documents the process of taking those raw files and turning them into a knowledge base can be broken down into four steps the first step is we'll load the documents what this consists of is getting together the collection of documents you want to include in the knowledge base and getting them into a ready to parse format the key thing here is that you want to ensure the critical information in your documents is in a text format because at the end of the day large language models only understand text so any information you want to pass to it needs to be in that format the next thing you want to do is chunk the documents the reason that this this is an important step is that large language models have a fixed context window which means you can't just dump all your documents into the prompt and pass it to the large language model it needs to be split into smaller pieces and even if you have a model with a gigantic context window chunking the documents also leads to better system performance because it often doesn't need the whole document it might just need one or two sentences out of that document so by chunking it you can and ensure that only relevant information is getting passed to The Prompt the third step is to take each of these chunks and translate them into the text embeddings we saw on the previous slide so what this does is it'll take a chunk of text and translate it into a vector or a set of numbers that represents the meaning of that text finally we'll take all of these numbers all these vectors and load them into a vector database over which we can do the text embedding based search we saw on the previous slide so now that we have a basic understanding of rag and some key Concepts surrounding it let's see what this looks like in code here we're going to improve the YouTube comment responder from the previous video with rag we're going to provide the fine-tuned model from the previous video articles from my medium blog so that it can better respond to technical data science questions and so this example is available on on Google colab as well as in the GitHub repository the articles that we use for the rag system are also available on the GitHub and the fine-tuned model is available on the hugging face Hub so we start by importing all the proper libraries this is code imported from the Google collab so there are a few libraries that are not standard including llama index the PFT Library which is the parameter efficient fine-tuning library from hugging face there's Auto gptq Q which we need to import the fine tune model as well as Optimum and bits and bytes and if you're not running on collab also make sure that you install the Transformers library from hugging face with all the libraries installed we can just import a bunch of things from llama index next we're going to set up the knowledge base there are a few settings we need to configure in order to do this first of which is the embedding model the default embedding model on llama index is actually open AI but for this example I wanted to keep everything within the hugging face ecosystem so I used this hugging face embedding object which allows us to use any embedding model available on the hugging face Hub so I went with this one from ba AI it's called BGE small version 1.5 but there are hundreds if not thousands of embedding models available on the hugging face Hub the next thing I do is set this llm setting to none and the reason I do this is that it gives me a bit more flexibility in configuring The Prompt that I pass into the fine-tuned model and then two things I set here are the chunk size which I go with 256 characters and the chunk overlap this wasn't something I talked about but we can also have some overlap in between the chunks and this just helps avoid abruptly chopping a chunk in the middle of a key idea or piece of information that you want to pass into the model with all these settings configured we can can create this list of documents using this simple directory reader object and the load data set method here I have a folder called articles which contains three articles in a PDF format from my medium blog and what happens is this line of code will just automatically go through read the PDFs chunk it and store them in this list called documents so there's actually a lot of magic happening under the hood here the next thing I do is just a little bit of ad hoc pre-processing of the text there are chunks that don't include any relevant information to the meat of the article itself and the reason is these PDFs were printed directly from the medium website so there's a lot of text that is before and after the article itself that's not really relevant to the use case here so here are just three ad hoc rules I created for filtering chunks the first thing I remove is any chunk that includes the text member only Story the reason is this will typically be the text before the article and it'll look something like this it'll say member only story then it'll have the title of the article and then it'll have my name the author's name and then it'll say like where it was published it was 11 minute read when it was published and it'll have the image caption and some just irrelevant text to the article itself another rule I use here is that I remove any chunk that includes the data entrepreneurs this is text I include in the footer of each of my articles which links the reader to the data entrepreneurs community so you can see what that might look like is this is the last sentence of the article although each approach has its limitations they provide practitioners with quantitative ways of comparing the fat tailed inness empirical data probably not helpful to any questions that you're going to ask about this article and most of it is just text from the footer of the article and then finally I remove any chunk that has Min read which typically comes up in the recommendations after the article so we can kind of see that in this chunk of text here and of course this isn't a super robust way of filtering the chunks but a lot of times your pre-processing doesn't have to be perfect it just really has to be good enough for the particular use case and then finally we can store the remaining chunks these remaining 61 chunks into a vector data store using this line of code so now we have our knowledge base set up so index is our Vector database that we're going to be using for retrieval with a knowledge base set up the next thing we're going to set up is the retriever first we're going to define the number of docs to retrieve from the knowledge base and then we're going to pass that into this Vector index retriever object the two things we need to pass here are the index or the vector database and the number of chunks to return from the search next we assemble the query engine so the query engine brings everything together it takes in the user query and spits out the relevant context and so we can use the query engine in the following way so let's say the query is what is fat tailedness which is the same technical question we passed to the fine-tuned model in the previous video of the series and the query engine spits out this response object which includes the top three most relevant chunks but it also includes a lot of other information such as the file name that the chunk was retrieved from the page number the date accessed and some other metadata so in order to take this response and turn it into something we can actually pass to a large language model we'll need to do a little reformatting which I do in this chunk of code here and then we can print what that looks like and so this text is probably small on your screen but you can see that there are three chunks of text and this is ready to go and be passed into a promp pred and subsequently fed into a large language model so at this point we now have all the ingredients of our rag module we have our knowledge base which was created using three PDFs and then we set up the retrieval step which takes in a user query and Returns the relevant context from the knowledge base so the next thing we need to do is to import the fine-tuned model so that we can generate a response to the user query and so here we're importing a few things from the PFT and transform forers Library this is the base model that we fine-tuned in the previous video and then here we transform the base model into the fine-tuned model based on the config file available on the hugging face Hub and then we load the tokenizer this is all stuff I reviewed in the previous video of the series so I would check that out if you're curious to learn more so now that we have the fine-tuned model imported let's use it to respond to a technical question without using the rag system so the way that'll look is we'll create a prompt this is the same prompt from the previous video and I'm creating a prompt template using a Lambda function which will dynamically take the instruction string here and the user comment which I Define here to create the prompt so when we print it it looks something like this where we have the instruction start and end special tokens here we have the instruction string here and this goes on for a long time and then here we have the comment so with that prompt defined we can pass the prompt into our tokenizer to translate it from words into tokens we can then pass those tokens into our model to generate response and then we can print the output this is what the model says without any context from the rag system great question fat tailedness is a statistical property of a distribution in simple terms it refers to the presence of extreme outliers or heavy tails in the distribution for instance consider the distribution of heights in a population a normal distribution would have most people cluster around an average height with a few people deviating slightly from the mean however in a fat tailed distribution you would observe a larger number of people being significantly taller or shorter than the average in finance fat tailedness is often used to describe the presence of extreme Market moves or outliers in financial returns it's an important Concept in risk management and financial modeling I hope this helps let me know if you have any questions sha GPT overall the tone in the format is really good here it explains things in simple terms however the description here doesn't really match the description in my video on fat tailedness or the article on fat tailedness so let's see what happens when we inject the proper context into the prompt using the rag System created earlier we'll again create a Lambda function so here we're not just passing in the comments but we're also going to pass in the context of the comment and so this is something we'll get from our rag system but given that we have both we'll have the same instruction string as before but now we're going to inject the context into the prompt and then we'll put the comment in the same place as it was before so with this new prompt template we can pass in the context and comment to get a new prompt we'll pass in the prompt to the token tokenizer in exactly the same way as before to generate tokens and we'll pass those tokens into the model to generate an output and then we'll print the results here's the model's response when we include context using rag great question fat tailedness is a property of a distribution that describes how much the presence of extreme values rare events affects the overall statistics of the distribution in the context of the video the author discusses how mediocris and extremist distributions differ in terms of fat tailedness mediocris distributions have a relatively small impact from extreme values while extremist distributions are heavily influenced by extreme values the author also mentions that fat tailedness is a more General concept than specific distributions like Paro or power laws instead it lives on a spectrum ranging from thin-tailed gausian to very fat tailed Paro 8020 I hope that clarifies things a bit let me know if you have any questions shot GPT so this does a much better job at capturing the way I describe fat tailedness in my video it defines fat tailedness as how much rare events Drive the overall statistics of a distribution and it even talks about mediocris and extremist which is something I talked about in the video to frame the difference between thin-tailed and fat tailed distributions I also like that it mentioned that fat tailedness is not like a binary thing which is something I talked about in the video but rather it lives on a spectrum from not fat tail to very fat tail looking ahead to the next video of the series I'm going to dive more deeply into text embeddings which was an essential part of the rag system so I'll talk in Greater detail about text embeddings and discuss two major use cases namely semantic search and text classification if you enjoyed this video and you want to learn more check out the blog in towards data science and even though this is a member only story you can access it completely for free using the friend Link in the description below and as always thank you so much for your time and thanks for watching
XpoKB3usmKc,2024-02-27T22:52:38.000000,QLoRA—How to Fine-tune an LLM on a Single GPU (w/ Python Code),"fine-tuning is when we take an existing model and tweak it for a particular use case although this is a simple idea applying it to large language models isn't always straightforward the key challenge is that large language models are very computationally expensive which means fine-tuning them in a standard way is not something you can do on a typical computer or laptop in this video I'm going to talk about Cur which is a technique that makes fine-tuning large L language models much more accessible and if you're new here welcome I'm Shaw I make content about data science and Entrepreneurship and if you enjoy this video please consider subscribing that's a great no cost way you can support me in all the content that I make since I talk in depth about fine-tuning in a previous video of this series here I'll just give a highlevel recap of the basic idea so like I said before fine tuning is tweaking an existing model for a particular use case so an analogy for this fine tuning is is like taking a raw diamond and refining it and Distilling it into something more practical and usable like a diamond you might put on a diamond ring in this analogy the raw diamond is your base model so this would be something like gpt3 while the final Diamond You Come Away with is your fine-tuned model which is something like chat GPT and so again the core problem with fine-tuning large language models is that they are computationally expensive to get a sense of this let's say you have a pretty powerful laptop and it comes with a CPU and a GPU where the CPU has 16 GB of RAM and your GPU has 16 GB of RAM let's say we want to finetune a 10 billion parameter model each of these parameters corresponds to a number which we need to represent on our machine standard way of doing this is using the fp16 number format which requires about two bytes of memory per parameter so just doing some simple math here 10 billion parameters time 2 bytes per parameter comes to 20 GB of memory just to store the model parameters so one problem here is that this 20 GB model won't fit on the CPU or GPU but maybe we can get clever in how we distribute the memory so the load of the model is split between the CPU and GPU and that allows us to do things like inference and make predictions with the model however when we talk about fine-tuning we're talking about retraining the model par parameters which is going to require more than just storing the parameters of the model another thing we need are the gradients these are numbers that we use to update the model parameters in the training process we'll have a gradient which is just going to be a number for every parameter in the model so this adds another 20 GB of memory so we've went from 20 to 40 and now even if we get super clever with how we distribute it across our CPU and GPU GPU it's still not going to fit so we'd actually need to add another GPU to even make that work but of course this isn't the whole story you also need room for the optimizer States so if you're using an Optimizer like atom which is very widely used this is going to take the bulk of the memory footprint for model training where this is coming from is an Optimizer like atom is going to store a momentum value and variance value for each parameter in your model so we'll have two numbers per parameter additional these values need to be encoded with higher Precision so instead of the fp16 format these are going to be encoded in the fp32 format and so when it's all said and done there's about a 12x multiplier for the memory footprint of these Optimizer states which means we're going to need a lot more gpus to actually fine-tune this model these calculations are based on reference number two which is a paper about zero which is a method for efficiently fine-tuning these deep neural networks works so we come to a grand total of 160 GB of memory required to train a 10 billion parameter model of course these enormous memory requirements aren't going to fit on your laptop and it's going to require some heavyduty Hardware to run so 160 GB if you get like a 80 gb GPU like the a100 you'll need two of those at least and those are about $20,000 a pop so you're probably talking about like $50,000 just for the hardware to fine-tune a 10 billion parameter model in the standard way this is where Cur comes in so Cura is a technique that makes this whole fine-tuning process much more efficient so much so that you can just run it on your laptop here without the need for all these extra gpus before diving into Cur a key concept that we need to understand is quantization and even though quantization might sound like this scary and sophisticated word it's actually a very simple idea whenever you hear quantization just think splitting a range of numbers into buckets so as an example let's consider any number between 0 and 100 obviously there are infinite numbers that can fit in this range you know there's like 27 55.3 83.7 823 and so on and so forth what quantization consists of is taking this infinite range of numbers and splitting it into discrete bins one way of doing this is quantizing this infinite range using whole numbers so what that would look like for our three numbers here is that 27 would go into this 27th bucket 55.3 would go into this 55 bucket and then 83.78% would go to 20 55 would go to 50 and 83 would go to 80 so that's the basic idea and the reason this is important is that quantization is required whenever you want to represent numbers in a computer and the reason is that if you wanted to encode a single number that lives in an infinite range of possibilities this will require infinite btes of memory it just can't be done at some point when you're talking about a physically constrained system like a computer you have to make some approximations and so if we go from this infinite range to this range quantized by whole numbers this would require about 0.875 bytes per number and then if we go one step further and just split it into these 10 different buckets it would require about half a bite per number one thing to point out here is that there's a natural tradeoff you know we could have a lot of buckets which would give us a lot of precision but it's going to increase the memory footprint of our model however you could have very few buckets for quantization which would minimize the memory footprint but this would be a pretty crude approximation of the model you're working with so balancing this tradeoff is a key contribution of Q Laura there are actually Four ingredients that come together to make up Q Laura the first is 4bit normal float the second is double quantization the third are paged optimizers and then finally is loraa I'm going to talk through each of these ingredients one by one starting with ingredient one one 4bit normal float all this is is a better way to bucket numbers it's a better way to do quantization so let's break it down when we say something is 4 bit what we mean is we're using four binary digits to represent that piece of information and since each digit can be either zero or one this gives us 16 unique combinations which means with a 4bit representation we have 16 buckets at our disposal for quantization compressing a range of numbers into just 16 buckets is great for memory saving you know we only have four bits which translates to half a bite per parameter so if we have 10 billion parameters that's going to translate to 5 GB of memory but of course this brings up the same problem I mentioned earlier which is we have this tradeoff it's like yeah we get huge memory savings but now we have a very crude approximation of the number we're trying to represent the way ingredient one this 4bit normal float works is it buckets the numbers in a particular and clever way suppose we have all the parameters in our model and we plot their distribution when it comes to these deep neural networks it turns out that most of the parameter values are going to be around zero and very few values are going to be much smaller and much larger than zero what that means is we have something that resembles a normal distribution when it comes to our model parameters so if we follow a quantization strategy that I talked about a couple slides ago where we just split the numbers into these equally spaced buckets we're going to get a pretty crude approximation of these model parameters because most of our numbers are just going to be sitting in these two buckets here with very few numbers sitting in these end buckets here an alternative way we can do quantization is instead of using equally spaced buckets we can consider using equally sized buckets so instead of mapping each parameter into these eight buckets we map these parameter values into these eight buckets so now you can see that we have a much more even distribution of model parameters across these buckets and this is exactly the idea that 4-bit normal float uses to balance that tradeoff between low memory and accurately representing model parameters so the next ingredient is double quantization which consists of quantizing the quantization constants I know the word quantize is appearing way more than anyone would ever like on this slide but let's break it down step by step to see what this all means so consider this simple quantization strategy so let's say we have this array of numbers X that's represented using 32 bits and we want to translate it into an 8bit representation on the left hand side here and then we want this 8bit representation to have values in between minus 127 and 127 essentially what we're doing is we're quantizing by whole numbers forcing it to live in this range ofus 127 to 127 so that's what we're trying to do so a simple way of doing that is we rescale all the values in this array by the absolute maximum value in the array and then we'll multiply it by the new maximum value which is 127 in our quantized range and then we'll round it just so that there are no decimal points so this is a very simple way we can quantize this arbitrary array encoded in 32bit into a 8bit integer representation and just to make this more simple we can translate this prefactor here into a constant encoded in 32bit so while this simple quantization strategy isn't how we do it in practice cuz again if we're doing the equally sized buckets it's not just going to be this linear transformation that we're seeing here but this does illustrate the point that anytime you do Quant ization there's going to be some memory overhead involved in that computation so in other words these constants are going to take up precious memory in your system so as an initial strategy you might think well if we have this input tensor or input array and we rescale all the parameters we're only going to have one new constant a 32bit number for all the parameters in our model what's the big deal about that what's another number compared to 10 billion parameters for example so while this does have trivial memory implications it may not be the best way to quantize our model parameters because this is going to be very sensitive to extreme values in our input tensor and the reason is if we're talking about these model parameters where most of them are close to zero but then you have this one parameter way far off in the tails that is your absolute Max it's going to introduce a lot of bias in your quantization process so this standard quantization approach does minimize memory but it comes with maximum potential for bias an alternative strategy could be as follows where we take the input tensor we reshape it to look like this and then we split this tensor into buckets and then within each bucket we do the rescaling process so this significantly reduces the odds of one extreme value skewing all the model parameters in the quantization process this is called blockwise quantization and although it comes with with a greater memory footprint it has a lot less bias so to mitigate the memory cost of this blockwise quantization approach we can employ double quantization which will do this quantization process here but then we'll do the quantization process once again on all these constants that pop up from this blockwise approach so if we just kind of repeat this very simple strategy here now we have an array of constants we have multiple constants popping out they're encoded in 32bit and then we can quantize them into a lower bit format using this simple approach that's double quantization so we are indeed quantizing the quantization constants while it might be an unfortunate name it is a pretty straightforward process so ingredient three is a paged Optimizer all we're doing here is looping in your CPU into the training process so let's say we have a small model like 51 which has 1.3 billion parameters which which based on those same calculations we saw earlier would require about 21 GB of memory for full fine-tuning the dilemma here is that although we have enough memory across the GPU and CPU for all 21 GB needed to fully fine-tune 51 this isn't something that necessarily just works out of the box these are independent modules on your machine and typically the training process will just be restricted to your GPU and so this paged Optimizer what that means is instead of just restricting training to only fitting on your GPU you can move memory as needed from the GPU to the CPU and then bring it back onto the GPU as needed what that might look like is you'll start model training and you'll have one page of memory and a page of memory is like a fundamental unit or block of memory on the GPU or CPU the pages will start accumulating during the training process until your memory gets full and then at which point if you have this paged Optimizer approach you can start moving pages of memory over to the CPU to make room for new memory for training and then if you need a page of memory that was moved to the CPU back onto the GPU you can make room for it there and then you can just move it back over using this paged Optimizer this is the basic idea honestly I don't know exactly how this all works I'm not like a hardware guy I don't know how computer architecture fully works but this is like my highlevel understanding as a data scientist so if you want to learn more check out the cura paper where they talk a little bit more about it and provide some additional references the final ingredient of cura is loraa which stands for low rank adaptation and so I actually talked about Lura in depth in a previous video on fine-tuning so here I'm just going to give a brief highlevel description of how it works if you want more details you can check out that previous video or check out the low R paper Linked In the description below what Laura does is that it fine-tunes a model by adding a small number of trainable parameters so we can see how this works by contrasting it with the standard full fine-tuning approach so let's say this is our model here this is our neural network and we have this input layer we have some hidden layer and then we have the output layer here full fine tuning consists of retraining every single parameter in this model we're just considering one layer at a time we'll have this weight Matrix corresponding to all these lines in this picture here we'll have this Matrix W KN consisting of all the parameters for that particular layer and all of these are trainable while that's probably not going to be a big deal about these six parameters in this shallow Network here if you have a large language model these matrices will get pretty big and you'll have a lot of them because you'll have a lot of layers Lura on the other hand instead of fine-tuning every every single parameter in your model it'll actually freeze every parameter in the model and it works by adding a small set of trainable parameters which you'll then fine-tune the way this works is you'll have your same hidden layer and then you'll add a small set of trainable parameters through this Delta W Matrix so if you're looking at this you might think well how does this help us because Delta W is going to be the same size as W KN so how is this adding a smaller set of trainable parameters and so the trick with Laura is that this Delta W will actually be the product of two smaller matrices b and a which have the appropriate Dimensions to make all the math work out so visually what that looks like is you have your W KN here but then you have BNA a which have far fewer parameters than W KN but when you multiply it together their product it'll have the proper shape to make all the Matrix operations work here so you'll actually freeze W KN so you won't train these parameters and then these parameters housed in BNA will be the trainable ones the result of training the model this way is that you can get 100 to even 1,000x savings and model parameters so instead of having to train 10 billion parameters you're only having to train like 100 million parameters or 50 million parameters so let's bring these four ingredients together let's first look at the standard fine tuning approach as a baseline so here let's say we have our base model represented in fp16 so we'll have this memory footprint from the base model and then we'll have this larger memory footprint from the optimizer States and then we won't have any adapters because adapters only come in when doing lowra or another parameter efficient fine-tuning method and so we'll do like the forward pass on the model it'll go to the optimizer and then the optimizer will do the backward pass and will update the model parameters this is the same standard fine-tuning approach we talked about earlier so a 10 billion parameter model will require about 160 gbes of memory another thing we could do is use lowra so we can get that 100 to 1,000x Savings in the number of trainable parameters we still have our model represented in 16bit but now instead of fine-tuning every single parameter in the model we only have a small number of trainable parameters and then each of those parameters will have an Associated op Optimizer state which significantly reduces the memory footprint so that a 10 billion parameter model would only require about 40 GB of memory while this is a tremendous savings like a 4X Savings in memory 40 GB is still a lot to ask for from consumer Hardware so let's see how Cur helps the situation even further the key thing here is that instead of using the 16bit representation we can use ingredient one and encode the base model as 4bit normal float and then we'll have the same number of trainable parameters from Lura so that'll be exactly the same and then we can use ingredient 3 with the paged optimizers to avoid any out of memory errors that might come up during the training process with that and including the double quantization here we can use Cur to fine-tune a 10 billion parameter model with just about 12 gigabyt of memory which is something that can easily fit in consumer Hardware and can even run using the free resources available on a Google collab so let's see a concrete example of that here we're going to do fine-tuning using mistol 7B instruct to respond to YouTube comments this example is available on the Google collab associated with this video the model and data set are freely available on hugging face and then additionally there is a GitHub repo that has all the resources put together as well as the code to generate the training data set here first thing we need to do is import some libraries everything here is coming from hugging face their Transformers Library their PFT Library which is parameter efficient fine-tuning this is what's going to allow us to do Q lur and then we're using hugging fa's data sets library because I uploaded the training data set onto hugging faes Hub and then finally we just import the Transformers Library these are kind of like sub dependencies to ensure some of these modules work I think it's mainly this one prepare mod for kbit training you don't need to import these but you need to make sure they're installed in your environment and this was actually a paino because bits and bytes only works on Linux and windows and on Nvidia hardware and then gptq this format for encoding models it doesn't run on Mac so as a Mac User this was kind of frustrating lots of trial and error to try to get it to work on my machine locally but I wasn't able to get it to work so if anyone was able to get it to run on a M1 or a M2 or or even M3 send me your example code or send me any resources you found helpful I would love to get a version working on my personal machine but since collab they have a Linux environment using Nvidia Hardware the code here works fine next we can load the quantized model and so here we're going to grab a version of mistol 7B instruct from the bloke and so if you're not familiar with the bloke he's actually quantized and shared thousands of these large language models completely for free on the hugging face Hub and then we can just import this model using this from pre-trained method so we just need to specify the model name on The Hub device map set to Auto just has the Transformers Library kind of figure out the optimal way to spread the load between the GPU and CPU to load in the model trust remote code basically it's not going to allow like a custom model file to run on your local machine so this is just a way to protect your machine when downloading code from The Hub and then revision main is just saying we want the main version of the model available at this repo here then again gptq which is the format used here does not run on Mac there are some other options with Mac but I wasn't able to get it working on my machine once we have the quantized model loaded we can load the tokenizer so we can do this actually pretty easily using this from pre-train method so we just specify the model name and then specify this use fast argument as true with just those two simple blocks of code we can use the base model one thing we do here is we put the model into evaluation mode which apparently deactivates the Dropout modules next we can craft our prompt so let's say we have a command from YouTube that says great content thank you and then we put it into the proper prompt format so mistal 7B instruct is an instruction tuned model so it's actually Expecting The Prompt in a very particular format and namely it's just expecting this instruction start and instruction end special tokens in the prompt so we set that up very easily what this is doing is it's just going to dynamically take this comment variable and stick it into this prompt here and then once we have that we can pass the prompt to the tokenizer so basically we're taking this prompt and We're translating it from a string into an array of numbers and then we can take that array of numbers and we can pass it into our model to generate more text once we do that we can get the outputs and then pass them back into our tokenizer and have the tokenizer decode the vector back into English the output of this great content thank you comment is I'm glad you found the content helpful if you have any specific questions or topics you'd like me to cover in the future feel free to ask I'm here to help in the meantime I'd be happy to answer any questions you might have about the content I've already provided just just let me know which article or blog post you're referring to and I'll do my best to provide you with accurate and up-to-date information thanks for reading and I look forward to helping you with any questions you may have so while this is a fine response there are a few issues with it one it's very long I would never respond to a YouTube comment like this second is it kind of just like repeats itself it's like glad you found it helpful feel free to ask and then it says happy to answer questions that you have happy to provide you with acurate update information and like look forward to helping you with questions so saying the same thing in different words like a few different times and then finally it says thanks for reading and if this is for YouTube comments people aren't reading this stuff they're watching videos so one thing we can do to improve model performance is by doing so-called prompt engineering I actually have a in-depth guide on prompt engineering where I talk about seven tricks to kind of improve your prompts in a previous video of the series so feel free to check that out if you're interested The Prompt that I ended up using here is something that I generated through trial and error and the way I did that is using a website called together. which I can link in the description below essentially together. a they have a chat interface kind of like chat GPT but for a lot of open- source models including mistl 7B instruct version 0.2 so I was able to test a lot of prompt ideas and get feedback and just kind of eyeball which gave the best performance and I ended up using that one so I have this set of instructions here sha gbt functioning as a virtual data science consultant on YouTube communicates in clear accessible language escalating to technical depth upon request it reacts to feedback aply and ends responses with its signatur sha GPT sha gbt will tailor the length of its responses to match the viewers comment providing concise acknowledgements to brief expressions of gratitude or feedback thus keeping the interaction natural and engaging then I have this instruction please respond to the following comment and then I have this Lambda function where given a comment I'll piece together this instruction string and comment together within the instruction special tokens that the model is expecting with that I can just pass the comment into the prompt template and generate a new prompt what that looks like is this so you see we have the instruction special tokens you see it's well formatted this is the instructions please respond to the following comment and says great comment thank you Now using this new prompt instead of just passing the comment directly to the model we have this set of instructions with the comment this is the response thank you for your kind words I'm glad you found the content helpful sha GPT so this is really good this is actually already pretty close to how I typically respond to YouTube comments and a lot of them tend to be something like this and it appropriately signed off as Sha GPT so people know that it came from an AI and not from me personally well maybe we could just call it here it's like okay this is good enough let's just start using this as the comment responder let's see how we can use Q Laura to improve this model even further using fine tuning so the way to do that is we need to prepare the model for training so we'll put it from eval mode into training mode we're going to enable gradient checkpointing which isn't something I talked about and it's not necessarily part of the qora technique because it's actually pretty standard it's just a memory saving technique that clears specific activations and then recomputes them during the backward path of the model and then we need to enable quantized training the base model is going to be in 4 bit and we're going to freeze them but we still want to do training in higher Precision with Lowa we need to make sure we enable this quantize training option next we want to set up lowra so we can use that using this low ra config file I talk more about low RA in the fine-tuning video so just briefly going through this we're going to set the rank as 8 set the alpha s32 we're going to Target the query modules in the model we're going to set drop out to 0.5 we're not going to have any bias values and then we're going to set the task as causal language modeling with the config file we can pass the model and the config into this method get PFT model so this will just create a lowr trainable version of the model and then we can print the number of trainable parameters so doing that we see that we actually have a significant saving so less than 1% of the original number of trainable parameters just one point of confusion for me personally is it's showing that mistol 7B instruct has 264 million parameters here based on the quick research I did seemed like when you do quantization there could be some terms that you can drop but honestly I don't fully understand why we went from 7 billion parameters to just 264 million parameters so if anyone knows that please drop it in the comments I'm very curious but the main point here is that we're only using 0.8% of the original number of train parameters so huge memory savings using lowon next we're going to load the data set which is freely available on the hugging face Hub it's called shot GPT YouTube comments also the code to generate this data set is available at the GitHub repo if you're curious on how to do the formatting and stuff and then here's an example from this data set you'll see we have the special token the start string and the end string we have the start instruction and end instruction and then we have the same set of instructions as before and then we have the comment here which is a real comment from the YouTube channel then after the instruction string we have the actual response I left to this comment and then I just appended this Shaw GPT sign off so the model learns the appropriate format in style that it should respond to we've got a data set of 59 of these examples so not a huge data set at all and then next we need to pre-process the text so this is very similar to how I did it in the previous find tuning video basically we Define this tokenized function which if the example is too long so if it's longer than 52 tokens it's going to truncate it so it's not more than this max length and then we'll return it as numpy values and then we can apply this tokenized function to every single example in the data set using this method here the map method where we have our data set and then we just pass in the tokenized function and set batched equal to true so it doesn't batches I guess instead of doing it one by one the other thing we need to do is this handles if the examples are too long but when you're training the model each example in a batch they actually need to be the same size so you can actually do matrix multiplication so for that we can create a data cator what that does is if you have multiple examples of different lengths so let's say you have like four examples in a batch and they're all of different lengths the data cator will dynamically pad each example example so they have the same length for that we need to define a pad token which I set as the end of string token and then I create the data collator using this method here and then I think this is masked language modeling set equal to false and that's because we're doing so-called causal language modeling not masked language modeling now we're ready to start setting up the training process so here we're setting hyperparameters we have the learning rate batch size number of epoch we're setting the output directory of the model the learning rate the batch size goes here number epochs goes here weight Decay we set it as 0.01 for logging evaluation and save strategy we set it to every Epoch that means every Epoch will print the training loss we'll evaluate at every Epoch we'll also print the validation data set loss and then save strategy so we'll save the model every Epoch in case something goes wrong we're going to load the best model at the end because maybe the best model was actually at the eighth Epoch and it got worse on the ninth Epoch or something like that gradient accumulation is equal to four warm-up steps equal to two so I actually talk a lot about gradient accumulation and weight decay in the previous video on training a large language model from scratch so if you're curious about what's going on there you can check out those videos next we'll set fp16 equal to true so here we're going to use 16bit values for training and then we'll enable the paged Optimizer by setting this optim equal to paged atom W 8bit so this is ingredient three from before lots of hyper parameters and of course you can spend your whole life tuning and tweaking this but once we have that we can run the training job so we initialize our trainer we give it the model give it our training data set our validation data set training arguments we defined on the previous slide and then the data collator we're going to silence warnings this is what I saw on an example from hugging face when they were introducing bits and bites so I just did it again here and then we can run the training process this took about 10 minutes to run on Google collab so it's actually pretty quick and this is this is what will get printed the training loss and validation loss so we can see a smooth monotonic decrease of both implying stable training which is good and then once it's all said and done we have our model and we can use it so if we pass in that same test comment great content thank you we get the response glad you enjoyed it shot GPT and then it even adds this disclaimer that note I am an AI language model I don't have the ability to feel emotions or watch videos I'm here to answer questions and provide explanations so this is good I feel like this is exactly how I would respond to this comment if I wanted to remove the disclaimer I could easily do that with some like string manipulation just keeping all the text before the sign off or something like that but the point is that the fine-tuning process at least from this one example seemed to work pretty nicely let's try a different comment something more technical like what is fat tailedness the response of the model is actually similar to what we saw in the previous video when we fine-tuned open AI model and then we asked it the same question where where it gives a good concise explanation of fat tailedness the only issue is it doesn't explain fat tailedness the same way that I explained it in my video series on the topic so this brings up one of the limitations of fine-tuning which is that it's great for capturing style but it's not always an optimal way to incorporate specialized knowledge into model responses which brings us to what's next instead of trying to give the model even more examples trying to include this specialized knowledge a simpler approach is that we can improve the model's responses to these types of technical questions by providing it specialized domain knowledge the way we can do that is using a so-called rag system which stands for retrieval augmented generation right now we just get the comment and we pass it into the model with the appropriate prompt and it spits out a response the difference with a rag system is that we take the comment we use the comment to extract ra relevant information from a knowledge base and then we incorporate that into the prompt that we pass into the model so that it can generate a response so that's going to be the focus of the next video in this series we're going to see how we can improve shot GPT using specialized knowledge coming from my medium blog articles and speaking of medium blog articles if you enjoy this video but you want to learn more check out the article published in towards data science on Cur there I cover details that I might have missed in this video here and even though this is a member only story you can access it completely for free using the friend Link in the description below other things I'll point out is that the code example is available for free on collab there's more code available on the GitHub and then again the model and the data set are available on hugging face and as always thank you so much for your time and thanks for watching"
qPrVqTIkobg,2024-02-27T00:59:53.000000,Difference Between #AI Chatbots and Assistants,here's the difference between an AI chatbot and an AI assistant although these terms are often used interchangeably they can mean slightly different things put simply a chatbot is an AI you can have a conversation with while an AI assistant is a chatbot that can use tools a tool can be things like web browsing a calculator or anything else that expands the capabilities of a chatbot for example if you use the free version of chat GPT that's a chatbot because it comes with a basic chat functionality however if you have the premium version of chat GPT that's an assistant because it comes built in with functionalities like web browsing knowledge retrieval and image generation assistants transform AIS from chat buddies to agents that can solve and automate complex problems
LqOJCPonUQU,2024-02-19T14:51:09.000000,The Best Way to Think About Goals #goalsetting,goals are essentially an excuse for us to grow this is actually something that you know I first heard from Ray Doo who has a book called like principles I think that's what it's called oh he said that I don't think it's a new idea I think people have known for a while that the point of a goal isn't necessarily the goal if you've watched the David Beckham documentary and there was something that he said in there I don't know what episode but he was like the game of football is just an excuse for them to keep playing yeah and I was like whoa okay something clicked it has nothing to do with the outcome it has nothing to do with the output it's about the process right levels I think should be more as like a bonus than the the focus and that's so different than how I feel like how we learned about goals yeah you need to get to the goal and that's the point and the person that you have to become yeah in the process that's the real reward take the focus off the goal put it on the person that you want to be the dream life that you want to have
r5qk3uIdkks,2024-02-05T14:59:54.000000,What is #ai? — Simply Explained,what is AI exactly I work in the industry and I can still find myself scratching my head about it but a simple way to think about it is as follows AI stands for artificial intelligence the word that's a little tricky here is intelligence what does that mean exactly the way I like to think about it is that intelligence is the ability to solve problems and make decisions so if we take that definition for intelligence it then follows that artificial intelligence is simply a computer's ability to solve problems and make decisions
4RAvJt3fWoI,2024-02-05T00:44:32.000000,"3 Ways to Make a Custom AI Assistant | RAG, Tools, & Fine-tuning","one of the most common asks I get from clients is how do I build a custom AI chatbot well a few months ago this was something you needed to hire a consultant for today that's not necessarily the case in this video I'll walk through three ways to make a custom AI assistant using a few new releases from open AI I'll review a no code solution via their new gpts feature a python solution via the new assistance API and then finally talk about how to fine-tune an assistant with your data and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoy this content please consider subscribing that's a great no cost way you can support me in all the videos that I make before jumping into the solutions I wanted to quickly differentiate a AI chatbot from a AI assistant while these terms might be used interchangeably the way I like to think about it is that a chatbot is an AI you can have a conversation with while an AI assistant is essentially a chatbot that can use tools a tool could be web browsing it could be a calculator it could be a python interpreter or anything else that helps augment the abilities of a chatbot for example if you have the free version of chat GPT that's a chatbot because it comes with a basic chat functionality without any additional tools however if you have the premium version of chat GPT that's an assistant because it has built-in tools like web browsing and document retrieval turning chatbots into assistance via tools is a powerful way to to expand its capabilities and Effectiveness each of the approaches I talk about in this video can be used to develop an AI assistant and I'll use each of these approaches to build an AI that can respond to my YouTube comments in my likeness which I'll call Shaw GPT the first way I'm going to talk about is the easiest and fastest of the three and it consists of using open ai's new gpts feature so the way to get to that is we go to chat GPT and if you have the premium version version you can go to this Explorer GPT thing here and then you can go look at my gpts and so you can make these custom gpts and they'll appear here so I've already made a version of this Shaw GPT but we'll go ahead and create a new GPT so we can see what this looks like from scratch the thing about this gpts feature is it actually has a chat interface to help you build the AI assistance so not only is this a no code solution it has a step bystep chatbot that will help you build this Ai and so here the GPT Builder says hi I'll help you build a new GPT you can say something like make a creative who helps generate visuals for my new product or make software engineer who helps format my code what would you like to make make a YouTube comment responder let's see what that spits out so you can see that GPT Builder is thinking what's happening is you say this to GPT Builder and then behind the scenes it's going to go ahead and update the instructions for the AI assistant so if you go to the configure tab it'll start filling in these things it'll film the description for the assistant and it'll give it its instructions but let's go back how about the name shot GPT for this GPT does that work that's perfect okay so now it's going to generate a profile picture so it'll use Dolly 2 which is if you're not familiar that's a different AI that instead of generating text like chat GPT it generates images like this one here so that's a cool thing about this gbt Builder it'll generate a profile picture for you you can also just upload your own so I've got a folder here called sha GPT and I've got some different images that we can use so I'll just upload this one and then I'll tell GPT Builder to use this image instead so it's going to use the picture that I uploaded okay so now it's asking for a bit more guidance on how to answer questions outside the expertise in data science and Entrepreneurship so a lot of questions that I get on YouTube aren't actually about data science or entrepreneur preneurship it's usually people just saying great content or something like that just sharing feedback about the video so I'll provide those instructions here okay so here I wrote out instructions for both positive and negative feedback so for positive feedback unrelated to data science Express gratitude for their viewership suggestions or positive feedback given negative feedback thank the viewer rephrase their criticism to convey not convert convey understanding and offer an idea for improvement so these are just some general rules of thumb of how to respond to comments how do you envision sha GPT addressing comments that ask for personal advice on Career changes to Day signs or starting a new business okay so that's a good question I didn't see this last time I made sha GPT through this interface basically when people ask me these questions I tell them that there's no way to just give general advice to anyone without knowing more details and ultimately I only know what's worked for me and I don't necessarily know what's the best choice for other people and that's ultimately up to them but I'm always happy to kind of share my personal experience and what has worked for me so basically going to try to put that into words so that GPT Builder can instruct the model accordingly so the way I tried to convey that is acknowledge that advice for their unique situation is hard to give without a full understanding of their situation however sha GPT can share Shaw's personal experience in case it is helpful for the viewer okay here we go sha GPT will now take the approach that when addressing personal advice inquiries okay so for the Final Touch what tone of L language do you prefer shbg use more formal and professional or casual and friendly casual and friendly and concise I don't like reading a lot of text and so I try not to put that burden on other people so I like to keep text very concise okay sweet feel free to try out shot yout in the preview and see how it responds if there's anything else you'd like to address what would it be okay so we can now give it a whirl how do I start in data science so it gives a very long response doesn't feel very concise starting data science is exciting here's a quick guide to getting you going learn the basics explore online courses Hands-On practice understanding data manipulation diving into machine learning build a portfolio Network and learn continuously so it's casual it's friendly but it's not very concise and doesn't really sound like me the response sounds a lot like what chat GPT would say so if someone reads this comment or response from me they'll be like did sha just copy paste my comment into chat GPT and paste the response so one thing we can do to remove that ambiguity is we can have sh GPT sign off with its name just so people know that it was generated by an AI assistant and not by me personally we can add this to the GPT Builder Okay so it updated the instructions so now it should sign up as Shaw GPT so here's another one can you explain machine learning in simple terms so it gave a pretty good response I like this example just learning by example and experiences it signed off with sha GPT which is good again however the response is way too long I would never respond like this but it's good that it identifies itself I want to move over to this configure tab so I guess some weird things even though it said it was going to use the name sha GPT that never made it over it's still I guess in beta they're still working out some Kinks if anything isn't what you want from this chat interface you can always move over to the configure Tab and just set these things manually so you can set the name of the assistant you can set the description this was autogenerated friendly and informative responder for a data science and Entrepreneurship YouTube channel and then these are the instructions that it generated through the conversation we were having sha GPT is the Casual friendly and concise voice behind a YouTube channel on data science and Entrepreneurship it acknowledges the complexity of giving personalized advice offering insights from Shaw's experiences instead positive feedback is met with gratitude and criticism is handled constructively sha gbt always signs off its messages with its name sha gbt so we can see like a lot of the aspects of the conversation were incorporated into to the instructions another cool thing here is you can set these conversation starters which is nice for the UI you can just put like frequently asked questions for your AI assistant here another really cool thing about gpts is that it has builtin retrieval also called retrieval augmented generation so basically what this means is you can equip your assistant with specialized knowledge that lives in like a PDF or like a Word document or whatever so you can upload those files and the assistant will be able to read those files and incorporate the appropriate context as needed so for example I have a bunch of my articles here so I could upload four ways to quantify fat taals with python I can upload that PDF and then I can ask it a question what is a fat tail it didn't use the PDF in this case it just gave a standard definition of fat Tails as something more extreme than a normal distribution which is a common way of defining bat Tales however it's not the definition of fat tails that I used in that article so another thing we could try is like how can we quantify fat tails so even though it didn't use the definition of fat tailedness from that article it did seamlessly grab the four ways to quantify fat taals from the article the way this is working is that GPT 4 or whatever GPT is underlying these custom gpts here it has been trained to know when to seek outside knowledge if it doesn't know something and so so here it does it pretty seamlessly it feels like it read the article and is like rephrasing it in its own words which is pretty interesting okay so that's knowledge you can upload I think up to like 10 files or something actually probably more the next thing I want to point out are these capabilities so this is what makes these gpts assistant and not just chat Bots it's because it has these extended capabilities such as web browsing image generation and code interpreter so web browsing is exactly what you expect if the user says hey look up something or search the web for some resources or whatever the assistant knows when to call upon this web browsing capability to do that if the user asks the assistant to generate an image it knows to use Dolly and the code interpreter allows the assistant to not only write python code because that's what a chatbot can do but actually run that python code and then return the values to the user or use the values from that computation to inform the response to the user so since for this YouTube responder use case we don't need image generation because you can't have images on YouTube probably don't need a code interpreter either web browsing could be cool to provide additional resources and of course I could upload all these other articles as well so we can just do that to give it more knowledge so we can't bulk upload PDFs at this point but I can go through one by one and upload all of these articles to have it have a bit more specialized knowledge about the actual content that is on the channel the last thing are these actions which are pretty interesting so essentially this allows gpts to make API calls so we can learn more here how to create a GPT okay so this answers one question you can have 20 files uploaded that answers that okay here custom actions you can make third party API calls available to your GPT so that's pretty cool like the canonical case here is like if you want to grab weather data so if you want to get weather from a particular City you can endow the assistant with that capability to look up the weather in a particular City not necessarily for this YouTube comment responder I think here just knowledge and the web browsing is sufficient and so what's cool about these is you can save these gpts and make it available to everyone so you don't have to worry about like deploying your assistant into production because they have this built-in way to do it so you can make it published only to yourself anyone with the link or everyone in the world and then you can select a category so that's pretty cool it got the category right we can uh confirm but I won't because I've already published a version of sha GPT and I spent a bit more time making that one so I don't want to put this example version out there but if you want to interact with sha GPT I'll put a link in the description and you can kind of see for yourself the performance I guess another thing is people say like you can become a millionaire making a GPT or something and I don't know maybe based on my interaction with this interface I don't know how that would work however looking at the other gpts this doesn't feel so much as like a direct way to generate Revenue however it feels like a very big opportunity for these companies or really any company to do promotion and lead generation for their business so like canva being number one on here it's like that's amazing advertising cuz whatever it is they get 100 million daily users of chat GPT that's a lot of eyeballs seeing canva basically for free feels more like an opportunity for these companies to promote their business so like Ali Academy I don't know who that is but I can just like go to her website look at that geni Innovation for Creative so look at that of course this might be a way to generate revenue or whatever but to me it feels more an opportunity for advertising a business or something like that as opposed to direct monetization so just my thoughts on becoming a millionaire with custom gpts okay so when it's all done and published so this is publicly available data scientist GPT for YouTube comments and so we can have these things ready to go so people can ask their data science questions even though it has the Articles available to it that I wrote it doesn't explain it in the way that I explained it in the Articles it kind of gives a more traditional explanation of these things so that's kind of one downside for these custom gpts it still feels a lot like chat GPT but just with slightly different wrapping paper and of course if I spent a lot more time with it I could build something that doesn't sound so much like Chad GPT but the second obvious downside of this is it requires a pre premium version of chat GPT and some people just like a hobbyist or something or you're like a student it may not make sense to pay $2 a month for this and of course this isn't something you can easily integrate into like an app that you make or a website for people to use this they have to come to the chat GPT website so if you don't have premium or you want to incorporate this assistant into some application or some website we can turn to the assistance API which is way number two I'm going to talk about so if you don't have a premium account there's still a no code solution for building a AI assistant and that's through the assistance playground this is a relatively new feature so the playground's been around for a while this is the original version the complete playgrounds you can pick whatever model you like here and it'll just do the auto regressive text generation it'll just keep predicting the next word recursively chat is similar where you can set the model and whatever parameters you like and set the system message and just do like a chat however assistance takes things to the next level where it's a lot like the gpts configure tab that we saw earlier where you can have a name instructions models and then you can add tools to the chat bot so we can really just copy paste all this over so we have sha GPT here are your instructions we can select the model so we can have GPT for Turbo we can add functions so we can write custom functions here it has to be in a particular format shown here so you need to give it a name for the function you need to describe it in words and then you need to provide the inputs for the function so what information does the assistant need to pass to the function in order to generate an output also has the code interpreter so it can not only write python code it can run the python code receive a response and then it also has retrieval like we saw with the custom GPT the one thing missing here is web browsing natively I think that's something that they're going to add later but as of right now it's January 30th 2024 web browsing is not integrated into the assistance playground or assistance API but we do have functions code interpreter retrieval and we can also add files like we did before let's see if we can do bulk file upload here aha we can there we go bulk file upload available in the playground okay so we can just see how this works we can do like a side by-side comparison so we'll ask sha GPT what is fat tailedness and we'll ask sha GPT playground what is fat tailedness okay so again it's kind of giving the more traditional definition of something fat tailed has heavier Tails compared to a normal distribution so the response is very similar to what we see over here in the gpt's case so while using the playground to make an assistant in this way seems like a free hack a free version of the custom gpts it has one serious limitation which is that if you only use the playground your assistant is trapped in the playground it's not like the GPT T's interface we saw earlier where you can with one click deploy the assistant onto the chat GPT website so if you want to release the assistant from this playground you want to put it into a website an app or whatever you're going to need to do some coding so let's do that okay so here we got a jupit notebook this is available in the GitHub repository Linked In the description below so if you want this code feel free to grab it there kind of walking through this just importing some modules of course we're going to use the open AI assistance API which is still in b so that's why I put that there you'll need to import your secret key and if you don't have a secret key or don't know how to get your secret key for the open AI API I have a video all about that I'll link it in the description below maybe pop it up on the screen here so you can check that out but you can get a key in a very straightforward way you don't need to be premium user if you just have an open AI account you can get an API key and if you're new you get some free credits to start what I have is a text file called sk. and I'm in importing the secret key from that text file that's the way I like to do it and also importing time for a helper function we'll see in a second here the first thing we need to do is to set up the client so set up communication with the opening eye API so you just do that oneline in code I make a quick helper function because when we're using these assistants they have to think so it takes time for us to get a response from the API so what this function does is weights and periodically grabs the status of the API call and then when it's done it'll move on to the next step of the code and it'll tell us how long it took to do that so that's just like a helper function it's not super important this is kind of what we were doing before in the playground and in the gpt's chat interface but now we're going to do it in Python the first step is defining our instructions so this is essentially the system message so I'm using the same thing we had in the playground here sha GPT functioning as a virtual data science consultant on YouTube blah blah blah blah blah just copy paste that here and then we can create the assistant using this chunk of code here the way this works is we created this client object earlier and then we're going to be accessing the assistance API and we're just going to use this create method what that allows us to do is create an assistant set its name its description its instructions and the model that we want to use notice that this is everything we said here the name description isn't here but you can set the description via the assistance API the instructions and the model so it's all the same and then I just print the assistant not that it gives us a whole lot of useful information but it's just this object that looks like this okay in order to talk to the assistant you can set up this thread object which is new in the assistance API compared to the chat completion API which is what I talked about in a previous video we can create this thread which is just like a back and forth conversation it helps avoid doing this boiler plate code of keeping track of what the system message is and what the user says and what the assistant says and what the user says and assistant says so on and so forth It's all handled by the thread so that's super convenient you can generate the user message so here I just put something like great content thank you so that's a comment and then we can add this comment to the thread using this so we have to specify which thread we're going to add the message to which role is saying the message and then the content of the message we get the thread idea from here role is user and then content is here then finally we can send the message to the assistant to generate a response the way that works is you use this method here so we're in the threads module the runs subm module and we're going to use this create method to create a run which is just an API call essentially so we'll need to provide the thread ID which we just added this user message to and we need to provide the assistant ID and so where this comes from is the assistant object we made ear earlier if we do that we can run that and then it'll take a few seconds to run so that's why I run this helper function I made and then once the run is complete we can view that returned object so it took about 5 Seconds to do that first one so actually we can just like run this whole thing I think that's fine yeah we'll just run this whole thing all right so it took less time this time so it took about 3 seconds to run so automatically when we do this run when the assistant generates the response it gets automatically added to the thread so we don't have to do anything extra so all we can do is grab the messages from this thread that was just updated by the assistant and we can print the most recent message in the thread so the response from the assistant is you're welcome I'm glad you found it helpful if you have any more questions or topics you're curious about feel free to ask shot GPZ it signed off correctly which is nice and it is a pretty positive response it captures the sentiment that I convey it's just super long I never talk like this again I don't like reading and I don't want to burden people people with just putting way too much text for them to read I try to keep my responses pretty concise then here I just delete the assistant because what happens is every time you make an assistant it'll pop up here and if you're running this notebook as just like a tutorial or you're running it multiple times debugging or something you're going to have a very long list of assistants here if you don't delete them so that's what this line of code's doing okay so we have this problem that it's giving nice responses but these are just way too long they don't really sound like me one thing we can do to get sha GPT to sound more like me is to give it some examples to learn from so that's what I do here through so-called fuse shot prompting it's essentially where you put a set of examples in the instructions for the assistant so these are real past comments and my responses to these comments and all I did was append this sha GPT to them so it has that format we're looking for and then we just do the same thing we create another system in the same way we name it shot GPT we give it a description we give it the new set of instructions which is is what we defined here and then we Define the model then we can create a new thread so we can talk to the assistant we'll use the same user message great content thank you we'll add the message to the thread so again specifying the thread ID the role of who's saying it and the message then we'll run the thread so we'll send it to the assistant to generate a response by passing in the thread ID and assistant ID and then we'll wait a few seconds for the assistant to generate a response so it took about 3 seconds and then here we're just printing the assistance response so this is what sha GPT says now with the updated instructions to the same exact comment which is you're welcome happy to hear you found it useful shot GPT so that's a lot better I would probably have said just something like glad it was helpful happy to hear you found it useful maybe just one of these to make it a bit more concise cuz you're just saying the same thing in more words but this is a lot better than the original and a lot better than what we were seeing in the playground with the no code Solutions but of course we could have have added the few shot examples in the no code solution and we would have gotten the same results so another thing is technical questions which Chad GPT might respond in a certain way but it may not be the same way that I would respond to that technical question so before we tried adding PDF versions of my article so we could get the context of how I talk about fat tailedness for example but it for whatever reason it wasn't capturing that but now we've done this few shot prompting so let's see how its response changes we're just doing the same exact thing as before we're creating a new thread we're generating a user message so a new thread is as if we opened a new chat GPT chat so it's not going to remember this previous message that's why we're making the new thread add user message to thread so exactly what we did before but now the user message is what is fat tailedness and then we're going to send the message to the assistant we actually don't need this here I haven't gone through and cleaned up this code okay and then we wait for the assistant to process the prompt it takes a bit longer now presumably because it's a longer response it's not just like glad you liked it it's explaining what fat tailedness is and in fact when we print the results this is what we get fat tailedness refers to property probability distribution which tails are fatter than those of a normal distribution okay so it's just giving the same thing as we saw before but notice that we haven't added any documents like we did in the no code Solutions so let's do that I quickly deleted the assistant so we don't have them piling up here's how we can add documents to the assistant so the way that works is like this so we go to the files module and just do this create method so we just use this syntax open which is just opening a file so it's creating a python readable version of the file so we're going to open the file we specify the path which is in this articles folder this is just setting that we'll be opening this file for reading purposes as opposed to writing to it we also need to specify the purpose of this file so we'll say it's for assistant the other option is for fine-tuning which we'll see here in a little bit once we create this file it'll actually populate here but I think it actually got deleted at the end of this notebook so if you were to just run this chunk of Code by itself it's going to create a file in your files tab on your open AI account and actually any assistant that you make can use these files for retrieval all you have to do is provide the file ID as we'll see in a second here but notice where all these came from is when we were in the playground and we uploaded all 10 of these articles with the file uploaded we can create a new assistant that can access that particular file so the way that works is we created this file object and now we can just pass it to the assistant so this is the same exact method we were using before to generate the assistant we said the name the description the instructions but now we're going to define the tools so here we're going to use retrieval as a tool and when we do that we also need to specify the file IDs of the documents that the assistant can use so in this case we're just going to use this one that we created up here but of course if you have a ton of files uploaded here you can use all of them and you just need to specify all of the file IDs in this list here and then finally you specify the model so now that we added retrieval let's try the technical question once again so this one took a lot longer to run took about 45 seconds but we're running it in the same way this is interesting it generated kind of like a mixed response something close to what we were seeing before where it's kind of giving a traditional definition of fat tailedness it's just more fat tailed than a normal distribution but then it's bringing in Concepts from the articles that I provided which is you know fat tailedness ranges from thin tailed to very fat tailed which is almost verbatim from the article and then it adds these four different ways to quantify F tailedness and then it kind of gives some context these methods offer different insights into distributions shape allowing for more detailed understanding of the data's Behavior especially regarding the occurrences and impact of rare and extreme events sha GPT yeah that's pretty interesting that it added in those four heris and gave this dichotomy of thin tailed versus fat tailedness which is something I talk about in the article and then finally I delete the assistant and delete the file if you're running this multiple times you don't accumulate a bunch of files and assistants on your account and then some more resources I'll put all these in the description you can read more about the assistance AP API just from like a high level the documentation which is the API reference breaks down the python side of things and then finally more On Tools so here we just used retrieval but there are two other tools that you can use which are the code interpreter which we saw in both the playground and the gpts and the functions tool which is something we saw in the assistants playground so just going back to that we only used retrieval here but we could have also used the code interpreter and functions so if you want to read more about that I'll provide this link in the description kind of talks about the tools talks about the code interpreter which as it says here allows assistance API to write and run python code so that's on the code interpreter reading images and files okay that's cool and also had the input output logs that's cool knowledge retrieval which is what we did in all the different examples I talked about here and then function calling so this is the cool thing and you can almost have any function with this capability so the way this works is you need to give the name of the function along with a natural language description and then you need to also provide a so-called Json schema which essentially outlines what the inputs of the function are so the assistant knows what information to give to this function in order to get a response at this point we've seen both no code and python ways of generating an assistant and we saw that we can get pretty far by using things like retrieval and fuse shot prompting to improve the assistant performance however there's still still something missing from the assistant responses it just doesn't really feel like something I would say it's very verbose and doesn't quite explain things how I would explain it so to kind of better capture this feel aspect of the assistant responses let's turn to fine-tuning the model unlike the assistance API there's no no code solution for the fine-tuning bit you have to write some python code to fine-tune the model and it requires a bit more work because in order to do fine tuning you need to curate a training data set so here I'm going to kind of walk through the process of how I did that for this YouTube comment responder use case here's another Jupiter notebook example code is on the GitHub repository so check it out if you like here we're importing some modules again importing the open AI python Library importing the secret key in the same way that we did in the previous example and then here importing a few other libraries that we're going to use for the data preparation so again we're going to set up our client and then we're going to need to prepare the training data so this is what makes fine tuning a lot of work it's just acquiring the right data and a important rule in machine learning if there are any rules in machine learning is that data quality is the most important thing not how fancy your model is not how efficient your code is but how good your data is at capturing the thing that you're trying to model and so in this case what I did was I went to my YouTube channel and I took real comments that people left and then real comments that I responded with to create this input output pair so what that looks like is this so I got about 60 comments and responses just copy pasted into this numbers sheet I use numbers because I'm on Mac but you could use Excel to just copy paste it in there and then I exported it as a CSV file just to see some examples of this comment was this was a very thorough introduction to llms and answer answered many questions I had thank you to which I said great to hear glad it was helpful smiley face and of course we can have emojis in here this one says Mr Moneybags over here which was funny so these are just real world comments which we incorporate into F shop prompting but you just can't fit 60 comments and responses into your assistant instructions and if you do it's just going to create this like bulky overhead because the instructions are going to be passed to the assistant every time a thread is initiated so you can imagine that the API calls are just going to become more costly over the long run if you just have a really large instruction set and so that's where fine tuning is helpful in fact open AI has a nice guide on fine tuning here we go common use cases so I'll also put this in the description some common use cases when fine-tuning can improve results setting the style tone format and other qualitative aspects so that's the main motivation for using it in this use case another one's improving reliability at producing the desired output next is correcting failures to follow complex prompts handling many edge cases in specific ways so I guess that's good if there's like a specific prompts or specific situations where the assistant is failing you can just include those in your training data set and it'll learn how to not do that and then performing a new skill or task that's hard to articulate in a prompt and so I like how they put it here the high Lev way to think about it is you want to find tune when it's easier to show not tell kind of like how I was experiencing in the initial no code GPT solution it was asking me these questions and I wasn't necessarily sure how to give the best instructions that's another downside of these no code Solutions is that even though it doesn't require us to write python code it may not be obvious how to give good instructions using natural language showing not telling is just a better way to convey the desired behavior of the assistant going back to the training data prep we have all these comments and responses in the CSV file how however the fine-tuning API doesn't take CSV files as input the data need to be in a very specific format which they talk about here data preparation and Analysis for chat model fine tuning and basically it wants the examples the inputs and outputs in a Json L format so basically it wants it in a text file here's an example it wants it in this format where each row is an example and it consists of a system message a user message and then an assistant message and each line is in the Json format which you can essentially think of as a python dictionary what we need to do is take the CSV file from here and translate it into the proper format in order to use it for fine tuning the way that works here is I initialize these lists to hold all the comments and all the responses then I open the CSV file this is a CSV file YouTube comments. CSV in read mode this like a common syntax tax for reading text files in Python not entirely sure how it actually works but it's something I've used a lot so maybe I should figure out how it works but anyway my understanding is that it handles the opening and closing of the text file but if someone knows better than me drop it in the comments below I'm curious so the first thing we do is use this CSV do reader method to read the file and then what that allows us to do is go through the CSV file line by line and just grab each chunk of text so we're doing that in this for Loop and then we're actually going to skip the first line cuz as you can see here the first line says comment and response so what I'm doing is if the first element of the line is equal to comment to just skip that line that's how I'm skipping the first line here but this will probably be different if your first line isn't comment and response with that first line out of the way the for Loop will go to the next line which is this comment and this response and then what I'm going to do is append the comment to the comment list and then append the response to the response list but also appending this Shaw GPT sign off to the response so we get that desired behavior and of course you can do whatever string manipulation you like here to ensure that the output format of the assistant is whatever you like so here I just have like a simple thing that I'm adding to it but if you want it to be in like a Json format or you want it to have like a particular format you can do whatever string manipulation you like to ensure that it has that format so this will just go all the way through the CSV file just grabbing each comment and each response so we do that it'll save all the comments to the comment list so that's what this is and then it'll have all my responses here or should I say all of sha gpt's responses here and so that's what that looks like you can see emojis and all okay so all we did right now is just put all the comments and responses from the CSV file in a list we haven't actually made it into this Json L format so in order to do that we go to this next cell where we Define our instruction string and we generate each example line by line so the way that works is again going back to this example here each example is a dictionary so it's this key value pair where the key is always messages for whatever reason and then the value is a list which is this list here however the list is a list of dictionaries so it contains three dictionaries the first one corresponding to the system message the second dictionary corresponding to the user's message and the third dictionary corresponding to the assistance response that's the goal that's what we're trying to do here and so the way I do it is Define the system message just once then I just go like index by index through the comment list we go 0 to 58 through the comment list because there are 59 elements and we just generate each of these three dictionaries we generate the system message we generate the user message and then we generate the assistant response and so that's what we're doing here so roll system content is always going to be this instructions string then we have roll user the content is going to be the I element of the comment list and then we have the assistant role and its content is the I element of the response list okay and then we just take these three dictionaries put them into a list and then we create a dictionary I kind of do two things in one step here so sorry if it's unclear but we take this list and create a dictionary with the key is messages and the value is this messages list and then we append this dictionary to the example list so it's matching this format here where each line of this text file is a Json format so it's essentially a dictionary and in the same way each element of this example list is a dictionary okay so there was a issue with with this line of code which I just fixed but what we're doing here is we're going to create our train test split we're going to designate a set of examples for training and then we're going to designate another set of examples for the validation data set and so the way I do that is I randomly generate nine integers between zero and the number of examples minus one and then I use those indices to create a new list for the validation data set so this is just creating a list here where it's going to go through each index in this index list here and it's going to copy that example from example list and put it into Data validation list and since these examples will still be an example list we can go through and iteratively remove them in this for Loop so now at this stage we've got two lists one is called example list which is a list of dictionaries that have the data in the proper format this should have 50 elements it does and then we have have validation data list which is just all the examples in our validation data set and so this is also a list of dictionaries and then we write these examples to two different files so the first one is for the training data and the second one is for the validation data and so kind of in a similar way as before we're opening this text file specifically a Json L file with the right flag on and then we'll just go through each element of example list and dump it into this Json file and then we'll do a new line character so it creates a new line for each example and then we'll do the same exact thing for the validation data set and so once we do that it should create these files here training data. Json L validation data. Json L and then we can upload these files to the open aai API for fine-tuning the way you do that is very similar to what we saw in the previous notebook when we're uploading the article for rag purposes for retrieval purposes so we just use this files. create method meod we open the file which we just saved but now we set the purpose as fine-tune as opposed to assistance do that for both the training and validation data I just put that there so it didn't run the job prematurely and then we can just run the fine-tuning job so the way it works here is we specify the training file the validation file ID which is just a property of these two objects we can set a suffix which is basically some unique identifier or some identifier we can put into the name of the fine-tuning job or name of the model so we can identify it and then we can specify what model we want to use so gp4 is not available for fine tuning the most state-of-the-art advanced model for fine tuning available is gbt 3.5 turbo so that's what we're going to use here and then we can just run this and it'll create this fine-tuning job and then if we go back to our open AI account click on this fine-tuning tab we can see this fine tuning job is running and so this actually kind of takes 15 or 20 minutes it's not something that runs like immediately fast but I did this yesterday so we can just use this model it's kind of like the cooking shows like we already cooked the pasta last night and we're going to eat it in front of you and so yeah it's already set up here so fine tuning jobs running once it finishes running after like 20 minutes or so we'll be able to use it another thing about fine-tuning is that your fine-tune models don't integrate into the assistance API they only work with the chat completions API which means those tools that we could include like retrieval like the code interpreter like the functions which we could just easily add to our system by specifying some keyword argument that's not available natively through the chat completions API so if you want to add rag you want to add tools you got to build out that functionality yourself using python code and of course you can use like Lang chain or llama index or some python library to do that but it's not integrated into chat completions I'm sure at some point the fine-tuning models will be incorporated to the assistance API but at this point it's not available okay so we're going to throw into test comment so we specify the model and then we have to define the messages like this so we Define the system message using the instruction string from before then we pass the user comment just like that then we can run that and then Sean GPT just says thanks it's more concise I'll say that for sure and I don't know if I've ever just said thanks to a comment but this is a pretty brief response let's see what another response looks like and so this is a comment that came in recently it wasn't in the training data set so let's see what the response to this looks like so the comment was I'm typing this after watching half of the video as I'm already amazed with the clarity of explanation exceptional and then sha GPT says wow thanks for the compliment that's pretty good that that is something that I would write I was playing around with this yesterday and it gave like a pretty cheeky answer I don't know if it'll generate that again but I found that funny it was something like thanks for commenting I hope the second half of the video is good and it had some emojis in there and I thought it was funny but now it's like generating responses that I would say oh yeah here we go this is something similar to what I saw yesterday so appreciate the compliment hope the second half doesn't disappoint so now let's see how it handles a technical question again the fine tune model doesn't have retrieval built into it if I wanted to add retrieval to this model I need to use like llama index or Lang chain or something like that to add it in there so let's see how it handles this of course it's not going to describe fat tilted how I did in the articles but it is giving a much more concise and closer response to what I would say compared to what we were seeing before like this monstrosity without the fine tuning there you go that's how you do fine tuning it takes a little bit more effort up front to get together your training data set but honestly it took me about maybe 20 to 30 minutes to manually copy paste those comments into the CSV file maybe another like 20 minutes to write this script to prepare the training data but you don't have to write the script you can just bring in your CSV file and it should be ready to go and then again I'll delete the training in valid ation files hopefully that doesn't ruin the fine-tuning job but it doesn't really matter and then more resources here I'll drop these in the description below so you can take a closer look at it so open AI guide for fine-tuning that was pretty helpful all of open ai's documentation is super clean and easy to understand let's see they have their API reference on fine tuning and then how to prepare your data for fine tuning is also available here so overall impressions of fine tuning I'm honestly pretty impressed I didn't expect it to work as well as it did especially because it only used 50 training examples in grad school if I wanted to train a good model I needed 100,000 examples to train a good predictive model but because of the power of GPT 3.5 turbo it's already a powerful model even fine tuning with just 50 examples results in really good results that's why personally I feel like fine-tuning is probably the biggest Innovation that we've seen in machine learning because if you can find the right base model for your use case and you can just tweak it with just a handful of examples you can get really good results in a fraction of the time that it would have taken you to develop that model from scratch okay so that's basically it we talked about three different ways to build a custom AI assistant using open AI we first talked about the no code solution which allowed you to build an assistant pretty quickly without any python coding next we looked at the assistance API which gave us a pretty straightforward way of creating assistance using python code which we can take and Port over to an application or a website and then finally we saw how we can fine tune an assistant to dramatically change its style and really Come Away with something that has good performance if you want to play around with the no Cod shot GPT I'll provide a link in the description below but if you want to interact with the fine tuned version of sha GPT drop a comment in the comment section below and I'll be sure to respond with shot GPT unless you explicitly don't want me to respond with shbt just let me know and I'll respond as myself so I hope this tutorial was helpful and gave you an idea of which approach might be best for your particular use case of building an AI assistant while open AI does currently have the state of the art large language models for building these things a natural question is how can we build these types of AI assistants using open-source Solutions so that's exactly what I'm going to cover in future videos of this series where we're going to explore both retrieval augmented generation or rag and model fine-tuning using open-source models and if you enjoyed this content this is just one video in a much larger series on using large language models in practice so if you want to learn more check out the series playlist Linked In the description below and in the comment section and as always thank you so much for your time and thanks for watching"
ytmK_ErTWss,2024-01-29T14:53:40.000000,LLMs EXPLAINED in 60 seconds #ai,I'm going to explain large language models in 60 seconds if you've heard of chat GPT then you've heard of large language models or llms for short while this might make you think that llms are just chat Bots that's not necessarily the case an llm is essentially a word predictor meaning given a sequence of words it produces the most likely next word this is just like the autocomplete function on your smartphone the way it gets good at this is by reading trillions of words from the internet and learning which word should come next what makes llms different than pre-existing Technologies like autocomplete is that they can take this ability to Simply predict the next word and generate humanlike and helpful responses to prompts one can then take an llm tweak it a bit and turn it into a powerful chat bot like chat GPT
mtu_v335bQo,2024-01-22T15:06:15.000000,3 Lessons from AI & Data Consultants #freelancing,recently sat down with 10 Ai and data science consultants and asked them how they get clients here are three key takeaways from those conversations first don't skip the discovery phase this avoids rushing into building something and inadvertently solving the wrong problem second is to always ask why why is this important to your business why do this now why me this simultaneously uncovers context for the project and the core problem the client is trying to solve third and finally is to find your lead Source while there are many ways you can get clients like referrals outbound marketing freelancing platforms content creation everyone that I talk to runs their business through one single lead source
8z-WPpP1_-8,2024-01-20T00:48:38.000000,AI for Business: A (non-technical) introduction,these days it seems like everyone is talking about AI with new Innovations seemingly coming out every single week however if you're a professional an entrepreneur or business operator you might be thinking to yourself what is this AI thing and how can I use it to drive value in my business so in this video I'm going to share a non-technical introduction to Ai and machine learning and share how it can fit into how we do business and if you're new here welcome I'm Shaw I'm a data scientist turned entrepreneur and if you enjoy this content please consider subscribing as a great noost way you can support me in all the videos that I make so in this video I'm going to be talking about two main things in part one I'm going to be answering the question what is AI and along the way defining some key terms such as artificial intelligence which is what AI stands for we'll be talking about models and why those are important for AI and then finally we'll be talking about machine learning so once we have a good understanding of what all these terms mean we can turn to part two and figure figure out how we can actually use these Technologies I'll give a concrete example of what AI might look like in practice then I'll share five rules of thumb that I like to use when thinking about how and when to use AI in practice starting from the top what is AI so when you hear the term AI you might think chat GPT or AI generated art or you might think of the Terminator or something similar but if we just take a step back AI stands for artificial intelligence so we've got two words here and one of them is a bit more problematic than the other the first word artificial is not the issue artificial simply means something that is made by humans however the second term intelligence isn't so well defined and even today there's not really a consensus of what this word actually means however a definition that I like to use and one that I think is relevant in a business context is intelligence is the ability to solve problems and make decisions based on this operational definition AI or artificial intelligence is simply a computer's ability to solve problems and make decisions and so to get a better sense of what we mean by intelligence let's see it in action so let's say we wake up Saturday morning and we're trying to figure out what we want to do today and we look out the window and see this so if we see this and we're trying to decide between a pool day or a Netflix day I think most people would pick the Netflix day because the dark clouds in the sky is probably a good indication that the weather's not going to be so great another example is if we see this sales data with this peak in November and someone asks us what caused the peak we might reasonably say that oh was probably because of Black Friday which is one of the biggest retail days of the entire year and then finally if we see this text exchange of someone saying fine do what you want the other person responding are you okay and then that original person saying yeah whatever and if we ask are they really fine most people probably say no know even though the person is saying that they're fine their choice of words like fine do what you want and their use of whatever is probably indicating that they are actually not fine each of these scenarios is a situation where we used our intelligence to make a decision or to solve a problem so even though each of these examples is very different there's a Common Thread that runs through each of them which is intelligence requires knowing how the world works but of course the world is a mass place and it's very complicated so the way we make sense of this huge and complicated world is through models and a model is simply a approximation of a real world thing that can fit into our heads and more specifically models allow us to make predictions for example when we saw the dark cloudy sky that information went into our mental model of the world and allowed us to make the prediction that it's probably going to rain later however models aren't only restricted to the ones that we have in our heads we can also have computer models and in fact essentially all weather forecasts are done by computer models instead of your weatherman standing outside for 5 minutes and making a forast for the day so models be they mental models or computer models are an essential part of intelligence but a natural question here is where do these models come from so there are two types of models that I'm going to talk about the first I'll call principle driven models which are based on a set of rules these are things you might read in a book or learn from your grandma the other kind of model is based on past examples a principle driven model would say that if we see dark clouds in the sky then it's probably going to rain later while a datadriven model would say the sky is similar to other times when it rained and so each of these models comes to the same conclusion that it's going to rain but they are built on top of a different different Foundation but of course each of these models isn't restricted to something we hold up in our heads but these are things we can program into computers so principle driven models we can explicitly program computers to execute using standard programming techniques but more recently we've seen the rise of datadriven techniques to derive models the most popular of which is called machine learning machine learning is potentially another one of those buzzwords you may have heard around but it's a really simple concept machine learning is just a computer's ability to learn by example so the way this works is we have a set of training data which consists of predictors and targets where targets are the things that we're trying to predict like if it's going to rain or not and predictors are all the information that we're going to use in order to estimate the target the key Point here is instead of explicitly telling the computer how to take predictor to estimate the target machine learning allows the computer to figure out the relationship between predictors and targets simply by seeing many different examples of the two so the way that works is we pass this training data into a machine learning algorithm and out poops our machine learning model with this machine learning model in hand what we can do is get new data pass it into the model and obtain a prediction which is exactly what we did when we saw the dark cloudy Sky we looked out the window we received some information and we were able to make a prediction that it's going to rain later and a machine learning model Works in exactly the same way so up until this point we've talked about three things we talked about artificial intelligence which we Define simply as a computer's ability to solve problems and make decisions we also talked about models which were a essential part of intelligence because they allow us to understand how the world works works and then finally here we talked about machine learning which is a way a computer can generate a model based on past examples with these three terms defined we can move on to the second part of the talk which is how do we use these things how do we use these Technologies like Ai and machine learning to drive value in our businesses so I'll start with a concrete example and talk about credit decisioning which is something I have some real world experience with so I can talk about it some somewhat intelligibly so when we're talking about credit decisioning what we're talking about is people applying for a loan and financial service providers evaluating that application and making a decision of whether to approve the loan or deny the loan so the way that works is someone submits an application for a loan and the financial services company makes a decision of whether to approve deny in the terms of the agreement so the traditional way of doing this is that the application goes to an underwriter which is a person who makes the decision and defines the terms of the contract however now that we've learned about Ai and machine learning we might think oh we can just replace the human underwriter with an AI underwriter right and the answer to this question is yes and no while it might be easy to imagine replacing a human job with an AI the reality is a bit more complicated so what this looks like in practice is something like this with all these steps within the blackbox being our AI underwriter and really what it is is not just a machine learning model but rather a large number of business rules data and it processes all working together to take the application and finally make a credit granting decision so although machine learning is a critical part of this AI underwriter here it is only a component in a much broader solution and so this is often the reality of what AI looks like in practice although from an outside view it might look as simple as we have an AI underwriter in reality what's going on under the hood is a bit more complicated which is an important point to keep in mind when trying to implement AI Solutions in your business so while this might be an illustrative example it may not give us a good idea of how and when to use AI to solve business problems so for that I'm going to talk about five rules of thumb that I like to use when thinking about how and when to use AI in practice so the first one is to focus on problems not Technologies next is to apply AI to problems that you solve repeatedly next we have look for problems in which a 70% solution is good enough to generate value the fourth is pick situations in which one failure of your AI solution doesn't erase nine successes and then finally start simple fast and easy and build sophistication as needed so I'm going to talk through each of these rules of THB one by one and share concrete examples of each first focus on problems not Technologies so this brings up what in data science we might call the hammer problem which is when you have a really nice Hammer everything looks like a nail so let's say you have some problem in your business something is broken if you take a technology first approach you might grab your hammer and say I got this which obviously is not going to solve the problem and is probably going to make things a lot worse so an example of this from my personal experience is something I saw over and over again which was last year I had a lot of clients reaching out to me asking for help in building a custom chatbot or fine-tuning a chatbot for a particular use case and this was a classic example of the hammer problem because often what had happened was the client had seen the power of chat GPT and saw all the incredible innovations that have been happening in the space of natural language processing and large language models and was probably thinking something like I need one of these for my business however the Trap that you fall into with the technology first approach is that you can spend a lot of time and money building a solution for a problem that isn't very critical to your business and essentially this time and money is wasted however let's flip things around instead of starting with the technology what was starting with the problem look like so let's say we have a problem where our customer support line is overwhelmed well from here instead of jumping into building something you jump into problem solving because when you have a tool your instinct is to build but when you have a problem your instinct is to solve the problem so you might ask why are people calling if people are calling for some specific piece of information you can update your FAQs and if that doesn't cut it you can improve call routing to make sure that callers are getting sent to the right person and there isn't time wasted where customer support representatives are on the phone with someone just to transfer them to someone else and then maybe after exploring a few Solutions then you start thinking about building a chatot for your website but the key Point here is that when you start with a problem you don't jump to building a solution you jump to finding the root cause of the problem so you can find the best solution and ultimately When comparing these two approaches the technology first approach approach on the left and the problem first approach on the right you almost always want to go with the problem first approach because that is almost guaranteed to generate value in your business while the technology first approach might be intellectually stimulating and exciting is often something that doesn't drive any real value the next rule of thumb is to apply AI to problems you solve repeatedly and the reasoning behind this is that AI is just the continuation of the story of Technology since the beginning of time it's simply a tool to help make our lives easier so the problems that you're solving over and over again are great candidates to apply AI to for a few reason one if you can automate it with AI you no longer have to spend a lot of your time solving that problem or even if you reduce the amount of effort it takes you to solve that problem by some marginal amount it can still translate to some big gains other reasons are if you're solving a problem repeatedly you likely have a deep understanding of that problem which puts you in a good position to build good solutions to solve it and finally if you're already solving a problem that means you have an existing solution which is a fantastic starting place for building an AI solution so an example of this is something that I use in my own work which is a literature review assistant so I read a lot of papers about Ai and machine learning for both my content and my Consulting business and often when reading research articles I discover gaps in my my understanding and so this is a problem that I face over and over again I'm reading the paper and then I stumble across a sentence that seems obvious to the authors but is completely not obvious to me so for that I will turn to chat GPT I'll upload the PDF of the paper ask chat gbt what the paper is about then ask chat gbt specific questions until I have a clear understanding of what's going on so using Chad gbt in this way has significantly sped up how quickly I can read articles because now instead of spending a 30 minute tangent on Google trying to figure out what a particular term means or putting an idea in a larger context Chach PD does a pretty good job of explaining things and adding additional context where needed the next rule of thumb is find situations where the 70% solution is good enough where this is coming from is a model is simply an approximation of the real world thing no model is ever going to be perfect and there's a famous quote from statis George box which goes all models are wrong some are useful so the key thing is to accept that your model is not going to be perfect but pick the ones that are actually useful to you in whatever problem you're trying to solve and so a good example of this is Spam filtering the way that works is you have a bunch of spam emails flooding your inbox in this situation even an imperfect model is very valuable because even a 70% reduction in spam emails is very helpful that'll give a thumbs up for from any user another important rule of thumb is ensure that one failure doesn't erase nine successes and essentially what we're talking about here is find the low stakes or low exposure situations an example of this might be using chat gbt as a writing assistant is pretty low stakes if it gives nine good recommendations followed by one bad recommendation for writing it's no big deal you can just ignore that recommendation and move on with your life however if you're using using Chachi BT to make cancer diagnosis it doesn't matter if it's right nine out of 10 times that one time when it's wrong can have a tremendous negative impact so that is a situation where you probably don't want to use Ai and if you do you have to be very thoughtful about how it's implemented and then the final rule of thumb is to start simple fast and easy and each of these words simple fast and easy is important so starting simple is important because sophisticated Solutions are fragile and costly they'll cost you a lot of money to build and they have a high likelihood of failure because they are well complicated next you want to build fast because to build good Solutions you'll need to iterate so that means you'll need to try out a lot of different things and if it takes you a long time to do one iteration it's going to take you a long time before you implement a good solution and then finally you want to make it easy so you want the solution to kind of be on the way and not something way out of the the way for people because if it's hard to access no one's going to use it including you even if you're the one that's implementing the solution so let's look at a specific example let's say we're trying to implement a sales email sequence in our business the way this start simple fast and easy approach would play out is you'll start by writing all these emails by hand so what that looks like in my business is I'll have someone book a discovery call with me and I'll send them a follow-up email asking them a couple of follow-up questions based on their specific use case that's me doing the process by hand however after doing that for a bit I've naturally developed email templates for responding to someone booking a discovery call and like a post Discovery call email and then maybe another template for following up with people after 40 days or following up with people after 90 days and so on and so forth so over time instead of just writing emails by hand you start to develop templates and then over time those templates can get loaded into a CRM so you use a CRM tool to automatically fill in these emails with some bit of personalization like including people's names and maybe some other information that they provide but then you can take this one step further and use some kind of large language model or NLP solution to make the emails a bit more personalized so instead of just using a template and just filling in a name you can make the email sound more like a person so all that to say it's good to start here you know start by just doing things by hand and build toward that sophisticated solution often times when you're a small business you'll find that just doing it by hand or having some templates are more than suitable so for me I have a small Consulting business so I'm spending most of my time here I don't have a CRM but let's say you have like a 10p person business then you might want to be looking at a CRM then let's say you have a larger Enterprise and let's say you're working with hundreds or thousands of clients then maybe building the AI solution makes sense but the value in taking this simple fast and easy approach is that you don't artific officially just jump to the end you take it step by step and you only move on to the next level of sophistication if the value is there if it makes sense for your business we talked about a lot of different things so just to recap a few key terms we talked about Ai and how it's a computer's ability to solve problems and make decisions we also talked about models and how they help us make predictions and that they're a necessary part of any AI system and then finally we talked about machine learning which is a datadriven way computers can generate models from past examples as opposed to being programmed explicitly and then we talked about how we can use AI through five different rules of thumb focusing on problems not Technologies applying AI to problems you solve repeatedly seeking problems where the 70% solution is good enough identifying problems where one failure doesn't erase nine successes and taking this simple fast and easy approach to iteratively develop AI Solutions so while this was a pretty highlevel introduction I hope it gave you some clarity about what AI is and how you can start to use it in your business this is the first video in a larger series on how to use AI in business in future videos I'm going to dive into more the project management side of machine learning and model development so if you have any specific questions or anything specific you'd like to see in future videos of this series please drop those in the comment section below and as always thank you so much for your time and thanks for watching
jGn95KDWZMU,2024-01-11T22:12:29.000000,5 Questions Every Data Scientist Should Hardcode into Their Brain,data science is more than just building fancy machine learning models when you boil it down the key objective of data science is to solve problems the trouble however is at the outset of most data science projects we rarely have a well-defined problem in these situations the role of a data scientist isn't to have all the answers but rather to ask the right questions in this video I'll share five questions that every data scientist should hardcode into their brain to make ident identifying and defining business problems second nature and if you're new here I'm Shaw I make content about data science and Entrepreneurship and if you enjoyed this video please consider subscribing that's a great no cost way to support me in all the content that I make before diving into the questions I want to give some context for where they are coming from like many others when I started my data science journey I was hyperfocused on learning tools and Technologies while this technical Foundation is necessary to be a a successful data scientist focusing too much on tools creates the hammer problem which is when you have a really nice Hammer everything looks like a nail this often leads to projects that are intellectually stimulating yet practically useless I finally outgrew this approach when I joined a data science team at a large Enterprise the key lesson from that experience was the importance of focusing on problems rather than Technologies this means that one should gain a sufficiently deep understanding of the business problem before writing a single line of code and since as data scientists we don't typically solve our own problems we gain this understanding through conversations with stakeholders and clients getting this right is important because if you don't you can spend a lot of time and money solving the wrong problem this is where problem Discovery questions come in over the past 6 months I've developed a bit of an obsession with cracking these early stage discover y conversations with stakeholders and clients my approach to getting better at this has been twofold first I interviewed 10 seasoned data Freelancers about their best practices and how they approach these conversations and second I took as many Discovery calls as possible which ended up being around 25 the five questions I share here are the culmination of all these efforts while this is by no means a complete list these are questions that seem to come up over and over again so the first question here is what problem are you trying to solve while in theory this should be the only question you need to ask in practice things don't typically work out that way in most instances clients aren't super clear on the problem that they need to solve and even if they are I typically will need to do some catching up to better understand the business context either way this question is helpful because it ideally brings up follow-up questions which allow me to dig deeper into the client's world for example if a client says we tried creating a custom chapot with open AI but it didn't provide good results I might ask what was the chapot used for or what makes you say the results weren't good and a lot of times if a follow-up question doesn't come to mind I find a really helpful practice is just to rephrase and summarize what the client tells me most times this is another way to keep the conversation going and keep digging into the challenges that the client is facing a natural way to follow up the what question is why this is one of the most powerful questions you can ask a client because it can unlock the floodgates to the client's motivations goals assumptions and Beyond however why questions have a tendency to make people defensive which is why having multiple ways of phrasing this question can be helpful some examples of this are as follows why is this important to your business why do you want to solve this now what does solving this mean for your business how does this fit into the larger goals of your business why do you want to use AI to solve this problem the key benefit of asking the why question or any of its variants is that they allow you to dig more deeply into the client's problem and ultimately identify the root cause this is reminiscent of Toyota's five wise approach which teaches to get to the root cause of any problem one should ask why five times these first two questions of what are we doing and why are we doing it are two of the most fundamental questions in business so getting really good at asking what and why in many different ways can take you very far the next question is what's your dream outcome I like this question because it essentially combines the what and why questions and it tends to get people to speak to their vision of the project in a way that may not come through when asked directly having multiple ways of asking what and why is important because it often takes a few passes to really get to the root cause of a client's problem two related questions here are what does success look like and how would we measure it these are a bit more pragmatic than a dream outcome but are helpful for transitioning from asking what and why to how the next question is what have you tried so far this helps narrow down potential Solutions in two ways one it helps avoid wasting time on things that didn't work and two any new project should build upon existing work this latter point is based on the philosophy that data science projects should seek incremental Innovation therefore they should be simple and iterative for situations where the client hasn't built anything so far one can ask any of the following questions what is the existing solution how do you solve this problem now what have others done to solve a similar problem in either case these questions help set the stage for the project and help you avoid Reinventing the wheel the final question is one I got from Master negotiator Chris Voss which is why me asking this question is an effective way to reveal people's motivations for talking to you often this Sparks additional context of what led them to to you and how they see you fitting into the project which is helpful for next steps sometimes however people don't have good answers to this question which may indicate they don't actually want to work with you and they're holding back some deeper motive such as they're looking for free consulting or they're looking for a competing bid to take to the person they actually want to work with a key lesson for me these past 6 months was to learn these questions I.E hardcode them into my brain but then forget about them the point isn't to mindlessly go down a list of questions when talking to clients but rather get to the point where these questions naturally form in your mind during the flow of conversation this intuition is something that can only develop through practice toward that end here are three key takeaways that have been helpful to me in developing this skill set first don't just study these questions use them while this may result in a fair share of awkward moments it's all part of the learning process and don't worry I'm still learning to second is to stay curious the goal of these early stage conversations isn't to look smart or sell but rather to learn which brings up the final takeaway listen more than you talk my rule of thumb is to wait until the last 5 to 10 minutes of a 30 minute call to start offering recommendations and next steps prior to that my challenge is to ask questions rephrase and summarize client answers and to ask follow-up questions following my Natural Curiosity if you enjoyed this content please consider subscribing that's a great no cost way you can support me in all the videos that I make to read more about this topic check out the blog in TS data science which you can access using the friend Link in the description below like I said earlier this is by no means a complete list so if you have anything to add please drop those in the comments section below and as always thank you so much for your time and thanks for watching
scAxgeGadv4,2024-01-10T14:38:49.000000,2 Types of Data You Should Know #datascience,are two types of data that every data scientist should know about they are what we can call thin tailed data and fat tailed data thin tail data are gaussian like things they have the key property that no single observation will significantly impact the aggregate statistics of the data some examples of thin tail data include Heights weights and test scores fat tail data on the other hand are more parade like these data have the key property that a single observ can and often will significantly impact the aggregate statistics some examples of fat tail data include sales wealth Wars pandemics and so many other things that we care about so before you do any kind of analysis or build any kind of model ask yourself whatat kind of data am I working with
GvRPKPCg5no,2023-12-26T23:07:01.000000,How to learn anything #learning,looking dumb makes you smart this was one of my biggest takeaways from grad school and the point is once I realized that by not being afraid to look dumb not being afraid to ask questions I actually learned so much more so much faster than I did before and this is something that has served me very well even after grad school into the corporate world working as a data scientist it's natural to want to look smart or want to give off this Persona that you know what you're talking about and you're professional you kind of get everything that everyone's talking about but more often than not this holds you back
BGZu6WxevoM,2023-12-19T22:05:55.000000,How Much YouTube Paid Me in My First 6 Months of Monetization (as a Data Science Creator),"6 months ago I joined the YouTube partners program which provides ways for creators to earn money from their YouTube videos while you may have heard stories of big creators making hundreds of thousands or even millions of dollars on YouTube what you don't often hear are the stories of smaller creators who are just at the beginning of their YouTube journey in this video I'm going to break down how much money I made as a small Creator in my first 6 months of monetization my goal with this video is to help give those considering a similar Journey a realistic view of What monetization on YouTube looks like in the early days if you're new here my name is Shaw I make content about data science and Entrepreneurship and if you enjoy this content please consider subscribing that's a great noost way you can support me in all the videos that I make to give a little background I started making YouTube videos about 3 years ago while I was getting my physics PhD as someone who had survived grad school with the help of YouTube I wanted to give back by sharing content that would have been helpful to a past version of myself I did this for about 2 and 1/2 years before finally satisfying all the different criteria for monetization for those who aren't familiar there are three main criteria for getting monetized on YouTube the first is to upload three videos the second is to get 1,000 subscribers and the third is to get 4,000 watchtime hours as far as timing goes it took me about 3 months to hit that first Criterion 2 years to hit that second Criterion of 1,000 subscribers and then finally 2 and 1/2 years to hit that final Criterion of 4,000 watchtime hours YouTube's partners program provides creators five different ways of monetizing their content the three that I have set up are supers watch page ads and shorts feed ads supers are a native way that viewers can leave tips for creators while watch page ads and shorts feed ads are exactly what they sound like advertisers pay YouTube to run their ads on the platform of which creators get a portion the two monetization options I don't currently use are memberships and shopping while these are things I may set up in the future at this point I don't really have any good ideas of how to provide value through these means all right with all the context out of the way let's get down to it how much money did I make on YouTube after 2 and 1/2 years of waiting in my first month monetized on YouTube I made a grand total of drum roll $20.82 while this might sound like a very small number it is actually more than I expected I've actually heard stories of creators getting about 20 cents in their first month of monetization so one thing that makes a big difference in how much creators earn are CPM and RPM which are short for cost per Millie and revenue per Millie Millie just means thousand so cost per Millie is how much YouTube charges advertisers to run their ad one th000 times on a particular video while Revenue per Millie or RPM is how much YouTube pays the Creator for those 1,000 views so different topics draw different audiences which correspond to different cpms and RPMs my content touches on education technology and business which may have higher CPM and RPM than some other genres so while my first month of monetization wasn't so hot lucky for me this number went up over time in my second month of monetization I made $285 in my third month I made $366 so what we're seeing here is linear growth with about an $8 increase in Revenue per month however this growth trajectory started to change in October in my fourth month of monetization my Revenue doubled and hit $73 86 in the next month it almost quadrupled and hit $287 38 and so far in December I've hit $21.56 so today's December 15th if we wanted to guesstimate what this month's Revenue might be it'll probably be around $400 these numbers highlight a key property of content creation namely growth isn't linear it's exponential the catch however is exponential growth is painfully slow in the early days a great example of this is that it took me 2 years to hit 1,000 subscribers but then it took me 6 months to hit the next 1,000 subscribers and then after that it only took me 3 months to hit the next 1,000 subscribers and now in the past 3 months my subscriber count has more than doubled so a natural question is what the heck is driving this growth and we get a pretty clear answer when looking at the analytics so let's see what that looks like so here we have the channel analytics and I'm a data scientist so I like to think about analytics I like to think about explanations and models for why do things happen the way that they do YouTube analytics is super interesting because it's a great example of exponential growth and what we might call a fat tailed distribution and if you're not familiar with fat taals I actually got a video series all about it so go check that out but anyway I think the growth that I'm seeing on my channel has a pretty simple explanation and so if we see the top five videos for the past month a pattern starts to emerge where three out of five of these are all about large language models but in particular there's this video fine-tuning large language models that has kind of gone viral and gotten a lot of traction so I think there are two things kind of driving this growth so like one people love large language models it's a really hot topic right now everyone's talking about generative AI Etc the second thing I think is driving this is the fact that these three videos are all part of the same six-part video series and I think making content in series format helps drive this exponential growth and there's a really simple explanation for that which is Network effects basically if someone watches one video in this series it increases the probability that they'll watch another video in this series and if they watch another video in this series it increases the probability they'll watch yet another video in this series so you get this positive feedback loop this positive reinforcement and this is really Amplified by my strategy which was I made a six-part video series and I also made a six-part Blog series on medium which was published in towards data science and towards data science has their own distribution and then you have Network effects working over there so I think all these different factors came together to kind of drive this viral phenomenon in this exponential growth however there are two other videos here that are also performing really well which is how to make a data science portfolio completely for free with GitHub pages and how to create a custom email signature with Gmail and I think the thing with these these two is that these are just like Evergreen content and these solve two very particular pain points that I have experienced in the past so it's not super surprising that other people also have these pain points and are looking for this content I personally found a hard time finding good tutorials on these two topics so I decided to just make them because they would have been very helpful to me when I was first trying to solve these problems so adding up all my earnings from these first 6 months of monetization my grand grand total revenue from YouTube in 2023 is $648 34 while there's no way I can pay my bills with this kind of income it still blows my mind that I can post a video on YouTube and money just appears in my bank account three other ways I've monetized content creation this past year our earnings from medium which is a blogging platform sponsorships actually just one sponsorship and consulting which I'll explain more starting with the first one medium is a membership based platform form so the way monetization works there is that writers have the option to make their stories for Members Only from which they gain earnings from readership and engagement mediums partners program works a bit differently where I was able to start earning money pretty quickly when I started posting on the platform which means I've been earning on medium for the entirety of 2023 and so when I add this all up this comes to a grand total of $5,185 182 the next way I made money outside of YouTube earnings was through a sponsorship deal this consisted of including an affiliate Link in the description of my video on how to make a custom Gmail signature this generated $200 in earnings plus a 15% referral fee for anyone that signs up for the sponsor service today we don't have any signups so so far the earnings have just been $200 then finally I put Consulting on the list since I don't make enough money to support myself from content creation my main source of revenue for the near future is my independent Consulting business where I help businesses build Ai and analytics products currently all my leads for this business come via my content whether it's YouTube videos or blogs I post on medium sometimes people even directly book paid calls with me through my calendly link this happened five times this past year generating $365 in revenue and so adding together all three of these non youube Revenue sources this comes to a total of 5,000 7478 which again is nothing you can pay your bills with but this is mindboggling when I first started making content I never thought I would be able to generate this kind of Revenue however content creation has unlocked so many new opportunities even Beyond this direct monetization like I mentioned earlier it's my main lead source for my Consulting business but what I would say is more valuable than that is all the amazing connections I've been able to make with like-minded people who are on a similar Journey as me while I didn't have time to cover all aspects of my earnings and analytics if you have any specific questions please feel free to drop those in the comments section below I'm more than happy to share details that might be helpful and relevant to you and I hope this has been helpful for anyone curious about content creation while it might be easy to glamorize the life of a content creator the reality of being a small Creator which is actually most creators may not be what you expect and if you enjoyed this content please consider subscribing and sh sharing with others that's a great no cost way you can support me in all the videos that I make and as always thank you so much for your time and thanks for watching"
sS10PXKqm7o,2023-12-19T15:03:57.000000,Why You Need to “Play” #getpaidtolive #podcast,think of play I think explore you know I think mistakes I think no rules essentially and if you're not living your dream life already then I would assert necessarily you need to play because your current life is not your dream life right so how do you expect to Live Your Dream Life by just continuing to live your current life you know continuing to live by the same rules not exploring new possibil not willing to make mes you know and if you kind of constrain yourself into your current life and your current situation and the only way out of that is to play is to explore and so you at least have a chance of achieving your dream life yeah but the only way to do that is through play
0iFEtnHyzE0,2023-12-18T14:55:36.000000,Fine-tuning EXPLAINED in 40 sec #generativeai,I'm going to explain fine-tuning a large language model in less than 60 seconds fine-tuning is when you take a pre-trained machine learning model and adapt it for a particular use case the analogy I like to use for this is fine-tuning is like taking a raw diamond as we see on the left here and refining it into something more valuable as we see on the right so with this analogy the raw diamond would be a pre-trained model like gpt3 while the refined Diamond that you would would actually buy is going to be something like instruct GPT or chat GPT drop your fine-tuning questions in the comments below
15Kd9OPn7tw,2023-12-11T18:29:09.000000,4 Ways to Measure Fat Tails with Python (+ Example Code),"this is the third video in a largest series all about power laws and fat tails in the previous video of the series we saw how we can fit power law distributions to empirical data while this can be handy when working with fat tailed data in the real world this idea of fat tailedness is something that goes beyond just a power law distribution in this video I'm going to break down four ways we can quantify fat tails and then I'm going to walk through how you can Implement these Tech techniques in Python and if you're new to the channel welcome my name is Shaw I make videos about data science and Entrepreneurship and if you enjoyed this content please consider subscribing that's a great no cost way you can support me in the videos that I make so jumping right into it in past videos of the series we've been talking about fat tales which we defined as when rare events Drive the aggregate statistics of the distribution we saw a celian example of this in the Paro distribution where for example 80% of sales May Come From 20% of your customers but this idea of fat tails is much more General than any particular distribution fat tailedness is something that lives on a spectrum here we have a sort of distribution landscape ranging from thin tailed distributions all the way to very fat tail distributions we can see the Paro distributions on the right hand side here with increasing level of fat tailedness on the far left we see the gaussian distribution which is an Exemplar of thin Tails or not fat tail data and then we have this middle region which we saw is populated by the log normal distribution taking this view of fat tailedness as not some binary property of a distribution being fat tailed or not fat tailed but rather a quantity that lives on a spectrum this naturally begs the question how do we Define fat tailedness while there is no true measure of fat tailedness there are a handful of heris we can use in practice to help us quantify how fat tailed a given distribution is and so here I'm going to talk about four heris the first being the power law tail index next being curtosis third heris is the log normal distributions Sigma value and the finalistic uses a metric called Kappa defined by n TB whose ideas we talked about in the first video of this series so starting with heris one the power law tail index as we saw in the first video of this series the smaller a power laws tail index I.E Alpha the fatter its tail this is illustrated by this plot here where we see with increasing Alpha values you can see the tales of the distribution getting fatter and fatter so rare events become more and more common as Alpha increases these plots here are generated by this probability d density function where a is just some constant X is a specific value of a random variable and Alpha is the tail index so this is the parameter that controls the shape of these Tails here so if a smaller Alpha implies a fatter tail we can naturally use this as a way to quantify fat tailedness so in practice what this might look like is a twep process step one is to fit a power lot to your data and this is exactly what we did in the previous video of the series and then step two is to extract the estimate for Alpha so while this might be a simple approach it has one obvious limitation which is if a power lot doesn't fit your data well then this technique is going to break down because the alpha value is essentially meaningless another way we can quantify fat tailedness is via curtosis which is a measure of non-gaussianity so just taking a step back the Exemplar thin tail distribution is the Beloved gaussian which looks like this so a gaussian is thin tailed because the rare events are so rare they are essentially negligible for instance an event that is 5 Sigma away from the mean has a 1 in 3.5 million chance of happening which is in stark contrast to what we see in a paredo distribution taking this as a starting point the rough idea with heuristic 2 is saying okay gaussian equal thin tailed so something that is ungan or not gaussian should be fat tailed this motivates us to look toward these so-called non-gaussianity measures of which curtosis is the most popular we can Define curtosis according to this expression here where mu4 and mu2 are the fourth and second moments of the distribution which we can Define like this so the intuition with curtosis is that it will increase as as data accumulates in the tail so more data in the tail means a higher kosis so higher kosis means fatter tails however there's one major limitation with using cetosis namely when talking about Paro distributions with an alpha value less than or equal to four it is not defined so even if in practice you can always compute a value for kosis when working with very fat tailed paros those values are meaningless so here is sck number three is using the log normals Sigma value where the bigger the sigma the fatter the tail we got a flavor of the log normal distribution in the past videos of the series where we saw that the log normal distribution was a bit mischievous because sometimes it could appear more gaussian like and other times it could appear more parade of like based on this Sigma value an example of this is shown here where a log normal distribution with mual 0 and sigma = 0.2 looks a lot like a gaussian however a log normal distribution with the same mean but Sigma equal to 2 looks a lot like a Paro digging a little deeper this is the probability density function for the log normal distribution so this equation is what is generating these plots up here and sigma sits here and here in the expression and so this is what is driving the shape of this long tail this observation of bigger Sigma implies fatter Tails naturally motivates us to try to use Sigma as a way to quantify the fat tailedness of a distribution so we can do this in a very similar way as we did heuristic one where we follow a twep process Step One is we fit a log normal distribution to our data and then the second step is to extract the sigma estimate and this is exactly what we did in the previous video of this series while again this is a straightforward procedure it like a good job at explaining your data then this technique is going to break down the finalistic is tb's cetric this is probably the most robust of the three heris we can use however it is also the most sophisticated so I'm going to try to break it down step by step if you don't really care about the mathematical details feel free to jump ahead to the example code to see what working with these heris in practice looks like Kappa is defined according to this expression here where it's a value that sits between 0 and one where zero implies the distribution is maximally thin tailed and one implies the distribution is maximally fat tailed and just as a note this is for unimodal data with finite mean so there's a lot going on in this expression here so let's just break it down step by step first let's look at these values here n KN and n n KN and N are both integers that correspond to two samples we can call S Sub and not and S subn and so Kappa is actually a metric that compares two samples together SN KN and SN where SN is equal to the sum of N samples drawn from a particular distribution so for example if we have a gaussian distribution we would just take n samples and then we would sum them all together and that would give us s subn notice these are little S's and this is a big S these are Big S's and then the simplest example is S Sub one which you just draw one sample from this distribution if we choose n to be 30 and N not to be 1 we would just plug in 1 and 30 here we would plug in a 30 here a one here a 30 here and a one here but what does this thing mean taking a closer look at this term here M denotes the mean absolute deviation which is a measure of dispersion around the mean breaking down this equation here this notation is just representing the mean absolute deviation of a particular sample s subn and it's defined as follows we get the expectation value of the sample so we compute its mean then we subtract the mean from each value in the sample take its absolute value and then take the average of all these subsequent values this is broken down a bit more here e is denoting the expectation value s subn is equal to the sum of a bunch of subsamples and then S Sub I is just some subsample of a distribution the intuition here is that more data in the tail implies a larger mean absolute deviation because again it's a measure of dispersion the more dispersed the data are the larger the mean absolute deviation so let's look at a specific example of this say we choose n not to be 1 and N to be 30 in the thin tailed case m30 is going to be about equal to M1 so their ratio is going to be about equal to one on the other hand if you're working with fat tail data m30 is is going to be much larger than M1 which implies that their ratio is going to be much greater than one this is illustrated by these plots here where on the left we're seeing how the mean absolute deviation scales with increasing number of subsamples so on the left we have the sum of N Gans and on the right we have the sum of N paros you can see for the sum of gaussians the mean absolute deviation goes from about 1 up to about 8 however when you look at the sum of N Paro it starts around one and it ends up at around 500 and so with this observation that for thin tail data this ratio is about one however for fat tail data it's going to be much greater than one we can see how this influences the cetric defined earlier and so we have our thin tail data this is just plugging in values of n notal 1 and Nal 30 and so if this ratio is about one then this value is going to be very small that's because the log of one is equal to zero so as the ratio approaches one this term will approach zero so if the denominator is small that'll make this whole term here big and then when you subtract it from two it's going to make the whole thing small that's how we can see that zero implies maximally thin tailed however in the fat tailed case we see the exact opposite so if this ratio is Big then this denominator is going to be big making this whole term small and then making this whole term big that's where one implying maximally fat tailed data comes from so if this was all way too much math then you were hoping for here's the key takeaway big Kappa implies a fat tail small Kappa implies a thin tail so now let's dive into some example code here we're going to quantify the fat tailedness of data from my social media accounts this is the same data set we saw in the previous video of the series and the data set and code are available on the GitHub repo linked here and in the description below we'll start by importing some handy modules namely matplot lib pandas numpy the power Law Library we saw in the previous video and then we'll import curtosis from the scipi library next we'll load in the data again the data is on the GitHub repo stored in separate CSV files what I'm doing here is iteratively Reading in each CSV as a pandas data frame and then I'm storing each data frame into a dictionary it's always a good idea to look at your data no matter what data science project you're working on here I'm just looping through each of the different data sets again so they're all stored in this dictionary and then I'm just iteratively extracting the data frame plotting its histogram so this plot histograms function is a bit long and nothing really special it just uses mat plot lips hist function that functions on the GitHub if you're curious and then the figure is being saved another thing that makes sense to look at since we're trying to evaluate fat tails here we're going to look at the top five record records of each data set by percentage here are the histograms so that function actually plots two histograms the histogram of the raw values and then the histogram of the log of the raw values this data here are the number of medium followers gained on a month-to-month basis here we have YouTube earnings on a video basis and then finally here we have daily LinkedIn Impressions looking at all these histograms it's pretty clear that they're all fat tailed to some extent medium seems to be the most fat tailed and maybe LinkedIn and YouTube are following slowly behind with LinkedIn being a little less fat tailed that's why I like histogram so much you can extract so much information from just looking at the histogram and a log histogram of any data set next looking at the top five records by percentage here we can see medium followers is significantly fat tailed where 60% of total followers gained is just coming from two months and we see a similar thing with YouTube earnings where about 50% of earnings are coming from the top four videos however for LinkedIn Impressions just looking at the top five we don't see something as Extreme as what we see for medium followers in YouTube earnings so from this view we might say that Medium followers is the most fat tailed YouTube earnings is the next most fat tailed and then LinkedIn impressions are the least fat tailed of the three however let's make this a bit more quantitative by Computing each heuristic for all three data sets starting with heuristic one the power law tail index here I'm using a for Loop just to again Loop through each of the three data sets stored in this dictionary but the key line of code is just this one line here where we fit a power LW to our data and so this is exactly what we did in the previous video of the series and then these seven lines here is just printing the results and so what that looks like is this where medium followers we have an alpha value of 0.83 for YouTube earnings we have an alpha value of 0.9 and then for LinkedIn we have Alpha value of 1.47 also note with these fits is that you'll have a x Min value so this fit isn't for the entire distribution it's starting at some minimum x value and then the alpha value corresponds to all the values greater than that the power lot tail index matches our intuitions from looking at the top five records and from the histograms where medium followers are the most fat tailed because they have the smallest Alpha followed by YouTube earnings and then LinkedIn impressions are the least fat t moving on to heuristic 2 curtosis we do a very similar thing but here it's super easy because we can just compute curtosis in one line of code and it's already available off the shelf in scipi we just load in our data and then here we're just printing the results basically doing some string concatenation here so curtosis equals curtosis of our data and then fer equals true it's just like a convention to make it such that negative curtosis values indicate something something is even more thin tailed than a gaussian distribution while positive values how more fat tailed the data are from a gaussian the results look like this with medium followers having a cetosis of about 21 YouTube earnings 11 and then LinkedIn impressions of 46 this is telling us a different story because the larger the curtosis implies the fatter the Tails so this is saying that LinkedIn actually has the fattest tail followed by medium followers and YouTube earnings one cause for suspicion here is as we saw in heuristic 1 when we did the power law fit all three data sets had Alpha values that were very small which was below that Alpha equals 4 cut off for which the curtosis is not defined for a Paro distribution so it's probably wise to take these values with a grain of salt because if the data truly do follow a power law and the alpha values are as small as we saw in the previous slide any empirical cryosis calculation you do is going to be meaningless because the TR true ptosis is not defined heris stick number three this is very similar to heris stick one we just load in the data do the power law fit it's actually the same exact syntax here the only difference is that when we print the results instead of extracting the alpha and X Min value from the power law fit we're extracting the MU and sigma value from the log normal fit and so that's one cool thing about this power Law Library when using this fit method it'll automatically estimate the power law distribution parameter and the log normal distribution parameters the results look like this where medium followers has a sigma of 2.4 YouTube earnings 5.4 and then LinkedIn Impressions 2.3 and so we're again seeing another story here where since the larger the sigma the fatter the tail Here YouTube earnings is the most fat tailed followed by medium followers and then LinkedIn Impressions however something that stands out to me is that the Mew value here is negative which might tell me that this log normal fit may not do a good job job at explaining the data and as we mentioned previously one of the limitations of heris 1 and herotic 3 is that if the power law or log normal fit doesn't do a good job at explaining the data then these approaches tend to break down moving on to herisk number four since I couldn't find an off-the-shelf implementation of this metric this is going to take a few more steps first we need to implement the mean absolute deviation again this is the expression we saw before and then this is what it looks like in code code next we need to generate end samples from our data set in practice you don't have like a true distribution which you can sample so readily often you just have a set of values you just have an array of numbers and we need to somehow generate samples from that empirical data to do the Kappa calculation we're calling the definition of SN as you just take a bunch of samples from some underlying distribution and sum them together all I'm doing in this code here is instead of sampling from some true distribution I just sample from the empirical data set this is probably a bit naive and I'm sure there are some issues with it but this will be sufficient for our purposes here what this looks like is just initializing SN to be zero and then taking a random sample from our data X so X is just some array of values take a random sample from it with the same size this is a very verbose way of doing it and it's just an artifact that I was playing around with different sizes of samples but ended up just using the same length as X and then this just recursively adds to itself until we reach n number of samples with the mean absolute deviation defined and a way to generate samples defined we can Implement Kappa this function is restricted to Kappa values with n not equal to one and we'll just set that equal to the data that we're working with and then SN will be generated from that function we defined in the previous slide then we'll compute the mean absolute deviation for both of these samples and then we'll compute Kappa in just one line of code Now using this definition of Kappa we can compute the metric in a similar way as we did the other humanistics we need to define the number of samples in the Kappa calculation so we're going to compare S100 to S1 so we're just going to grab each data set and then we're going to compute Kappa in one line of code here so the results look like this where medium followers have a Kappa value 0.27 YouTube earning 0.39 and Linkedin impression 0.31 while this might seem to tell us a different story as we've seen with the other heuristics given the implicit Randomness in the Kappa definition namely when we defined s00 we're randomly sampling our empirical data 100 times so if you run that calculation three times you're going to get three different Capa values so instead of just running this calculation once and drawing conclusions from it the better idea to run this calculation many times before drawing your conclusion that's what I do here so I just set number of runs equal to 1,000 I just Define some Kappa dictionary to store Kappa values for each data set here I'm just grabbing one of the data sets I initialize a Kappa list because we're going to run this calculation 1,000 times and every time we run it we'll just append the Kappa value to this list here is where we actually do the Capa calculation so this Loops 1,000 times and then this list will store it in the Kappa dictionary with a key equal to the file name of that data set and then we just do this three times for each of the data sets the results look like this and so again when you run this calculation you're going to get all sorts of different values for Kappa but it'll start to look like a gaussian as you do more and more runs so the mean is a representative value of this distribution here we see the mean Kappa for medium followers is about 0.4 for YouTube earnings it's about 0.3 and for LinkedIn Impressions it's about 0.34 the story we're getting here is that Medium followers is the most fat tailed because again the larger the Kappa the fatter the tail followed by LinkedIn Impressions and Then followed by YouTube earnings based on all these theistic and looking at the plots and stuff it's pretty clear that Medium followers is the most fat tailed of the three distributions with YouTube earnings and Linkedin Impressions being kind of debatable which of the two is more fat t so if you want to dig a bit more into this topic check out the blog published in towards data science even though this is a member only story you'll be able to access the article completely for free even if you're not a medium member using the friend Link in the description below and this is the case for any video and blog combo that I make also check out the GitHub to get access to the data sets I looked at here as well as the example code and if you enjoyed this content please consider subscribing and sharing with others that's a great no cost way to support me in all the videos that I put out and as always thank you so much for your time and thanks for watching"
TyhlSNB5Ko0,2023-12-11T15:23:39.000000,How I’d learn data analytics (if I had to start over in 2024) #dataanalytics,I was going to start from scratch my dad has a business he has a car dealership if I was starting over I would go to him and be like hey is there any data that I can use to solve a problem for you so if I was trying to get into Data analysis I would take his sales data and make a dashboard and this is actually something I did in grad school but you know maybe your your dad doesn't have a business but if you know anyone that has a business or you know anyone that has data that's one Avenue another Avenue there's so many public data sources out there in the US we have the US Census so like working with that API to get that data and then to do some kind of analysis on US Census Data that could be another Avenue
rtUpRMWFu7k,2023-12-06T14:51:18.000000,How to Move Toward Your DREAM LIFE #getpaidtolive,wherever you are is a starting Place yeah maybe you're getting the money at your job but maybe it's not in the right industry or maybe you're not learning the right skills um or maybe it's not the right role that you want to be working in what you can do is like okay I checked one box but I have like three boxes unchecked is it possible for me to find another job maybe in just in the right industry or at the right company or in the right direction so I can just check one more box and then let me do that for some period of time and then let me re-evaluate and see if I can check another box you know this approach to it is just a checklist that you're ultimately just going down and you know you're not going to do it overnight but like over the course of years you can check off everything on your list and you this is your dream list this is your dream life exactly and just slowly just keep going because what else are you going to do in your life if not check everything off this list oh it's it's like your bucket list
8uwHRVaRsmY,2023-12-04T14:56:41.000000,PCA explained in 60 seconds #datascience,I'm going to explain principal component analysis in less than 60 seconds principal component analysis or PCA for short is a technique commonly used in data science and data analytics i' like to explain it through the following analogy so imagine you have the huge rock band talking a super group you got two drums you got the conas you got the background singers you got the saxophone the trumpets the guitars the piano the keyboard the strings and of course the lead singer while some great music can come out of all these players getting together together for the most part there's a lot of redundant music that gets generated from all these players this is where PCA can help us essentially what PCA can do is take all these players all the music that they're creating and translate it to a compressed representation with PCA you can basically play the same song but with less players but this analogy is saying if you have a high-dimensional data set you can use PCA to reduce the dimensionality of your data while still capturing the essence of what is
x5-IW1m3zPo,2023-11-30T23:54:09.000000,Detecting Power Laws in Real-world Data | w/ Python Code,"this is the second video in a larger series on power laws and fat taals in the previous video I gave a beginner-friendly guide to power laws and presented three problems with using our standard statistical tools and analyzing them while in awareness of these problems can help us avoid them in practice it's not always clear whether some data follows a power law or not in this video I will describe how we can detect power laws from Real World data and share example python code for how you can do this in analyzing real world data from my social media accounts and if you're new here welcome I'm Shaw I make videos about data science and Entrepreneurship and if you enjoyed this content please consider subscribing that's a great no cost way you can support me and all the videos that I make the title here is detecting power laws in data a maximum likelihood based approach with python if that doesn't make any sense hopefully it will in just a few minutes just a quick recap of what we talked about in the previous video the central issue that we discussed was that these things called Power laws break many of our favorite statistical tools in the previous video we talked about two general types of distributions the gaan distribution shown on the left hand side here and the power LW distribution shown on the right hand side qualitatively these distributions look very different and it turns out this qualitative difference raised three major problems with using our standard statistical tools in analyzing power laws namely we saw that the mean was meaningless as well as many other standard statistical tools we saw that regression doesn't work so well and then we also saw that payoffs can diverge from probabilities when working with power laws and this all boils down to a single core property of power laws which is they are driven by rare events and this rare event driven property of power laws is more generally described by so-called fat tail Tails if you're unfamiliar with power laws or fat tails check out the previous video because that's going to be a good primer for everything that we discuss here so this rare event driven property of power laws giving rise to these three problems here motivates us to want to detect power laws from Real World data and so one popular way of doing this is what I'll call the log log approach which essentially boils down to plotting a histogram of your data on a log log plot I'm going to walk through the math of where this is coming from from in the previous video we defined the power laws probability density function according to this expression here where P of Z is the probability density function Z is some random variable F of Z is a slowly varying function of our variable and then we have this Alpha index which is the so-called tail index which defines the shape of the long tail of the power law we also saw that power laws are defined according to a minimum value so everything here here has Z greater than some minimum what this log log approach does is you start with this probability density function and then you take the log of both sides so what happens is you can rewrite the log of the PDF as this and it turns out this is a linear model so we can see that more clearly here where the natural log of our probability density function is r y natural log of our random variable is our X and then this term here will be the slope and then this term here will be the Y intercept since the PDF is linear when you take the log and rearrange these things this implies that the histogram when plotted on a log log plot will look something like this you'll have this line essentially with a negative slope as we can see here because Alpha is always going to be positive and then you can take this one step further and actually do a linear regression and estimate values for the slope and Y intercept and then that'll allow you to estimate the Alpha parameter here so while this is a simple way to try to assess out whether your data follows a power law distribution or not there are a handful of limitations with this approach for one the slope that we estimate from the linear regression is prone to systematic errors another is that when fitting a linear model to data that doesn't follow a power law you can sometimes still get a good fit and then one thing that you'll notice here the data that I'm plotting here is truly a power LW distribution that I artificially generate however this line isn't exactly straight which also seems a bit problematic and so there are a few other issues that I won't get into but if you want to learn more check out reference number one where in the appendix they break down a handful of issues with this log log approach and both these references are available in the description below so an alternative to this and the main topic of this video is a maximum likelihood based approach maximum likelihood is a very popular technique in statistics statistics it's a method for inferring the best parameters for a model given some data and this consists of two main steps the first step is to obtain a likelihood function and then the second step is to maximize that likelihood function with respect to our model parameters hence the name maximum likelihood and so I'm going to walk through step by step what this looks like step one is to write the likelihood function and we do this as follows so we have the likelihood of function denoted by L here the arguments of the likelihood function are our model parameters so for a power law we have two parameters the tail index Alpha and then the minimum value for the distribution and the likelihood is defined as the product of the probability density function over all our observations given our model parameters so what this likelihood function will generate is a number which will tell us how likely some choice of model parameters are given our data next we can use a Paro distribution for our probability density function and you'll notice that this takes the form of our power laws probability density function that we defined previously here's the power law term and then l ofx in this case is just a constant which depends on the model parameters plugging this into our likelihood function we get something like this and then doing a little bit of algebra we get this expression here so again for some choice of model parameters we can plug those values in and then take the product over all of our observations to obtain a likelihood of that particular choice of model parameters next what I call step 1B often people will work with the log likelihood function because one it's a little easier to work with and two the log likelihood function and the likelihood function are maximized by the same choice of model parameters so what this looks like is we'll Define the log likelihood function as lowercase L same inputs as the likelihood function and it's just the natural log of the likelihood of function here and then from the previous slide this is the likelihood function here for a Paro distribution and we'll just take the natural log of that using the property of logs we can translate the product within a log function to the sum of log functions and then again using another property of logs we can bring down the exponents to the front which brings us to this expression here and then in step two we maximize the log likelihood function and so the standard way of maximizing or optimizing a function is we take its derivative and set that derivative equal to zero so what that looks like is we'll take the derivative of the log likelihood function with respect to our tail index which gives us an expression like this and then setting this equal to zero implies that Alpha is equal to this expression here and then rearranging this a little bit we get this expression this expression for Alpha it's called the maximum likelihood estimator for that parameter what this allows us to do is given some data and some value for XMen we can derive the optimal value for Alpha that's all the theory stuff let's see what this looks like in code I'll start with an example using artificial data just so we can get a sense of what to expect when we apply it to real world data first we're going to import some helpful libraries namely numpy matplot lib this power Law Library which implements this maximum likelihood based approach that we just reviewed we'll also import pandas and then we'll fix the random seed for numpy just so the results are repeatable to generate the artificial data we can do it like this here I'm generating data following a Paro distribution with Alpha value 2 X-Men value equal 1 generating 1,000 observations this is just an array x with 1,1 values in it and then we can use this function from the numpy library to generate our random sample from a Paro distribution and then similarly we can generate data following a log normal distribution with mean equal to 10 and sigma equal to 1 using again a function from the numpy library we briefly touched on log normal distributions in the previous video what we saw there is that log normal distributions are a little tricky because they can at times appear more gaussian like and then other times appear more like a power law depending on the value of Sigma just to show this graphically we see that a log normal distribution that is thin tailed can look like a gaussian so this is Sigma equal to 0.2 while at the same time a log normal distribution can also appear fat tailed so this is a sigma value equal to two this presents some more difficulties when working with data in the real world it might be hard to discern whether data follows a power law or maybe it just follows a fat tailed log normal distribution and so lucky for us the power Law Library that we're going to be using here has this functionality built into it so that when we do our fit it'll generate parameter estimates for both a power law and a log normal distribution as well as some other distributions it's always a good idea to look at your data whatever data set you're working with so here it's an artificial data set so we have a pretty good idea of what to expect nevertheless here's what the histogram looks like this is just our data plotted in a regular histogram you can see most of the data are here and then there are so few values in the tail you can't even see them in the histogram however when you take the log the tail becomes a bit more visible so you can see that these values start to pop up more and then we can do a similar thing for the log normal distribution qualitatively it can be kind of easy to misinterpret a log normal distribution as a power law distribution because the regular histogram looks very similar however when you take the log of data following a log normal distribution the histogram looks very different than that of the power Lot distribution so this almost looks like a gaussian distribution a lot of times in practice just doing something as simple as plotting histograms and plotting the log of a histogram gives you a pretty good idea of how fat tailed your data are and whether or not it follows a power law distribution however let's see how we can make this a bit more objective and quantitative using the power Law Library we can use the fit method to fit our data generated from a par distribution to a power log with just one line of code and then we can print the results so Alpha is our estimated tail index Xmen is the estimated X-Men value and then p is a quality score it's a number between zero and one that represents how good of a fit the power law distribution is the closer the value is to one the better the power law fit so that's telling us that this is a pretty good fit before comparing to the ground truth values it's important to point out that the power law libraries Alpha definition is different than the standard definition that I've been using throughout this video series so namely the library's Alpha value is equal to the standard Alpha Value Plus one so all we need to do is subtract one from the alpha value generated from the power law fit to make the comparison to the true Alpha value here the true Alpha value we used was two and so subtracting one from the estimate gives us 1.9 so that's a pretty good fit it got pretty close and then the true value of Xmen is 1 and the fit was 1.27 so overall the library did a good job of estimating these parameter values even though we don't have a whole lot of data here okay we can do the same exact thing for the log normal distribution and again we get an alpha value an xmin value an quality score so this should raise some eyebrows again we're getting a very good quality score but this is a log normal distribution why is the log normal distribution described very well by a power law distribution so the thing to point out here is that the X Min value is actually pretty far into the tail looking at this visually here's our histogram of the log normal data and the X-Men value starts about right here so another thing we can do is we can manually fix the xmin value to force the library to fit the distribution to all of the data not just the tail which best fits a power LW so what that looks like is this we can just set this Xmen argument in the fit method and then it'll generate parameter estimates for us of Alpha and Xmen another cool thing about the fit method is that it'll automatically generate estimates of the log normal distribution parameters so without any extra steps we can just plot the MU and sigma estimates for a log normal distribution and we can see that it does a pretty good job at estimating these parameters so comparing to the ground truth of 10 and one the fit method does a pretty good job but of course in practice we don't know what the true parameter values are so just from these results here we wouldn't be able to tell which is a better fit is it going to be the power law fit with these parameters or is it going to be the log normal fit with these parameters so for that we can actually go one step further and compute likelihood ratios between the power law fit and a list of other candidate distributions to give us a sense of which distribution best explains the data so what that looks like is this here I'm just defining a list of different candidate distributions so here we have a log normal exponential truncated power laws stretch exponential log normal positive then I just go through in a for Loop here and use this distribution compare method to compare the power law distribution to each candidate distribution in this list here and then I just print the results what the results look like is an R value and a P value this is different than the P value we saw earlier and this is a P value that we're more used to it is quantifying the significance level of the likelihood ratio R is denoting the likelihood ratio and the way to interpret this is that a positive r value means that the power law distribution is a better fit a negative r value implies that the second distribution is a better fit and then a likelihood ratio of zero means that there's really no difference between the two here in every single case we can see that all the likelihood ratios are negative meaning that all the other candidate distributions are preferred over the power law and we see that the P values are very low they're basically zero in reference number one they use the rule of thumb cut off of 0.1 and this is below that rule of thumb threshold here we see that the likelihood ratio for the log normal distributions has the largest magnitude so we can conclude that the log normal distribution is probably the best fit in this situation and indeed since we know the data was generated from a log normal distribution that it is the correct fit now that we've gone through this whole process with artificial data let's see what this looks like with Messy data from The Real World first we'll just grab this chunk of code cuz we'll use it in a bit here here we're going to be looking at three data sets all coming from my social media accounts we'll be looking at medium followers gained on a month-to-month basis we'll be looking at YouTube earning on a video by video basis and then we'll be looking at LinkedIn Impressions on a day-to-day basis here we just have histograms like we saw before with the artificial data we have the regular histogram on the left and then on the right we have the histogram of the log values it's pretty clear that all of these histograms have pretty fat tails most of the data are sitting in this first bin of the histogram here with a small minority of data in the tals however when we take the log of each value it's more reminiscent of what we saw with the log normal distribution where there is kind of like a typical value that the log of the data tends to be clumped around so based on this we might be thinking the data may be more like a log normal distribution than a power Lot distribution but let's keep going another thing we can do is look at the top five records by percentage what we can see here is that I gained 42% of all of my medium followers in just a single month and so that's the most followers gained in a single month the second place month accounted for about 18% of my medium followers so kind of bringing this together I gained 60% of my medium followers in Just 2 months even though I've been writing on medium for about 3 years now we see a similar thing on YouTube where my number one video by earnings generated 50% of my earnings across all my videos even though I have about 50 videos on YouTube these are very fat tailed distributions which might pull us in the other direction maybe medium followers and YouTube earnings are better explained by the power law and not the log normal distribution but on LinkedIn it's a different story we don't have such a huge distance between the top record and the second and third and so on so this is less fat tailed and I wouldn't be surprised if it's well fit by a log normal distribution based on these numbers and the histogram we saw in the previous slide but let's keep going let's see what the fit looks like the code for this I actually do this all in a for Loop and the full code and notebook is available at the GitHub repository linked here but this is just like a chunk of code from that notebook and it's essentially what we did before on the artificial data set where we're just using this fit method applying it to our data set so I'm doing it in a for Loop so this will kind of iterate between each of the three data sets and then I am manually setting the minimum value the xmin value equal to the smallest value in each data set to forc the fit to use all the data and then from there we can print the power law distribution parameters and the log normal distribution parameters looking at these numbers we see that these Alpha values are very small and again this Alpha value generated from the power law library is one greater than the standard definition of alpha so we actually would need to subtract one from all these values to translate it into the more standard Alpha definition so this would mean that the alpha value is 0.29 0.79 and 0.15 which are super small Alpha values you know typically Alpha values are going to be between 2 and three and if it's really fat tailed it's going to be greater than one but below Alpha equal to 1 as we're seeing in this case here it's what author Nim taleb called the forget about it domain where the mean for the power law distribution is not defined so it becomes very difficult to do any kind of analysis on that data and and just as easily we can get the log normal parameter values one thing that stands out is that the mean for the log normal fit is negative for YouTube earnings so that might be like a red flag that maybe the log normal fit isn't going to be so good for YouTube earnings and just like before we can compute the likelihood ratios to compare the power law fit to that list of other candidate distributions so this is just taking that same chunk of code we saw a couple slides ago and these are the results going through one by one we can see from medium followers the log normal fit is preferred over the power law and it has a significant P value and the magnitude of the likelihood ratio is largest for the log normal fit from this data we would say that the medium followers best follow a log normal distribution for YouTube earnings only one of the ratios is statistically significant and then for that one it's saying the power law distribution is a better fit than the exponential distribution while the other comparisons are inconclusive but kind of based on everything we saw with 50% of earnings coming from one video the negative mean value of the log normal fit I would say that the power law is the best candidate of all the distributions that we're looking at here for the LinkedIn Impressions we actually see something very similar as to what we saw with the artificial log normal data set where all the P values are significant and they're all preferring the non power law distribution So based on this I would say that the LinkedIn Impressions best follow a log normal distribution there's a small caveat here that the medium followers and YouTube earnings data that we're looking at here are a relatively small data set there are less than 100 observations in each and when you're talking about fat tails when you're talking about power laws where you have data driven by rare events small data is a killer because all it takes is one additional data point to completely skew the fits that we're looking at here so all it takes is maybe a couple more observations of like extreme values of medium followers to completely change this from best being described as a log normal fit to a power law fit and conversely maybe as I put more videos out and get more data on earnings I find that the data better follows a log normal fit than a power law fit so we should just take these results with a grain of salt which should always be your mindset as a data scientist skepticism is the default mindset for for a scientist okay so looking ahead to what's next while it is helpful to have a idea of whether some data follows a power law distribution or not this idea of fat tailedness that we described in the previous video is something more General than data following a power law or not following a power law and as we saw before fat tailedness is really on a spectrum from not fat tailed at all to very fat tailed this is why it can be handy to forget about this idea aidea of fitting data to particular distributions and just try to focus on quantifying the fat tailedness of some data set that's what we're going to talk about in the next video of this series through four different heris for quantifying fat tailed in this and then a couple other things I want to call out if you enjoyed this video and you want to dig a Little Deeper check out the blog published in towards data science this kind of goes into a bit more details that I may not have covered here even though this is a member only story you can access it completely for free even if you're not a medium member using the friend Link in the description below and this is the case for any one of my YouTube videos they will all have friend links in them so everyone who is trying to learn this stuff can access the blogs and then finally the code is available on the GitHub repository shown here and Linked In the description below so if you enjoyed this content please consider subscribing that's a great no cost way to support me in all the content that I generate if you have any questions questions or suggestions for future content please drop those in the comments section below and as always thank you so much for your time and thanks for watching"
0oBoHwwJYJQ,2023-11-27T17:29:19.000000,Do NOT become an entrepreneur #entrepreneurship,do not become an entrepreneur if you cannot handle these three things first and foremost failure yeah no one likes failure but not being able to handle failure is a different thing entirely being able to turn failures into success is a key part of Entrepreneurship second thing is uncertainty if you can't handle uncertainty entrepreneurship is going to be even more miserable than it needs to be and finally managing yourself this means being your own manager most of the time what entrepreneurship looks like is you working by yourself so you can't rely on the social structure you might see at a larger organization or Corporation however if you can endure these three things you can turn failures into success you can persevere through uncertainty and you have the discipline to manage yourself and Entrepreneurship might be for you
2Axas1OvafQ,2023-11-20T19:35:35.000000,DON’T study Gen AI #generativeai,don't study generative AI those early in their career will often ask me what's the best degree to get if I want to get into Ai and while I don't think there's a best path that's true for everyone I do think that there are some paths that are more fragile than others rule of thumb that I buy into to is that when it comes to developing a foundational knowledge about something the older the subject the better and this is based on something called the Lindy principle which B basically says that the life expectancy of a subject is proportional to its current age for example math has been around for thousands of years so we can probably expect math to be around for a few more Thousand Years conversely generative AI has been around for about a year so I expect it to stick around for about another year so the moral of the story is if you want to study something that's going to last study something that's been around for a while
Wcqt49dXtm8,2023-11-15T16:06:40.000000,"Pareto, Power Laws, and Fat Tails","statistics is the Bedrock of Science and data analysis this is why we all learn about it in some form or fashion in school however many of our favorite statistical techniques are completely useless when applied to a certain type of data this specific type of data are called Power laws in this video I'll be giving a beginner friendly introduction to power laws and describe three problems that come up when trying to apply our standard statistical tools to analyze them if you're new to the channel I'm Shaw I make content about data science and Entrepreneurship and if you enjoyed this video please consider subscribing that's a great no cost way to support me in all the videos that I make and with that let's get into it so the official title of this talk is Paro power laws and fat tales what they don't teach you in statistics we'll start with the background information I'll talk about the gaussian distribution Fredo's 8020 rule introduce the power lock class class and describe the difference between weight and wealth then I'll move on to three big problems when trying to use traditional statistical approaches to analyze data following a power law distribution and then finally I will introduce the idea of fat tails which generalizes a key property of these power law distributions so many quantities in nature tend to Clump around a typical value one example of this is if you go to a busy coffee shop and measure the weights of all the customers coming in and out of the coffee shop you would eventually observe a pattern like the one shown here so in other words the weights would tend to Clump around some typical value and then Decay rapidly toward these Tails this is a distribution that most people are familiar with it's called a gaussian distribution also called a bell curve and the great thing about data that follows a gausian distribution is that we can capture a lot of the essential information of the underlying data with just a single number which is the mean and you can go even further and capture how spread out this distribution is via measures like the standard deviation and so these concepts of a gaussian the mean the standard deviation variance Etc these are all Concepts that people will learn in an introductory statistics course or a business statistics course and indeed these are powerful techniques for analyzing ing data solving problems and making decisions however not all data that we care about follows a distribution like a gaussian and a great example of this comes from the work of vredo paredo and so many people have probably heard of paro's principle or the 8020 Rule and typically how this is quoted is that 80% of sales come from 20% of customers however this idea did not originate from the business world or sales and marketing it actually originated from the work of an Italian economist IST and mathematician vredo paredo in his study of Italian land ownership where he found that about 80% of the land in Italy was owned by about 20% of the citizens this simple observation is indicative of Statistics that are very different from the gaussian distribution that we saw in the coffee shop and so what this 8020 rule or Paro principle implies is that the underlying data follows a Paro distribution which looks like this this just qualitatively this looks very different than the gaussian distribution from the previous slide and the biggest difference here is that there's no typical value around which the data is clumped so in the case of a gaussian the mean is very representative of the overall distribution however when looking at a Paro distribution the mean doesn't give you a whole lot of information so in this case the mean is going to be somewhere around here which doesn't tell you much about a lot of the dat data that's living in the so-called tail over here putting this another way while knowing the average weight of an Italian man gives you a good idea of what to expect on your next trip to Rome knowing the average population of an Italian city which is about 7500 is completely useless in grounding your expectations and the reason for this is that weight tends to follow a giian distribution while city populations tend to follow a parade of distribution so the parade distribution is actually part of a broader class of distributions called Power laws and so here are a few different Power laws in red we actually see a power law matching this 8020 rule like we saw in the previous slide making this a bit more General a power law is defined by this equation here so PDF is the probability density function X is a random variable Little X is some specific value of that random variable L of X is some slowly varying function and then Alpha is is just some number which defines the shape of the distribution here and another important note is that power laws are only defined Beyond a minimum value so in these plots here the minimum value is one but this value could be anything these two types of distributions the gaussian like distributions and now these like parol like power law distributions they give us these two conceptual anchors by which we can qualitatively categorize data that we observe in the the real world author Nim Nicholas TB in his book The Black Swan defines these two categories as mediocre Stan and extremist where mediocris are the gaussian like data while extremist are the Paro like data and so the key property of data from mediocris is that no single observation will significantly impact the aggregate statistics to see an example of this suppose on your trip to Rome you go visit the Coliseum and then again you have your scale with you and you decide to start weighing random strangers at the Coliseum so let's say you weigh a th000 people at the Coliseum and compute the average and it turns out to be 175 lb then suppose you add to this 1,000 person sample the heaviest Italian that you can find and so if you do this this will have very little impact on the mean the average might go from 175 lb to 175.2 lb and this is the key property of data from mediocre Stan which is again that no single observation will significantly impact the aggregate statistics there's going to be no person on Earth that you can add to this sample that will dramatically change the mean of the weight distribution however data from extremist on is different in this case a single observation can and often will drive the aggregate statistics so let's say instead of weighing people at the Coliseum you ask them what their net worth is again you get that same sample of 1,000 people and you compute their mean net worth and you find it to be about $300,000 and then let's say you add the richest Italian to the sample what's going to happen here is that the average net worth is going to go from about $300,000 to $7.5 million so about a 25x increase in the average from just a single observation and so that's the key property of data from extremist on and data following a Paro like distribution to get a bit more intuition about this here are some more examples from mediocris Stan and extremist respectively gaussian like data will be things like IQ weight height calorie consumption test scores car accidents mortality rates blood pressure on the other side data from extremist on will be things like wealth as we saw at the Coliseum sales as people talk about with the 8020 rule in business city populations which we mentioned earlier pandemics deaths in wars and terrorist attacks word occurrences and text a very small number of words will be used the most amount of times academic citations a very small number of researchers get the bulk of the citations and Company sizes there are very few number of companies that employ most of the world's Workforce as you can see the things that live an extremists on isn't some trivial set of things in fact you could argue that most of the things that we care about as a society and civilization are not gaussian likee at all while this may seem just like splitting hairs some like technical exercise of categorizing data as gaussian like or par like it turns out there are major limitations to our standard statistical Tools in analyzing data from extremist on and so here I'll highlight three such problems with using our so-called stat 101 techniques to try to analyze these quantities that we care about and so this all boils down to one thing the law of large numbers which basically says if we take n random samples the sample mean will approach the true mean as the number of samples goes to Infinity put another way if we start collecting data generated from a gaussian distribution as we collect more and more samples more and more observations the average that we compute from our sample will approach the true average of the underlying distribution this is also true for the Poo distribution and a uniform distribution and a log normal distribution any distribution that has a finite mean the law of large numbers is true however in practice we never have infinite data we can only have a certain number of observations and this results in some complications with the law of large numbers assumption if we take 10 observations we'll get a pretty accurate sample mean of a gaussian distribution however if we take 10 observations of something generated from a Paro distribution the sample mean is going to be biased this is all because the the law of large numbers Works more slowly for power laws than gaussian distributions which brings us to our first problem the mean is meaningless as well as many other metrics when it comes to working with finite sample sizes of data that follows a power law distribution is that it takes much longer for the mean to converge to the true value compared to a gaussian so we can see this from the plots shown here so on the left we have the number of samples on the x-axis and then on the y- AIS we have the sample mean so this black line here is the true mean and then the blue line is the mean that we compute when the data is generated from a gaussian while this orange line here is the mean that we compute when the data is generated from a Paro distribution as you can see the gussian is never too far off from the True Value you know maybe in the super small sample sizes you have a biased mean but pretty quickly it starts to get really close to the True Value however for the power lot we can see the sample mean is not only much more biased than the gaussian but it's also much more erratic and this extends to not just small sample sizes like 100 observations but to a th000 observations and even 10,000 observations this whole time the Paro sample me is much more erratic than the gaussian and much more biased this even extends to when we 10x the sample size even more to a 100,000 observations at this point the gaussian is right on the money the mean isn't changing at all with additional observations however with the power law the mean is still wiggling around and not quite the True Value and so we're seeing bias at 100,000 observations for the power law similar to what we were seeing at about 10 observations for the gaussian but this isn't limited to just the mean we see this for many other standard statistical quantities that's what's being shown here on the left hand side of these plots we have the respective quantities so we have the median the standard deviation the variance the mean the max first percentile the 99th percentile ptosis and entropy and then horizontally oriented we have 100 samples the th sample case and the 10,000 sample case so while some of these quantities are relatively stable like the median once you get to sufficient sample size it tends to level out the minimum value even in small sample size it's pretty accurate and the first percentile in small sample size is pretty accurate and stable some of these other quantities can't seem to land on a particular value so namely standard deviation variance the maximum the 99th percentile to some extent curtosis and then entropy seems to continually be changing Without End so the one quantity I want to highlight here is the maximum and that's because given this property that rare events Drive the statistics of power LW distributions as sample size increases we see a order of magnitude increase in the maximum value when we go from a th000 samples to 10,000 samples the danger here is that you could have a maximum value that seems stable in a relatively small sample size let's say you have 7,000 observations and the max value seems to have plateaued and it seems pretty stable but then as you collect more data you have this huge jump in the max value and so the danger here is that you can be in this period where it seems like things are stable and predictable but then all of a sudden you have this huge change in the data that you're observing so to connect this to the real world if this data were say deaths from a pandemic what this might look like is the deadliest pandemic in a 100-year time span will be in order of magnitude less severe than a pandemic in a Thousand-Year time span the deadliest pandemic in the past 100 years was the Spanish Flu which killed about 50 million people and we might think okay that was the deadliest pandemic it's not going to get any worse than that if the data is following a power law we can't be surprised if over a Thousand-Year time period the deadliest pandemic claims 500 million victims so this is highlighting this key property of data from extremist on which is that rare events Drive the underlying statistics however this doesn't stop with the mean and all the other standard statistical quantities that we see here it also impacts our ability to make predictions effectively which brings us to problem two regression doesn't work so what regression boils down to is predicting future events from past data and intuitively if your data is driven by rare events you may simply just not have enough past observations to make good predictions about the future and this problem is exacerbated when working with power law distributions so let's look at a particular example let's suppose that we want to do linear regression between the variable X and Y here x is a normally distributed random variable m and b are the parameters that we're trying to learn and e is a noise term that follows a power law distribution so one case where regression just completely breaks down is when this noise term has an alpha value that tail index we saw earlier when we defined power laws is less than or equal to two because in this case the power law has infinite variance so the variance of this noise term is going to be infinity and it turns out if the variance of this noise term is infinite then the variance of this whole equation will be infinite which makes the R 2 value go to zero there's a quick derivation of this in citation number two Linked In the description below in chapter 6.7 but of course you can't observe infinite variance in practice because your data is necessarily finite so what's going to happen when doing regression in practice is going to going be similar to what we saw before with the max value where the results might seem stable in small sample size but then break down as more data are collected we can see this through an example taking our normally distributed random variable with the added power law noise term and doing a linear regression with a 100 samples the results of our regression might look like this which looks pretty good you know maybe there's some outliers here but overall we get a pretty good fit and the r squ isn't bad however this is incorrect correct because the noise term has infinite variance which means r s should actually be zero in this case and indeed as we collect more and more data we can see the R squ value quickly deteriorating so we go from 100 samples to a th000 to 10,000 to 100,000 to a million to 10 million to 100 million and so on this is the danger of doing regression with data that follows a power LW your results might look deceivingly well in small sample size but then as you collect more data your model performance quickly deteriorates but at this point you might say Shaw what's the big deal you know so what if our model can't predict some super rare events like these like 1 in a th000 one in 10,000 Etc events the model can predict 99% of things pretty well why do we care about these super rare events and I agree with you when data are generated from a power law it's not hard to be right most of the time because most of the data do not live in this long t of the power law however when solving problems and making decisions in the real world probabilities are only half of the story the other half of the story are payoffs which brings us to problem number three payoffs diverge from probabilities in other words it's not just about how often you are right or wrong but also what happens when you're right or wrong so let's see what this might look like in a business context consider a software company with three key offerings offer one is they have a free software that has ads they have a premium offer which it's no ads with some monthly subscription and then they have a third offer which is a Enterprise level software with different customizations and add-ons and whatever those clients need and let's say that the 8020 rule is in play so 80% of sales comes from 20% of customers what this might look like is that 80% of customers go with offer one they just use the free version 16% of customers use the premium version and then 4% of clients are the Enterprise clients what this means for revenue is that 20% of the revenue comes from the free users 16% of the revenue comes from the premium users and 64% of the revenue most of the revenue comes from the Enterprise customers so let's say the software company wants to optimize the core service making it run 25% more efficiently and as any good company might do they're not just going to roll this out blindly they're going to ask the customers first they're going to ask their customers you like this update is this something that you need so they do a survey and they find that 95% of the customers like the update 4% of the customers don't really care and 1% of the customers said the update was bad seeing that the overwhelming majority of the customers like the update the company decides to move forward with the update but now fast forward 6 weeks and the company notices a 50% drop in Revenue so what happened it turns out that the company's three biggest clients dropped the service because the software update killed some Legacy data Integrations that were critical to their business while this is just like a madeup artificial example it's meant to illustrate the point that in extremist being wrong one time can erase the gains of being right 99 times and even Beyond if 1% of your customers are driving 50% of your Revenue that means that you can do something that 99% of your customers love and 1% of your customers hate and be much worse off and so now we're going to talk about about fat tales there has been a bit of controversy in extremist on an example of this is Illustrated around wealth going back to Paro this idea that 80% of the land is owned by 20% of the citizens has kind of been applied throughout economics with the prevailing sentiment being that wealth follows a parol like distribution so maybe you've heard something like this when it comes to income inequality where it's like the top 1% has like a third of the wealth or something like that but there's a bit of contr I around whether wealth truly follows a Paro distribution or power law distribution or not so the story goes something like this I'll summarize wealth distribution via the mean and standard deviation but of course if wealth is following this power law the mean and standard deviation are going to be useless because these are parameters for a gaussian distribution not so much helpful for a power Lot distribution so someone will say that's useless because wealth follows a power law but then you have someone else that's saying actually wealth fits a log normal distribtion tion better and then you'll have someone else that says Well Log normal behaves like a power log distribution for high Sigma so this kind of summarizes the controversy here and to just avoid this altogether instead of trying to say does some particular data set follow some particular distribution we can instead focus on fat tails this idea of fat tailedness we can Define as the degree to Which rare events Drive the aggregate statistics of the distribution so this Maps directly onto what we were talking about before with mediocris Stan and extremist where in mediocris Stan rare events do not drive the aggregate statistics while in extremist they do to kind of connect this to different distributions we have a sort of map of mediocris and extremist here so on the far left we have the Gan distribution that we all know and love and then more generally we can call these like student te distributions on the right hand side in extremist da we have the power law distributions that we've been discussing but then we have this land in between and we can Define this as the subexponential domain so an example subexponential distribution is the log normal distribution so we can see for low Sigma it kind of looks like a gaussian but for high Sigma it kind of looks like the Paro distribution and we can kind of index different Power lot distributions according to this Alpha parameter so if Alpha is greater than or equal to two the distribution has finite mean and variance which allows us to do some productive statistics with it if the alpha value is between 1 and two it has finite mean but infinite variance so now regression blows up but at least we have a mean we can work with however when the alpha value is below one the mean is infinite and this is what author Nim TB calls the forget about it domain you can't really do much when the power law has a tail as fat as this as you can see the space between mediocris and extremist ston between gaussian distributions and power law distributions is really a spectrum so instead of thinking of this as like a binary thing as like fat tailed or not this is really a quantity that lives on a spectrum from not very fat tailed to very fat tailed while there's no like true way to quantify fat tailedness there are a few heris that we can employ and so here's some ideas the first one is power Latin and we kind of saw this on the right hand side of that image in the previous slide where as the alpha parameter of the power law got smaller and smaller the tail got fatter and fatter so we can use this tail index to kind of quantify how fat the tails are in other words the lower the alpha value the fatter of the Tails and this is kind of demonstrated in this plot here on the other side instead of thinking of it as like power law we can think of it as like non gaussian there are measures for non-gaussianity the most popular being curtosis however the problem with curtosis is that it breaks down when the alpha value is less than or equal to four because it has infinite curtosis another idea is to use the variance of the log normal distribution and this kind of goes from what we saw in the previous slide where for low Sigma log normal distribution looks gaussian but for high Sigma it looks like a power LW so if you have a log normal distribution you can look at the variance to quantify the fat tailedness and then finally TB defines this Kappa metric which generalizes to any type of distribution where lower values have thin Tails or don't have fat tails and large values have fat tails and Kappa has a max value of one so if you want to learn more about that he talks about in reference number six Linked In the description below so that was a ton of information but to try to boil everything down when it comes to data that follows a power law distribution to Fat tailed data the central problem that comes up in practice is insufficient sample size essentially we don't have enough data to truly capture the underlying statistics to cope with this fact I want to leave the data practitioner with a few key takeaways that I like to think about when navigating these types of problems so first and foremost is to plot distributions plot histograms plot PDFs plot cdfs to get an impression of how fat tailed the data might be just kind of visually another takeaway is to ask yourself is this data from mediocris or extremist or somewhere in between maybe turning to some of those heris in the previous slide to try to quantify the fat tailedness another key take away is ask yourself what's the value of a correct prediction but just as importantly what is the cost of an incorrect prediction and then finally if working with fat tailed data don't ignore rare events don't just chop off outliers if 50% of your Revenue comes from 1% of your clients instead of this being something detrimental to your analytics figure out how you can come up with efficient interventions in that 1% to drive even more business and then a couple things I want to call out is if you enjoyed this video and you want to learn more check out the blog published in towards data science Linked In the description below there I cover a bit more details that I may not have covered in the video here all the code to generate the plots that I showed here are available on the GitHub repository linked here and if you enjoyed this content please consider subscribing to the channel that's a great no cost way to support me and the content that I generate and as always thank you so much for your time and thanks for watching"
JgWV1skSpEc,2023-10-25T16:44:10.000000,I Spent $716.46 Talking to Data Scientists on Upwork—Here’s what I learned.,"a few months ago I interviewed 10 top data science Freelancers on upwork and made a video summarizing my key learnings while this might sound like a very expensive way for me to learn I found it to be an unreasonably effective way to accelerate my entrepreneurial Journey it allowed me to fast forward Time by picking up years of hard- learn experiences through just a few hours of conversation this whole experience is summarized well by a quote from Benjamin Franklin who said for the best return on your investment pour your purse into your head to make this a bit more concrete I want to shed more light on some of the upsides that I've realized since the first round of interviews data science skills are highly valued it's not uncommon for experienced Freelancers to charge anywhere from $75 to $150 per hour and even more specialized Freelancers may even charge anywhere from $200 to $300 an hour this is why any tactical tip that can make you a more effective data science freelancer can quickly translate to a tremendous amount of value however all the lessons and knowledge that I gained from these interviews was not the only upside here are four other benefits that were a bit unexpected the first is that I've maintained relationships with many of the Freelancers that I spoke to which has been an enormous resource and support for me as someone who relies on data Consulting as their main source of income second many of the Freelancers I talked to ended up joining a community that I run called the data entrepreneurs and have shared their expertise through community events and workshops third one of the Freelancers I spoke to actually connected me with a Consulting opportunity which generated about $900 in revenue and then fourth and finally the blog that I wrote summarizing my key learnings from the first round of interviews has generated ated $472 in earnings as of making this video so even these lessons and connections and relationships put to the side my first round of conversations generated about $1,400 in Revenue which is more than double what I spend on the calls so that's why it was a super easy decision to get back on upwork and have a second round of interviews with top data science Freelancers however a key difference in the second round of interviews is I went for quality over quantity which basically means I spent more money to talk to less people more specifically I spent about $700 talking to four top Freelancers while many of the takeaways I talked about in my first video were reinforced in the second round of conversation new points were raised and nuances of past takeaways were revealed in this video I'll summarize these key points and nuances and if you find Value in this content please consider subscribing to the channel that's a a great no cost way to support me on this entrepreneurial journey and all the content that I generate from it so one of the key questions I asked in the second round of interviews was what's the number one reason Freelancers fail I find this question helpful because often success isn't just about doing things right but not doing things wrong this brought up a wide range of responses from the Freelancers which I'll summarize through three key points the first point is misalignment one of the the biggest challenges in data freelancing is a poorly defined business problem or project scope this often leads to miscommunications and a lot of times project failures this seem to especially be a risk when working with non-technical clients in other words working with clients that have little to know experience of data science and AI the most common strategy for navigating opportunities with poorly defined business problems is simply p passing on the opportunity while this is definitely a judgment call that depends on the details of the opportunity this does seem to be a common red flag that successful data Freelancers tend to avoid the second point is that Freelancers fail because they commit too early in other words those who are new to freelancing may feel compelled to give a number too early which means they might commit to something before they fully understand what the desired outcome is or before they fully understand what it'll take to get there one freelancer recommended the following line when clients press for a commitment prematurely they would simply say I do not commit to something I cannot do and the third reason that new data Freelancers will fail is because of unrealistic expectations while freelancing in data science comes with Incredible freedom and income opportunities it is not easy especially early on those who are new to freelancing and expect too much too quickly set themselves up for failure in other words they come in with super high expectations which sets them up for disappointment and giving up too early this reinforced a key Insight from the first round of interviews which was new Freelancers should focus more on repetition and reviews than money one of the key takeaways from the first round of interviews was to find a niche niching is powerful because it gives a freelancer services greater Clarity for prospective clients and it allows them to charge premium prices for their specialized expertise however in this second round of interviews some nuances of niching were brought up one freelancer advised the following don't pige and hole yourself into a single Tech stack or solution the more adaptable you are the more valuable you become in a freelance capacity another freelancer shared a similar sentiment and said a diversified Consulting business is more robust what this all boils down to is that niching comes with an inherent risk it works great as long as there's demand for that specific service or expertise however if that specialization becomes irrelevant niching can be catastrophic just ask any former Blockbuster executive while you might be confused now and ask wait Shaw do I Niche do I not Niche what am I supposed to do here's my takeaway from these conversations Niche to differentiate yourself but don't lose sight of other opport unities to expand your Consulting business another key takeaway from the previous video was to form alliances across the whole Tech stack the central reason for this is that data science skills can be limited in their business impact and value in other words it doesn't matter how good your r squar or Au is if you can't deploy Your solution into the real world this is why Freelancers from both round one and round two advise me not to only form alliances across the Tex stack but to learn it for myself and I didn't fully appreciate this point until this second round of interviews and heard it reinforced over and over again part of the reason I didn't fully see this is that learning the full Tex St is a tall order these days the Tex St involves data engineering data analysis data science ml engineering and Beyond which are all their own specializations and I definitely had a bit of apprehension and disbelief that one person could be an expert in everything however one freelancer shared good perspective which changed how I looked at the situation they said you don't need to learn everything you just need to learn enough to containerize your script in other words you don't need to be an expert in everything you just need to know enough to get the job done to make things a bit more concrete that same freelancer shared their specific text act with me which is as follows for most things they used AWS and for architecting the data backend they used RDS which is a way to implement a postgrad database and and S3 buckets for building a data pipeline they use tools like ECS ECR kubernetes airite Docker and R modules for building out computational infrastructure they use terraform for writing data science and data analytics code they would use R and for making web apps they used something called R shiny and while I'm not an R developer seeing this concrete example of a full Tex stack was super helpful to me and made this idea of being a full stack data scientist much more accessible the question that I've asked every single freelancer that I've interviewed is where is this going for those who were interested in scaling up their Consulting business two general paths seem to emerge which mirrored the two general career paths that I saw doing data science at a large Enterprise path one is the manager or leadership route this involves less technical work and more people work on the other side we have path 2 which Which is less people work and more technical work both of these paths can be super rewarding and typically come with increasing compensation what I realize through these conversations is that there are two very similar paths in data freelancing and Entrepreneurship so the freelancer version of path one was embodied by one of the people I interviewed their ultimate goal was to continue scaling their Consulting business into an agency where many consultants served many clients while some level of of technical expertise is required to be successful at this scaling this way relies much more on one's business Acumen managerial experience and communication skills conversely we have the freelancer version of path 2 which was embodied by another person I spoke with they had actually tried path one and realized it wasn't for them their preference was to continue doing the technical work and not having to worry about employees subcontractors and all the fixed costs that are associated with scaling a business business like that while scaling path 1 might seem obvious more clients means more money the way path 2 scales here is one simply increases their hourly rate and returning back to the idea of niching in freelance an interesting observation was the freelancer on path one was much more aligned with not committing to a niche and going after client demands while the freelancer on path 2 had found a strong Niche to operate in which allowed them to charge an hourly rate of $200 an hour however for many Freelancers including myself the long-term goal isn't to scale the Consulting business but rather build a product focused business this was the sentiment shared by the two other Freelancers I spoke to while I've received mixed advice from successful Founders on whether freelancing is an optimal path to product development it does check two important boxes for those trying to launch a product the first is flexibility freelancing allows one to turn up or turn down their workload load to accommodate for product development time the second box is that it generates immediate cash flow in other words freelance is a straightforward way entrepreneurs can translate their skills Into Cash with that being said one freelancer and former founder did warn me that freelancing can easily become a treadmill meaning that one can get so caught up in the cycle of Consulting of marketing closing contracts executing Services Etc that they don't end up building that product and long-term Equity this is why they recommended that I reserve time to take a step back and think strategically about how I spend my time and attention to wrap things up here I want to highlight three key takeaways to add to those from my previous video the first takeaway is that Clarity of scope is the most important thing when assessing a freelance opportunity don't commit to any opportunity until you have Clarity this will help avoid many of the challenges that arise in freelance work and help ensure that that the project provides value to both sides the second key takeaway is to never stop learning this goes for both the technical skills and the non-technical skills data science freelancing is unique because it involves ever evolving technology and perishable skills such as communication and negotiation which makes continual learning a requirement to be successful in this field and the third and final takeaway is to find a niche but always have back doors don't Niche yourself out of a job and don't try to be everything to everyone find that right balance that matches your goals and what the market needs if you got value from this content please consider subscribing to the channel that's a great no cost way of supporting me and the content that I generate if you have any questions or insights of your own as a data freelancer drop those in the comment section below and as always thank you so much for your time and thanks for watching"
Xn_Zw6KSxYU,2023-10-18T16:04:24.000000,I Have 90 Days to Make $10k/mo—Here's my plan,"3 months ago I left my sixf fig data science job to pursue entrepreneurship full-time since I have a video all about why I left that role I won't get into that however here I will talk about how it's going so far and my plan for the next 3 months for how I'm going to get back to my full-time salary but now as an entrepreneur this journey started for me on July 14th 2023 which was my last day as a data scientist at Toyota when I walked out the door I had my SES on three main goals which mapped to the three key pillars of my business Consulting my community called the data entrepreneurs and content my first goal for Consulting was to land one single client that came through an inbound lead so essentially a client found me through a Blog that I wrote or a YouTube video that I made and reached out to me because they wanted to work together on some data science project the second was to get 100 people in attendance across all Q3 events in the data entrepreneurs Community third and finally was to generate $11,000 in one month from my medium blogs and of these three goals I only managed to hit one of them which was Land one client that came through as an inbound lead ironically this was the one that I was most worried about because the relationship between content and Consulting clients is a very unpredictable one while I had always known that content was a great way to generate leads in land clients it was all theoretical until I finally landed that first contract and what surprised me about many of my inbound leads is that they did not come from my most popular content in fact the client that I'm working with currently found me through a Blog that I wrote about topological data analysis which is not only an esoteric data science technique but is not even an approach for using in the project that we're working on together however the most significant takeaway of this for me is seeing this idea of cont content turning into contracts become a reality which gave me a boost of confidence and a signal that I'm on the right path as for the second goal of getting 100 people in attendance across all Q3 events I fell short of this goal while we only had 62 people in attendance I still see this past quarter for the community to be a success one reason for that is this goal of 100 attendees was very much a stretch goal and A good rule of thumb when you're dealing with these types of stretch goals is that even getting 60 to 70% completion can be considered a success this goal accomplished a higher level objective of growing the community a few metrics that reflect that are the Discord Community almost doubled in membership our email and newsletter list almost quadrupled and then our YouTube subscribers increased by 42x so we went from five subscribers on YouTube to about 210 and at this point we're even past that so this failure or success however you want to look at it really inspired ired a new direction for the goals in Q4 which I'll touch on a little later in the video and the final goal of generating $1,000 in earnings in one month for my medium blog turned out to be a bit of a roller coaster ride which I feel is very representative of my life as an entrepreneur medium is a blogging website which I first discovered through technical articles coming from towards data science I've been writing on medium for almost 3 years now putting out content about data science entrepreneurship and other things that I find interesting and if you've been following me on YouTube you probably notice that most of my YouTube videos have an Associated blog with them in June 2023 I made almost $500 writing on medium and the way this works is that anytime a medium subscriber reads one of my blogs medium pays me a portion of their membership fee coming into July 2023 my rationale was since I won't be committing 40 hours of my week to my full-time job I'll have more time to make more blogs on medium so if I double the amount of blogs that I write right I should be able to double my earnings this sounded like a good idea until in August something unexpected happened medium changed their monetization structure which took my monthly earnings from $500 all the way down to $200 even though I was putting out more content on medium and so by the end of August I had basically given up hope that I was going to hit this $11,000 goal but I still move forward with the plan of making twice as much content on medium then something unexpected happened yet again my blog series all about large language models started getting a lot of traction in the final 2 weeks of September and generated about $800 in earnings which brought me $20 Within Reach of my $1,000 goal but of course $980 is less than 1,000 so I technically did not hit this goal my main takeaway from this experience is that entrepreneurship is a wild ride and what I find helpful in the face of all this uncertainty and volatility is to find the work intrinsically rewarding and for me making content isn't about making money while that is a nice upside to it the main benefit that I get from it is that it gives me a way to structure and continue my learning which is super important in a space that is as rapidly evolving as data science and AI That's How The First 3 months went as a full-time entrepreneur now here's my plan for the next 3 months I again set three main goals for the quarter corresponding to each key pillar of my business and if I manage to hit all these goals I'll be in a place where my income will match what I made working full-time as a data scientist which was a little more than $10,000 a month I'll go through these goals one by one and talk more about why that goal is important and how I'm going to achieve it so starting with the first landing 20 hours a week of client work going into q1 the reason that this goal is important is that the overwhelming majority of my Revenue comes through Consulting this is about 90% or more of my business's revenue is from Consulting engagements My Hope Is I can hit this goal of 20 hours a week of work relying solely on inbound leads coming from my YouTube videos blog posts and other content that I put out there however given the unpredictable nature of content this cannot be my only strategy that's why if things aren't looking great halfway through the quarter I'll start an outbound campaign to try to get more clients what this could look like is reaching out to other Freelancers in my network to see if they know of any prospective clients this could be cold emailing or cold dming local businesses and finally applying to contracts on upwork the next goal is to put on 9 to 12 community events and this is about double the number of events that we did in Q3 and the idea is if we double the number of events we'll also double the number of attendees and continue to grow all the different Community channels the reason growing the community is important is because the value of the community scales with the number of members so the more people in the community the more people you can learn from the more people you can collaborate with the more jobs are listed in our job board the more projects are listed in our project board the more people showing up to networking events and all the other amazing things that can happen when you're interacting with like-minded people who are trying to go to the same place as you and while the community doesn't generate any Revenue directly it actually costs me some money and a lot of time it provides me with tremendous value in other ways and hopefully it does the same for other people too three of the biggest benefits I found through running this community are as follows first and foremost is learning you can learn a tremendous amount from someone that's just a few steps ahead of you I've learned a tremendous amount about freelancing and Consulting from data scientists who have been doing this a bit longer than me and on the flip side passing along lessons to those who are a few steps behind you in a sense I find is intrinsically rewarding and helps me solidify my my understanding of these Concepts because a lot of the lessons of Entrepreneurship you're not going to find in the textbook anywhere and I find this type of information is best learned by talking directly to practitioners the second big benefit for me of this community is support in other words sometimes it's great to just have someone to tell you you're not crazy the entrepreneurs journey is definitely not the norm which often leads to entrepreneurs being constantly misunderstood by friends family and others however that one stimulating ation with someone that gets it with someone that is on a similar journey to you makes up for the 10 conversations with people that don't understand what you're doing and the third big benefit is alignment and collaboration in other words when you and the person next to you are going to the same place that alignment naturally generates opportunities for collaboration that takes you both further even faster than you could have gone alone and the third big goal for this quarter is to keep up the content more specifically what that means is posting two blogs a week or two videos a week or one blog in one video a week posting three to five times a week on LinkedIn and posting one to two times a week on Instagram and Tik Tok while content generates some revenue for me so about $1,000 a month at this point the Main Financial upside of making content as a data scientist comes from the inbound leads that it generates for me but of course the leads aren't the main benefit of making content like I said before the key benefit of making content for me is that it gives me a way to structure my learning and keep up with the rapidly evolving space of data science and AI not only does content force me to read articles watch YouTube videos write example code do projects using whatever new technique that I'm learning but it also forces me to synthesize these learnings into a narrative which gives me a tremendous amount of clarity and understanding of these topics there's one last thing that has given me a lot more clarity and a good perspective for looking at my goals for this quarter often times with the uncertainty and volatility of Entrepreneurship things can get hard and it can be easy for me to lose perspective of why am I doing this why don't I just go back to a full-time job where I don't have to worry as much I don't have to stress as much the income is guaranteed why put myself through all this trouble and so what helps me in keeping a positive mindset is looking at all these goals through the lens of learning and giving these two aspects of learning and giving makes it easier to maintain that positive mindset which could be elusive in the ups and downs of Entrepreneurship and so when things get hard and when things get uncomfortable I can just remind myself that it's hard because I'm learning it's hard because it's new and then when I'm putting in a bunch of work for the community or I'm putting in a lot of work for content and maybe it feels like it's not worth it reminding myself that you're helping someone you're helping someone understand a complicated subject you're helping someone level up their data science skills you're giving someone the opportunity to speak to the community about their passions about their expertise you're giving a freelance client access to AI giving them the opportunity to leverage these new technologies in their business and I found that to be kind of like a superpower you know if you're just doing something for yourself it's easy to give up but if you're doing something for other people it makes it easier to stay motivated and engaged in the work that you're doing and so I'm leveraging both of these mindsets learning and giving to help me stay engaged stay productive and stay motivated this quarter if you have any specific questions about my decision- making for transitioning into entrepreneurship or how it's been so far or any questions on why I set my Q4 goals as I did please drop those in the comment section below I'm happy to share anything I've picked up along the way and as always thank you so much for your time and thanks for watching"
ZLbVdvOoTKM,2023-10-05T19:01:23.000000,How to Build an LLM from Scratch | An Overview,"hey everyone I'm Shaw and this is the sixth video in the larger series on how to use large language models in practice in this video I'm going to review key aspects and considerations for building a large language model from scratch if you Googled this topic even just one year ago you'd probably see something very different than we see today building large language models was a very esoteric and specialized activity reserved mainly for Cutting Edge AI research but today if you Google how to build an llm from scratch or should I build a large language model you'll see a much different story with all the excitement surrounding large language models post chat GPT we now have an environment where a lot of businesses and Enterprises and other organizations have an interest in building these models perhaps one of the most notable examples comes from Bloomberg in Bloomberg GPT which is a large language model that was specifically built to handle tasks in the space of Finance however the way I see it building a large language model from scratch is often not necessary for the vast majority of llm use cases using something like prompt engineering or fine-tuning in existing model is going to be much better suited than building a large language model from scratch with that being said it is valuable to better understand what it takes to build one of these models from scratch and when it might make sense to do it before diving into the technical aspects of building a large language model let's do some back the napkin math to get a sense of the financial costs that we're talking about here taking as a baseline llama 2 the relatively recent large language model put out by meta these were the computational costs associated with the 7 billion parameter version and 70 billion parameter versions of the model so you can see for llama 27b it took about 180,000 th000 GPU hours to train that model while for 70b a model 10 times as large it required 10 times as much compute so this required 1.7 million GPU hours so if we just do what physicists love to do we can just take orders of magnitude and based on the Llama 2 numbers we'll say a 10 billion parameter model takes on the order of 100,000 GPU hours to train while 100 billion parameter model takes about a million GPU hours to train so how can we trans at this into a dollar amount here we have two options option one is we can rent the gpus and compute that we need to train our model via any of the big cloud providers out there a Nvidia a100 what was used to train llama 2 is going to be on the order of $1 to $2 per GPU per hour so just doing some simple multiplication here that means the 10 billion parameter model is going to be on the order of1 15 $50,000 just to train and the 100 billion parameter model will be on the order of $1.5 million to train alternatively instead of renting the compute you can always buy the hardware in that case we just have to take into consideration the price of these gpus so let's say an a100 is about $110,000 and you want to form a GPU cluster which is about 1,000 gpus the hardware costs alone are going to be on the order of like $10 million but that's not the only cost when you're running a cluster like this for weeks it consumes a tremendous amount of energy and so you also have to take into account the energy cost so let's say training a 100 billion parameter model consumes about 1,000 megawatt hours of energy and let's just say the price of energy is about $100 per megawatt hour then that means the marginal cost of training a 100 billion parameter model is going to be on the order of $100,000 okay so now that you've realized you probably won't be training a large language model anytime soon or maybe you are I don't know let's dive into the technical aspects of building one of these models I'm going to break the process down into four steps one is data curation two is the model architecture three is training the model at scale and four is evaluating the model okay so starting with data curation I would assert that this is the most important and perhaps most time consuming part of the process and this comes from the basic principle of machine learning of garbage in garbage out put another way the quality of your model is driven by the quality of your data so it's super important that you get the training data right especially if you're going to be investing millions of dollars in this model but this presents a problem large language models require large training data sets and so just to get a sense of this gpt3 was trained on half a trillion tokens llama 2 was trained on two trillion tokens and the more recent Falcon 180b was trained on 3.5 trillion tokens and if you're not familiar with tokens you can check out the previous video in the series where I talk more about what tokens are and why they're important but here we can say that as far as training data go we're talking about a trillion words of text or in other words about a million novels or a billion news articles so we're talking about a tremendous amount of data going through a trillion words of text and ensuring data quality is a tremendous effort and undertaking and so a natural question is where do we even get all this text the most common place is the internet the internet consist of web pages Wikipedia forums books scientific articles code bases you name it post J GPT there's a lot more controversy around this and copyright laws the risk with web scraping yourself is that you might grab data that you're not supposed to grab or you don't have the rights to grab and then using it in a model for potentially commercial use could come back and cause some trouble down the line alternatively there are many public data sets out there one of the most popular is common crawl which is a huge Corpus of text from the internet and then there are some more refined versions such as colossal clean crawled Corpus also called C4 there's also Falcon refined web which was used to train Falcon 180b mentioned on the previous slide another popular data set is the pile which tries to bring together a wide variety of diverse data sources into the training data set which we'll talk a bit more about in the next slide and then we have hugging face which has really emerged as a big player in the generative Ai and large language model space who houses a ton of Open Access Data sources on their platform another place are private data sources so a great example of this is fin pile which was used to train Bloomberg GPD and the key upside of private data sources is you own the rights to it and and it's data that no one else has which can give you a strategic Advantage if you're trying to build a model for some business application or for some other application where there's some competition or environment of other players that are also making their own large language models finally and perhaps the most interesting is using an llm to generate the training data a notable example of this comes from the alpaca model put out by researchers at Stanford and what they did was they trained an llm alpaca using structured text generated by gpt3 this is my cartoon version of it you pass on the prompt make me training data into your large language model and it spits out the training data for you turning to the point of data set diversity that I mentioned briefly with the pile one aspect of a good training data set seems to be data set diversity and the idea here is that a diverse data set translates to to a model that can perform well in a wide variety of tasks essentially it translates into a good general purpose model here I've listed out a few different models and the composition of their training data sets so you can see gpt3 is mainly web pages but also some books you see gopher is also mainly web pages but they got more books and then they also have some code in there llama is mainly web pages but they also have books code and scientific articles and then Palm is mainly built on conversational data but then you see it's trained on web pages books and code how you curate your training data set is going to drive the types of tasks the large language model will be good at and while we're far away from an exact science or theory of this particular data set composition translates to this type of model or like adding an additional 3% code in your trading data set will have this quantifiable outcome in the downstream model while we're far away from that diversity does seem to be an important consideration when making your training data sets another thing that's important to ask ourselves is how do we prepare the data again the quality of our model is driven by the quality of our data so one needs to be thoughtful with the text that they use to generate a large language model and here I'm going to talk about four key data preparation steps the first is quality filtering this is removing text which is not helpful to the large language model this could be just a bunch of random gibberish from some corner of the internet this could be toxic language or hate speech found on some Forum this could be things that are objectively false like 2 + 2al 5 which you'll see in the book 1984 while that text exists out there it is not a true statement there's a really nice paper it's called survey of large language models I think and in that paper they distinguish two types of quality filtering the first is classifier based and this this is where you take a small highquality data set and use it to train a text classification model that allows you to automatically score text as either good or bad low quality or high quality so that precludes the need for a human to read a trillion words of text to assess its quality it can kind of be offloaded to this classifier the other type of approach they Define is heuristic based this is using various rules of thumb to filter the text text this could be removing specific words like explicit text this could be if a word repeats more than two times in a sentence you remove it or using various statistical properties of the text to do the filtering and of course you can do a combination of the two you can use the classifier based method to distill down your data set and then on top of that you can do some heuristics or vice versa you can use heuristics to distill down the data set and then apply your classifier there's no one- siiz fits-all recipe for doing quality filter in rather there's a menu of many different options and approaches that one can take next is D duplication this is removing several instances of the same or very similar text and the reason this is important is that duplicate texts can bias the model and disrupt training namely if you have some web page that exists on two different domains one ends up in the training data set one ends up in the testing data set this causes some trouble trying to get a fair assessment of model performance during training another key step is privacy redaction especially for text grab from the internet it might include sensitive or confidential information it's important to remove this text because if sensitive information makes its way into the training data set it could be inadvertently learned by the language model and be exposed in unexpected ways finally we have the tokenization step which is essentially translating text into numbers and the reason this is important is because neural networks do not understand text directly they understand numbers so anytime you feed something into a neural network it needs to come in numerical form while there are many ways to do this mapping one of the most popular ways is via the bite pair encoding algorithm which essentially takes a corpus of text and deres from it an efficient subword vocabulary it figures out the best choice of subwords or character sequences to define a vocabulary from which the entire Corpus can be represented for example maybe the word efficient gets mapped to a integer and exists in the vocabulary maybe sub with a dash gets mapped to its own integer word gets mapped to its own integer vocab gets mapped to its own integer and UL gets mapped to its own integer so this string of text here efficient subword vocabulary might be translated into five tokens each with their own numerical representation so one two three four five there are python libraries out there that implement this algorithm so you don't have to do it from scratch namely there's the sentence piece python Library there's also the tokenizer library coming from hugging face here the citation numbers and I provide the link in the description and comment section below moving on to step two model architecture so in this step we need to define the architecture of the language model and as far as large language models go Transformers have emerged merged as the state-of-the-art architecture and a Transformer is a neural network architecture that strictly uses attention mechanisms to map inputs to outputs so you might ask what is an attention mechanism and here I Define it as something that learns dependencies between different elements of a sequence based on position and content this is based on the intuition that when you're talking about language the context matters and so let's look at a couple examples so if we see the sentence I hit the base baseball with a bat the appearance of baseball implies that bat is probably a baseball bat and not a nocturnal mammal this is the picture that we have in our minds this is an example of the content of the context of the word bat so bat exists in this larger context of this sentence and the content is the words making up this context the the content of the context drives what word is going to come next and the meaning of this word here but content isn't enough the positioning of these words is also important so to see that consider another example I hit the bat with a baseball now there's a bit more ambiguity of what bat means it could still mean a baseball bat but people don't really hit baseball bats with baseballs they hit baseballs with baseball bats one might reasonably think bad here means the nocturnal mammal and so an attention mechanism captures both these aspects of language more specifically it will use both the content of the sequence and the positions of each element in the sequence to help infer what the next word should be well at first it might seem that Transformers are a constrained in particular architecture we actually have an incredible amount of freedom and choices we can make as developers making a Transformer model so at a high level there are actually three types of Transformers which follows from the two modules that exist in the Transformer architecture namely we have the encoder and decoder so we can have an encoder by itself that can be the architecture we can have a decoder by itself that's another architecture and then we can have the encoder and decoder working together and that's the third type of Transformer so let's take a look at these One By One The encoder only Transformer translates tokens into a semantically mean meaningful representation and these are typically good for Tech classification tasks or if you're just trying to generate a embedding for some text next we have the decoder only Transformer which is similar to an encoder because it translates text into a semantically meaningful internal representation but decoders are trying to predict the next word they're trying to predict future tokens and for this decoders do not allow self attention with future elements which makes it great for text generation tasks and so just to get a bit more intuition of the difference between the encoder self attention mechanism and the decoder self attention mechanism the encoder any part of the sequence can interact with any other part of the sequence if we were to zoom in on the weight matrices that are generating these internal representations in the encoder you'll see that none of the weights are zero on the other hand for a decoder it uses so-called masked self attention so any weights that would connect a token to a token in the future is going to be set to zero it doesn't make sense for the decoder to see into the future if it's trying to predict the future that would kind of be like cheating and then finally we can combine the encoder and decoder together to create another choice of model architecture this was actually the original design of the Transformer model kind of what's depicted here and so what you can do with the encoder decoder model that you can't do with the others is the so-called cross attention so instead of just being restricted to self attention with the encoder or mask self attention with the decoder the encoder decoder model allows for cross attention where the embeddings from the encoder so this will generate a sequence and the internal embeddings of the decoder which will be another sequence will have this attention weight Matrix so that the encoders representations can communicate with the decoder representations and this tends to be good for tasks such as translation which was the original application of this Transformers model while we do have three options to choose from when it comes to making a Transformer the most popular by far is this decoder only architecture where you're only using this part of the Transformer to do the language modeling and this is also called causal language modeling which basically means given a sequence of text you want to predict future text Beyond just this highlevel choice of model architecture there are actually a lot of other design choices and details that one needs to take into consideration first is the use of residual connections which are just Connections in your model architecture that allow intermediate training values to bypass various hidden layers and so to make this more concrete this is from reference number 18 Linked In the description and comment section below what this looks like is you have some input and instead of strictly feeding the input into your hidden layer which is this stack of things here you allow it to go to both the hidden layer and to bypass the hidden layer then you can aggregate the original input and the output of the Hidden layer in some way to generate the input for the next layer and of course there are many different ways one can do this with all the different details that can go into a hidden layer you can have the input and the output of the Hidden layer be added together and then have an activation applied to the addition you can have the input and the output of the Hidden layer be added and then you can do some kind of normalization and then you can add the activation or you can have the original input and the output of the Hidden layer just be added together you really have a tremendous amount of flexibility and design Choice when it comes to these residual Connections in the original Transformers architecture the way they did it was something similar to this where the input bypasses this multiheaded attention layer and is added and normalized with the output of this multi attention layer and then the same thing happens for this layer same thing happens for this layer same thing happens for this layer and same thing happens for this layer next is layer normalization which is rescaling values between layers based on their mean and standard deviation and so when it comes to layer normalization there are two considerations that we can make one is where you normalize so there are generally two options here you can normalize before the layer also called pre-layer normalization or you can normalize after the layer also called post layer normalization another consideration is how you normalize one of the most common ways is via layer norm and this is the equation here this is your input X you subtract the mean of the input and then you divide it by the variance plus some noise term then you multiply it by some gain factor and then you can have some bias term as well an alternative to this is the root mean Square Norm or RMS Norm which is very similar it just doesn't have the mean term in the numerator and then it replaces this denominator with just the RMS while you have a few different options on how you do layer normalization the most common based on that survey of large language models I mentioned earlier reference number eight pre-layer normalization seems to be most common combined with this vanilla layer Norm approach next we have activation functions and these are non-linear functions that we can include in the model which in principle allow it to capture comp Lex mappings between inputs and outputs here there are several common choices for large language models namely gelu relo swish swish Glu G Glu and I'm sure there are more but glus seem to be the most common for large language models another design Choice Is How We Do position embeddings position embeddings capture information about token positions the way that this was done in the original Transformers paper was using these sign and cosine basic functions which added a unique value to each token position to represent its position and you can see in the original Transformers architecture you had your tokenized input and the positional encodings were just added to the tokenized input for both the encoder input and the decoder input more recently there's this idea of relative positional encodings so instead of just adding some fixed positional encoding before the input is passed into the model the idea with relative positional encodings is to bake positional encodings into the attention mechanism and so I won't dive into the details of that here but I will provide this reference self attention with relative position representations also citation number 20 the last consideration that I'll talk about when it comes to model architecture is how big do I make it and the reason this is important is because if a model is too big or train too long it can overfit on the other hand if a model is too small or not trained long enough it can underperform and these are both in the context of the training data and so there's this relationship between the number of parameters the number of computations or training time and the size of the training data set there's a nice paper by Hoffman at all where they do an analysis of optimal compute considerations when it comes to large language models I've just grabbed a table from that paper that summarizes their key findings what this is saying is that a 400 million parameter model should undergo on the order of let's say like 2 to the 19 floating Point operations and have a training data consisting of 8 billion tokens and then a parameter with 1 billion models should have 10 times as many floating Point operations and be trained on 20 billion parameters and so on and so forth my kind of summarization takeaway from this is that you should have about 20 tokens per model mod parameter it's not going to be very precise but might be a good rule of thumb and then we have for every 10x increase in model parameters there's about a 100x increase in floating Point operations so if you're curious about this check out the paper Linked In the description below even if this isn't an optimal approach in all cases it may be a good starting place and rule of thumb for training these models so now we come to step three which is training these models at scale so again the central challenge of these large language models is is their scale when you're training on trillions of tokens and you're talking about billions tens of billions hundreds of billions of parameters there's a lot of computational cost associated with these things and it is basically impossible to train one of these models without employing some computational tricks and techniques to speed up the training process here I'm going to talk about three popular training techniques the first is mixed Precision training which is essentially when you use both 32bit and 16 bit floating Point numbers during model training such that you use the 16bit floating Point numbers whenever possible and 32bit numbers only when you have to more on mixed Precision training in that survey of large language models and then there's also a nice documentation by Nvidia linked below next is this approach of 3D parallelism which is actually the combination of three different parallelization strategies which are all listed here and I'll just go through them one by one first is pipeline parallelism which is Distributing the Transformer layers across multiple gpus and it actually does an additional optimization where it puts adjacent layers on the same GPU to reduce the amount of cross GPU communication that has to take place the next is model parallelism which basically decomposes The Matrix multiplications that make up the model into smaller Matrix multiplies and then distributes those Matrix multiplies across multiple gpus and then and then finally there's data parallelism which distributes training data across multiple gpus but one of the challenges with parallelization is that redundancies start to emerge because model parameters and Optimizer States need to be copied across multiple gpus so you're having some portion of the gpu's precious memory devoted to storing information that's copied in multiple places this is where zero redundancy Optimizer or zero is helpful which essentially reduces data redundancy regarding the optimizer State the gradient and parameter partitioning and so this was just like a surface level survey of these three training techniques these techniques and many more are implemented by the deepe speed python library and of course deep speed isn't the only Library out there there are a few other ones such as colossal AI Alpa and some more which I talk about in the blog associated with this video another consideration when training these massive models is training stability and it turns out there are a few things that we can do to help ensure that the training process goes smoothly the first is checkpointing which takes a snapshot of model artifacts so training can resume from that point this is helpful because let's say you're training loss is going down it's great but then you just have this spike in loss after training for a week and it just blows up training and you don't know what happened checkpointing allows you to go back to when everything was okay and debug what could have gone wrong and maybe make some adjustments to the learning rate or other hyperparameters so that you can try to avoid that spike in the loss function that came up later another strategy is weight Decay which is essentially a regularization strategy that penalizes large parameter values I've seen two ways of doing this one is either by adding a term to the objective function which is like regular regularization regular regularization or changing the parameter update Rule and then finally we have gradient clipping which rescales the gradient of the objective function if it exceeds a pre-specified value so this helps avoid the exploding gradient problem which may blow up your training process and then the last thing I want to talk about when it comes to training are hyperparameters while these aren't specific to large language models my goal here is to just lay out some common choices when it comes to these values so first we have batch size which can be either static or dynamic and if it's static batch sizes are usually pretty big so on the order of like 16 million tokens but it can also be dynamic for example in GPT 3 what they did is they gradually increased the batch size from 32,000 tokens to 3.2 million tokens next we have the learning rate and so this can also be static or dynamic but it seems that Dynamic learning rates are much more common for these models a common strategy seems to go as follows you have a learning rate that increases linearly until reaching some specified maximum value and then it'll reduce via a cosine Decay until the learning rate is about 10% % of its max value next we have the optimizer atom or atom based optimizers are most commonly used for large language models and then finally we have Dropout typical values for Dropout are between 0.2 and 0.5 from the original Dropout paper by Hinton at all finally step four is model evaluation so just cuz you've trained your model and you've spent millions of dollars and weeks of your time if not more it's still not over typically when you have a model in hand that's really just the starting place in many ways next you got to see what this thing actually does how it works in the context of the desired use case the desired application of it this is where model evaluation becomes important for this there are many Benchmark data sets out there here I'm going to restrict the discussion to the open llm leaderboard which is a public llm Benchmark that is continually updated with new models un hugging faces models platform and the four benchmarks that is used in the open El M leaderboard are Arc H swag MML and truthful QA while these are only four of many possible Benchmark data sets the evaluation strategies that we can use for these Benchmark data sets can easily port to other benchmarks so first I want to start with just Arc helis swagen MML U which are multiple choice tasks so a bit more about these Ark and MML U are essentially great school questions on subjects like math math history common knowledge you know whatever and it'll be like a question with a multiple choice response A B C or D so an example is which technology was developed most recently a a cell phone B a microwave c a refrigerator and D an airplane H swag is a little bit different these are specifically questions that computers tend to struggle with so an example of this is in the blog associated with this video which goes like this a woman is outside with a bucket ET and a dog the dog is running around trying to avoid a bath she dot dot dot a rinses the bucket off with soap and blow dries the dog's head B uses a hose to keep it from getting soapy C gets the dog wet then it runs away again D gets into a bathtub with a dog and so this is a very strange question but intuitively humans tend to do very well on these tasks and computers do not so while these are multiple choice tasks and we might think it should be pretty straight forward to evaluate model performance on them there is one hiccup namely these large language models are typically text generation models so they'll take some input text and they'll output more text they're not classifiers they don't generate responses like ABC or D or class one class 2 class 3 class 4 they just generate text completions and so you have to do a little trick to get these large language models to perform multiple choice tasks and this is essentially through prompt templates for example if you have the question which technology was developed most recently instead of just passing in this question and the choices to the large language model and hopefully it figures out to do a BC or D you can use a prompt template like this and additionally prend the prompt template with a few shot examples so the language model will pick up that I should return just a single token that is one of these four tokens here so if you pass this into to the model you'll get a distribution of probabilities for each possible token and what you can do then is just evaluate of all the tens of thousands of tokens that are possible you just pick the four tokens associated with a B C or D and see which one is most likely and you take that to be the predicted answer from the large language model while there is this like extra step of creating a prompt template you can still evaluate a large language model on these multiple choice tasks and in a relatively straightforward way however this is a bit more tricky when you have open-ended tasks such as for truthful QA for truthful QA or other open-ended tasks where there isn't a specific one right answer but rather a wide range of possible right answers there are a few different evaluation strategies we can take the first is human evaluation so a person scores the completion based on some ground truth some guidelines or both while this is the most labor int ensive this may provide the highest quality assessment of model completions another strategy is we could use NLP metrics so this is trying to quantify the completion quality using metrics such as perplexity blue score row score Etc so just using the statistical properties of the completion as a way to quantify its quality while this is a lot less labor intensive it's not always clear what the mapping between a completions statistical properties is to the quality of that that completion and then the third approach which might capture The Best of Both Worlds is to use an auxiliary fine-tuned model to rate the quality of the completions and this was actually used in the truthful QA paper should be reference 30 where they created an auxiliary model called GPT judge which would take model completions and classify it as either truthful or not truthful and then that would help reduce the burden of human evaluation when evaluating model outputs okay so what's next so you've created your large language model from scratch what do you do next often this isn't the end of the story as the name base models might suggest base models are typically a starting point not the final solution they are really just a starting place for you to build something more practical on top of and there are generally two directions here one is via prompt engineering and prompt engineering is just feeding things into the language model and harvesting their completions for some particular use case another Direction one can go is via model fine-tuning which is where you take the pre-trained model and you adapt it for a particular use case prompt engineering and model fine tuning both have their pros and cons to them if you want to learn more check out the previous two videos of this series where I do a deep dive into each of these approaches if you enjoyed this content please consider liking subscribing and sharing it with others if you have any questions or suggestions for future content please drop those in the comment section below and as always thank you so much for your time and thanks for watching"
eC6Hd1hFvos,2023-10-01T20:19:38.000000,Fine-tuning Large Language Models (LLMs) | w/ Example Code,hey everyone I'm Shaw and this is the fifth video in the larger series on how to use large language models in practice in the previous video we talked about prompt engineering which is concerned with using large language models out of the box while prompt engineering is a very powerful approach and can handle a lot of llm use cases in practice for some applications prompt engineering just doesn't cut it and for those cases we can go one step further and fine-tune a existing large language model for a specific use case so Navy's question is what is model fine tuning the way I like to Define it is taking a pre-trained model and training at least one internal model parameter and here I mean the internal weights or biases inside the neural network what this typically looks like is taking a pre-trained existing model like gpt3 and fine-tuning it for a particular use case for example chatgypt to use an analogy here gpt3 is like a raw diamond right out of the earth it's a diamond but it's a bit rough around the edges fine tuning is taking this raw diamond and transforming it into something a bit more practical something that you can put on a diamond ring for example so the process of taking the raw base model of gpt3 and transforming it into the fine-tuned model of gbt 3.5 turbo for example is what gives us applications like chat GPT or any of the other incredible applications of large language models we're seeing these days to get a more concrete sense of the difference between a base model link gpt3 and a fine-tuned model let's look at this particular example we have to keep in mind that these Foundation large language models like gpg3 llama 2 or whatever your favorite large language model is these models are strictly trained to do word prediction given a sequence of words predicting the next word so when you train one of these launch language models on huge Corpus of text and documents and web pages what it essentially becomes is a document completer what that translates to in practice is if you plug into a lot of these base models like gpt3 the prompt tell me how to find tune a model a typical completion might look something like this where it's just listing out questions like you might see in a Google search or maybe like a homework assignment or something here when I prompted gpt3 to tell me how to fine-tune a model the completion was as follows how can I control the complexity of a model how do I know when my model is done how do I test a model well this might be reasonable for gpt3 to do based on the data that it was trained on this isn't something that's very practical now let's look at the fine-tuned model completion so now we have text DaVinci zero zero three which is just one of the many fine-tuned models based on gpt3 coming from open AI we give it the same prompt tell me how to fine tune a model and this is the completion fine-tuning a model involves a adjusting the parameters of a pre-trained model in order to make it better suited for a given task there are generally three steps involved to fine-tuning a model select a base model adjust parameters train the model while this completion may not be perfect it's much more aligned what we were hoping to get out of the language model compared to the base model's completion so if you want to learn more about model fine tuning and how open AI did their fine tuning their alignment tuning and instruction tuning check out the references in the description and comment section below so as we saw when comparing a base model to a fine-tuned model we see that the fine-tune model can generate completions that are much more aligned and desirable for our particular use case Beyond this performance there's actually a deeper reason why you might want to fine tune and that is the observation that a smaller fine-tuned model can often outperform a larger base model this was demonstrated by open AI in their instruct GPT model where they're small 1.3 billion parameter fine tuned instruct GPT model generated to completions that were preferred to gpt3 completions even though gpt3 had about 100 times as many internal parameters this is one of the biggest upsides of fine tuning you don't have to rely on some massive general purpose large language model to have good performance in a particular use case or application now that we have a better understanding of what fine-tuning is and why it's so great let's look at three possible ways one can fine tune an existing large language model the first is via self-supervised learning this is the same way these base models and Foundation large language models are trained so in other words you get your training Corpus of text and you train the model in a self-supervised way in other words you take a sequence of text like listen to your you feed it into the model and you have it predict a completion if we feed in listen to your it might spit out hard what would differentiate fine tuning with self-supervised learning versus just training a base model through self-supervised learning is that you can curate your training Corpus to align with whatever application that you're going to use the fine-tuned model for for example if I wanted to fine-tune gpt3 to write text in the likeness of Me Maybe I would feed it a bunch of my torch data science blogs and then that resulting fine-tuned model might be able to generate completions that are more like my style the second way we can fine tune a model is via supervised learning this is where we have a training data set consisting of inputs and Associated outputs or targets for example if we have a set of question answer pairs such as who was the 35th President of the United States and then the answer is John F Kennedy we can use this question answer pair to fine-tune an existing model to learn how to better answer questions so the reason this might be helpful as we saw before if we were to just feed in who was the 35th President of the United States into a base model the completion that it might generate is who was the 36th president of the United States who was the 40th President of the United States who is the speaker of the house so on and so forth but through having these question answered pairs we can fine tune the model to essentially learn how to answer questions but there's a little trick here these language models are again document completers so we actually have to massage these input output pairs a bit before we can feed it into our large language model for training one simple way we can do this is via prompt templates for example we could generate a template please answer the following question where we input the question here so our input would go here and then we input the target here so the answer would go here and then through this process we can translate our training data set to a set of prompts and generate a training Corpus and then go back to the self-supervised approach and the final way one one can fine tune an existing model is via reinforcement learning while there are many ways one could do this I'm going to just focus on the approach outlined by open Ai and generating their instruct GPT models which consisted of three steps the first was supervised fine tuning so essentially what we were talking about in this second way to fine-tune a model this consists of two steps one curating your training data set and then two fine-tuning the model The Next Step was to train a reward model and all this is is essentially a model that can generate a score for a language model's completion so if it generates a good completion it'll have a high score it generates a bad completion it'll generate a low score so what this looked like for the instruct GPT case was as follows you start with a prompt and you pass it in to your supervised fine-tuned model from here but you don't just do it once you actually do it many times so you generate multiple completions for the same prompt then you get human labelers to rank the responses from worst to best and then you can use that ranking to train the reward model which is indicated by this Square here and then the final step is to do reinforcement learning with your favorite reinforcement learning algorithm in the case of instruct GPT they used proximal policy optimization or PPO for short what this looks like is you take the prompt you pass it in to your supervised fine-tuned model and then you pass that completion to the reward model and then the reward model will essentially give feedback to the fine-tuned model and this is how you can update the model parameters and eventually end up with a model that's fine-tuned even further I know this was a ton of information but if you want to dive deeper into any one of these approaches check out the blog in towards data science where I go into a bit more detail on each of these approaches okay so to keep things relatively simple for the remainder of the video we'll be focused just on the supervised learning approach to model fine tuning here I break that process down into five steps first choose your fine tuning task so this could be text summarization it could be text generation it could be binary classification text classification whatever it is you want to do next you prepare your training data set if you're trying to do text summarization for example you would want to have input output pairs of text and the desired summarization and then you take those input Alpha Pairs and generate a training Corpus using prompt templates for example next you want to choose your base model there are many Foundation large language models out there or there are many existing fine-tuned large language models out there and you can choose either of these as your starting place next we can fine tune the model via supervised learning and then finally we evaluate model performance there's certainly a lot of details into each of these steps but here I'm just going to focus on step number four the fine tuning the model with supervise learning and here I want to talk about three different options we have when it comes to updating the model parameters the first option is to retrain all the parameters given our neural network given our language model we go in and we tweak all the parameters but perhaps obviously this comes with the downside when you're talking about billions tens of billions hundreds of billions of internal model parameters the computational cost for training explodes even if you're doing the most efficient tricks to speed up the training process retraining a billion parameters is going to be expensive another option we can do is transfer learning and this is essentially where we take our language model and instead of retraining all the parameters we freeze most of the parameters and only fine tune the head namely we fine tune the last few layers of the model where the model embeddings or internal representations are translated into the Target or the output layer and while transfer learning is a lot cheaper than retraining all parameters there is still another approach that we can do which is the so-called parameter efficient fine tuning this is where we take our language model and instead of just phrasing a subset of the weights we freeze all of the weights we don't change any internal model parameters instead what we do is we augment the model with additional parameters which are trainable and the reason why this is advantageous is that it turns out that we can fine tune a model with a relatively small set of new parameters as can be seen by this beautiful picture here one of the most popular ways to do this is the so-called low rank adaptation approach or low raw for short like I mentioned in the previous slide this fine tunes a model by adding new trainable parameters here we have a cartoon of a neural network but let's just consider one layer the mapping from these inputs to this hidden layer here we can call our inputs X and then we can call the hidden layer essentially some function of X and to make this a bit more concrete we can write this as an equation h of X is just equal to X we can think of it as a vector to keep things simple and some white Matrix which is just some two-dimensional Matrix to see this a bit more visually we have our weight Matrix which is some d by K Matrix we have X which will just take to be a vector in this case the multiplication of these two things will generate our hidden layer and for the mathesonatos here the spaces that these objects live in and so this is what the situation looks like without Lora where if we're going to do the full parameter fine-tuning what this will look like is all the parameters in this weight Matrix are trainable so here W naught is a d by K Matrix and let's just say d is a thousand K is a thousand this would translate to one million trainable parameters which may not be a big number but when you have a lot of layers this number of triangle parameters can really explode now let's see how low raw can help us reduce the number of trainable parameters again we're just going to look at one of the layers but now we're going to add some additional parameters to the model what that looks like mathematically is we have the W naught times x is equal to H of X like we saw in the previous slide but now we're adding this additional term here which is Delta W Times X this is going to be another weight Matrix the same shape as W naught and looking at this you might think Shaw how does this help us we just doubled the number of parameters yeah sure if we keep W naught Frozen we still have Delta W with the same number of parameters to deal with but let's say that we Define delta W to be the multiplication of two matrices b and a in this case our hidden layer becomes W naught times X Plus ba times x looking at this more visually we have W naught which is the same same weight Matrix we saw in the previous slide but now we have b and a which have far fewer terms than W naught does and then what we can do is through matrix multiplication generate a matrix of the proper size namely Delta W add it to W naught multiply all that by X and generate our h of X looking at the dimensionality of these things W naught and Delta W live in the same space they're matrices of d by K B is going to be a matrix of d by R A is going to be a matrix of R by K and then h of X is going to be D by 1. the key thing here is this R number what the authors of this method called the intrinsic rank of the model the reason that this works and we get the efficiency gains is that this R is a lot smaller than D and K to see how this plays out unlike before where W naught was trainable now these parameters are going to be Frozen and B and a are trainable and maybe as you can just tell visually from the area of this rectangle versus the areas of these two rectangles b and a contain far fewer terms than W naught to make this a bit more concrete let's say d is equal to a thousand K is equal to a thousand and our intrinsic rank is equal to two what this translates to is 4 000 trainable parameters as opposed to the million trainable parameters we saw in the previous slide this is the power of low raw it allows you to fine tune a model with far fewer trainable parameters if you want to learn more about Laura check out the paper Linked In the description below or if you want something that's a bit more accessible check out the blog and towards data science where I talk about this a bit more let's dive into some example code and how we can use low raw to fine tune a large language model here I'm going to use the hugging face ecosystem namely pulling from libraries like data sets Transformers p e f t and evaluate which are all hugging face python libraries also importing in pi torch and numpy for some extra things with our Imports the next step is to choose our base model here I use distill burnt uncased which is a base model available on hugging faces model repository this is what the model card looks like we can see that it only has 67 million parameters in it and then there's a lot more information about it on the model card here we're gonna take distilbert uncased and we're gonna fine tune it to do sentiment analysis we're going to have it take in some text and generate a label of either positive or negative based on the sentiment of the input text so to do that we need to Define some label Maps so here we're just defining that 0 is going to be negative and one is going to mean positive and vice versa that negative means zero and positive means one now we can take these label maps and we can take our model checkpoint and we can plug it into this Nifty Auto model for sequence classification and class available from the Transformers library and very easily we import this base model specifically ready to do binary classification the way this works is that hugging face has all these base models and has many versions of them where they replace the head of the model for many different tasks and we can get a better sense of this from the Transformers documentation as shown here you can see that this Auto model for sequence classification has a lot of Base models that it can build on top of here we're using distilber which is a smaller version of bird here but there are several models you can choose from the reason I went with distillbird is because it only has 67 million parameters and it can actually run on my machine the next step is to load the data set so here I've actually just made the data set available on the hugging face data set repository so you should be able to load it pretty easily it's called IMDb truncated it's a data set of IMDb movie reviews with an Associated positive or negative label if we print the data set it looks something like this there are two parts to it there's this train part and then there's this validation part and then you can see that both the training and validation data sets have 1000 rows in them this is another great thing about model fine tuning is that while training a large language model from scratch may require trillions of tokens or a trillion words in your training Corpus fine-tuning a model requires far fewer examples here we're only going to be using a thousand examples for model fine tuning the next step is to pre-process the data here the most important thing is we need to create a tokenizer if you've been keeping up with this series you know that tokenization is a critical step when working with large language models because learner networks do not understand text they understand numbers and so we need to convert the text that we pass into the large language model into a numerical form so that it can actually understand it so here we can use the auto tokenizer class from Transformers to grab the tokenizer for the particular base model that we're working with next we can create a tokenization function this is a function that defines how we will take each example from our training data set and translate it from text to numbers this will take in examples which is coming from our training data set and you see we're extracting the text so going back to the previous slide you can see that our training data set has two features it has a label and a piece of text so you can imagine each row of this training data set has text and it has a label associated with that text so when we go over here the examples is just like a row from this data set and we're grabbing the text from that example and then what we do is we Define the side that we want to truncate truncation is important because the examples that we pass into the model for training need to be the same length we can either achieve this by truncating long sequences or padding short sequences to like a predetermined fixed length or a combination of the two so we're just choosing the current truncation side to be left and here we're tokenizing the text here's our tokenizer that we defined up here passing in the text we're returning numpy tensors we're doing the truncation and we defined how to do that here and then we're defining our max length and this will return our tokenized inputs since the tokenizer does not have a pad token this is a special token that you can add to a sequence which will essentially be ignored by the large language model here we're adding a pad token and then we're updating the model to handle this additional token that we just created finally we've applied this tokenize function to all the data in our data set using this map method here we have our data set and we plan this map method we pass in our tokenized function and it'll output a tokenized version of our data set to see what the output looks like we have another data set dictionary we have the training and validation data sets but now you see we have have these additional features we don't only have the text in the label but we also have input IDs and we also have this attention mask one other thing we can do at this point is to create a data collator this is essentially something that will dynamically pad examples in a given batch to be as long as the longest sequence in that batch for example if we have four examples in our batch the longest sequence has 500 but the others have shorter ones it'll dynamically pad the shorter sequences to match the longer one and the reason why this is helpful is because if you pad your sequences dynamically like this with a collater it's a lot more computationally efficient than padding all your examples across all 1000 training examples because you might just have one very long sequence at 512 that is creating unnecessary data that you have to process next we want to define a valuation metrics so this is how we will monitor the performance of the model during training so here I just did something simple I'm going to import the accurac see from the evaluate python Library so we can package our evaluation strategy into a function that here I'm going to call compute metrics and so here we're not restricted to just using one evaluation metric or even just using accuracy as an evaluation metric but just to keep things simple here I just stick with accuracy here we take a model output and we unpack it into a prediction and label the predictions here are the logits and so it's going to have two elements one associated with the negative class and one associated with the Positive class and all this is doing is evaluating which element is larger and whichever one is larger is going to become the label so if the zeroth element is larger the ARG Max will return zero and that'll become the model prediction and vice versa if the first element is the largest this will return a one and then that will become the model prediction and then here we just compute accuracy by comparing the model prediction to the ground truth label so before training our find tuned model we can evaluate the performance of the base model out of the box so let's see what that looks like here we're going to generate a list of examples such as it was good not a fan don't recommend the better than the first one this is not worth watching even once and then this one is a pass then what we do is for each piece of text in this list we're gonna tokenize it compute the logits so basically we're going to pass it into the model and take the logits out then we're going to convert the logits to a label either a zero or one and so the output looks like this we have the untrained model predictions it was good the model says this has a negative sentiment not a fan don't recommend the model says this as a negative sentiment so that's correct better than the first one the model says this has a negative sentiment even though that's probably positive this is not worth watching even once model says it's a negative sentiment which is correct and then this one is a pass and the model signs a negative sentiment to them as you can see it got two out of five correctly essentially this model is as good as chance as flipping a coin it's right about half the time which is what we would expect from this unfine-tuned based model so now let's see how we can use Lora to fine-tune this model and hopefully get some better performance the first thing we need to do is Define our low configuration parameters first is the task type we're saying we're going to be doing sequence classification next we Define the intrinsic rank of the trainable weight matrices so that was that smaller number that allowed b and a to have far fewer parameters than just W naught next we Define the lower Alpha value which is essentially a parameter that's like the learning rate when using the atom Optimizer then we Define the low raw Dropout which is just the probability of Dropout and that's where we randomly zero internal parameters during training finally we Define which modules we want to apply low raw to and so here we're only going to apply it to the query layers and then we can use these configuration settings and update our model to get another model but one that is ready to be fine-tuned using low raw and so that's pretty easy we just use this get p e f t model by passing in our original model and then our config from above then we can easily print the number of trainable parameters in our model and we can see it's about a million out of this 67 million that are in the base model so you can see that we're going to be fine-tuning less than two percent of the model parameters so it's just a huge cost savings like 50 times fewer model parameters than if we were to do the full parameter fine tuning next we're going to Define our hyper parameters and training arguments so here we put the learning rate as .001 we put the batch size as four and the number of epochs as 10. next we say where we want the model to be saved here I dynamically create a name so it'll be the model checkpoint Dash low raw text classification learning rates what we defined before batch size is what we put before find weight Decay as 0.01 then we set the evaluation strategy as Epoch so every Epoch is going to compute those evaluation metrics the safe strategy is every Epoch is going to save the model parameters and then load best model at the end so at the end of training it's going to return us the best version of the model then we just plug everything to this trainer class trainer takes in the model and takes in the learning arguments it takes in our training and validation data sets it takes in our tokenizer it takes in our data collator and it takes in our evaluation metrics put that all into this trainer class and then we train the model using this dot train method so during training these metrics will be generated so we can see the epochs the training loss the validation loss and the accuracy so as you can see the training loss is decreasing which is good and the accuracy is increasing which is good but you can see that the validation loss is increasing so this is a sign of overfitting which I'll comment on in a bit here now that we have our fine-tuned model in hand we can evaluate its performance on those same five examples that we evaluated before 5 fine tuning basically same code copy pasted but here's the different output the text it was good is now correctly being classified as positive not a fan don't recommend is correctly classified as negative better than the first one correctly classified as positive and then this is not worth watching even one correctly classified as negative and then this one this one is a pass it's classified as positive but this one's a little tricky even though we don't get perfect performance on these five like baby examples we do see that the model is performing a little bit better and so returning back to the overfitting problem this example is meant to be more instructive than practical in practice before jumping to low raw one thing we might have tried is to Simply do transfer learning to see how close we can get to something that does sentiment analysis well after doing the transfer learning then maybe we would use low Rod to fine tune the model even further either way I hope this example was instructed and gave you an idea of how you can start fine-tuning your very own large language models if you enjoyed this content please consider liking subscribing and sharing it with others if you have any questions or suggestions for future content please feel free to drop those in the comment section below and as always thank you so much for your time and thanks for watching
FEEvnpfD16c,2023-09-30T13:03:33.000000,The REALITY of entrepreneurship. #entrepreneurship #startup #smallbusiness,when people think about entrepreneurship it's typically something like VC funding or Tech startup or scaling the business to the moon but if you look out into the real world the overwhelming majority of businesses do not look like that at all typically what entrepreneurship looks like is a mom-and-pop shop it's a family-owned business it's a small boutique consulting firm it's a e-commerce business someone making wedding cakes out of their kitchen on the weekend in other words entrepreneurship isn't this like highbrow sophisticated super high status thing in fact entrepreneurship is like the lowest status thing because literally anyone can do it but funny enough even though anyone can do it most people don't do it and that's for one simple reason entrepreneurship is very uncomfortable and most reasonable people do not enjoy being uncomfortable but if you're very unreasonable
0cf7vzM_dZ0,2023-09-26T22:38:42.000000,Prompt Engineering: How to Trick AI into Solving Your Problems,hey everyone I'm Shaw and this is the fourth video in the larger series on using large language models in practice today I'm going to be talking about prompt engineering and now before all the technical folks come after me with their pitchforks let's just address the elephant in the room so if you're a technical person like Tony Stark here you might be rolling your eyes at the idea of prompt engineering you might say prompt engineering is not engineering or prompt engineering is way overhyped or even prompt engineering is just a complete waste of time and when I first heard about the concept I had a similar attitude it didn't seem like something worth my time I was more concerned with the model development side of things like how can I fine-tune a large language model but after spending more time with it my perspective on prompt engineering has changed my goal with this dog is to give a sober and practical overview of prompt engineering and the technical people out there who are rolling their eyes like this version of Tony Stark maybe by the end of this you'll be more like this version of Tony Stark oh wow imprompt engineering will be another tool in your AI data science and software development Arsenal so since this is kind of a long session I apologize in advance first I'll talk about what is prompt engineering then I'll talk about two different levels of problem engineering what I call the easy way and the less easy way next we're going to talk about how you can build AI apps with prompt engineering then I'll talk about seven tricks for prompt engineering and then finally we will walk through a concrete example of how to create an automatic grader using Python and Lang chain what is prompt engineering the way I like to Define it is it's any use of an llm out of the box but there's a lot more that can be said about prompt engineering here are a few comments on prompt engineering that have stood out to me the first comes from the paper by white at all which defines prompt engineering as the means by which llms are programmed with prompts and this raises this idea that prompt engineering is a new way to program computers and this was something that was really eye-opening for me when I first saw tragedy PT and heard this idea of prompt engineering it felt like oh this is just like a chat bot kind of thing but as I dove deeper into it and I read this paper and consumed other resources out there the deeper picture here is that large language models provide a path to making programming and computation as easy as asking a computer what you want in natural language another definition comes from the paper by Hugh at all which defines prompt engineering as an empirical art of composing and formatting the prompt to maximize a model's performance on a desired task the reason this one stood out to me is because it highlights this aspect of prompt engineering then it's at this point it's not really a science it's a collection of heuristics and people throwing things against the wall and accidentally stumbling across techniques and then through that messy process it seems like some tricks and heuristics are starting to emerge and this might be part of the reason why people are so put off by prompt engineering because it doesn't seem like a serious science and that's because it's not a serious science it's still way too early in this new paradigm of large language models that we're operating in it's going to take a while for us to understand what these models are actually doing why they actually work and I think with that we'll have a better understanding of how to manipulate them how to throw stuff at them and get desired results out and the final comment that I really liked about prompt engineering comes from Andre carpathy in his state of GPT talk from Microsoft build 2023 where he said language models want to complete documents and so you can trick them into performing tasks just by arranging fake documents I feel like this captures the essence of prompt engineering language models are not explicitly trained to do the vast majority of tasks we ask them to do all these language model want to do is to predict the next token and then predict the next one and the next one and the next one and so I love this concept of tricking the AI into solving your problems and that's essentially all prompt engineering is constructing some text that generates the desired outcome from the large language model and so the way I like to think about it is that there are two levels of prompt engineering the first level is what I call the easy way which is essentially chat GPT or something similar so now Google has barred out there Microsoft has Bing chat all these different applications provide a very user-friendly and intuitive interface for interacting with these large language models and so while this is the easiest and cheapest way to interact with large language models it is a bit restrictive in that you can't really use chat GPT to build an app maybe it'll help you write some code but you can't integrate chat gbt into some piece of software or some larger application that you want to build out that's where the less easy way come comes in the less easy way is to interact with these large language models programmatically and so you could use Python for this you could use JavaScript or whatever programming language the key upside of the less easy way here is that you can fully customize how a large language model fits into a larger piece of software this in many ways unlocks a new paradigm for programming and software development and that brings us to building AI apps with prompt engineering like I just said the less easy way unlocks a new paradigm of software development and to demonstrate this let's just look at a specific use case suppose we wanted to make an automatic grader for a high school history class and while this might be easy enough if the questions are multiple choice or true false this becomes a bit more difficult when the answers are short form or even long form text responses and so an example of this is as follows consider the question who was the 35th president of of the United States well you might think that there's only one answer John F Kennedy there are many answers that are reasonable and could be considered correct and so here's a list of a few examples so there's John F Kennedy but JFK a very common abbreviation of his name could also be considered correct there's also Jack Kennedy which is a common nickname used for JFK there's John Fitzgerald Kennedy which is his full name and someone probably trying to get extra credit and then there's John F Kennedy where the student may have just forgotten to put one of the ends in his last name let's see how we can go about making a piece of software that can do this grading process automatically first we have the traditional Paradigm this is how programming has always been done and here it's on the developer to figure out the logic to handle all the variations and all the edge cases this is the hard part of programming it's like writing a robust piece of software that can handle all the different edge cases so this might require the user to input a list of all possible correct answers and then that might be hard you know with homework with a bunch of questions you can't anticipate every possible answer that a student is going to write down and traditionally if you're trying to evaluate texts against some like Target text you probably would be using some kind of like exact or fuzzy string matching algorithm but now let's look at this new paradigm where we can incorporate large language models into the logic of our software and here you can use an olm to handle all the logic of this automatic grading task using prompt engineering instead of coming in with some code that does exact matching or fuzzy matching and figuring out the logic that gives you the desired outcome you could just write a prompt and so what this might look like is you write the prompt you are a high school history teacher grading homework assignments based on the homework question indicated by q and the correct answer indicated by a your task is to determine whether the student's answer is correct grading is binary therefore student answers can be correct or wrong simple misspellings are okay then we have this template here where we have q and a as indicated by The Prompt and these curly brackets are indicating where a question is going to be placed in and where the single correct answer is going to be placed in and then we also have a place for the student answer all this can be fed to a large language model and the language model will generate a completion that says the student answer is correct or the student answer is wrong and maybe it'll give some reasoning behind why this student answer is wrong taking a step back and comparing these two approaches to this problem approach one was to manually sit down think and write out a string matching algorithm that tried to handle all the different edge cases and variations of potentially correct answers I'm an okay programmer at best so it would probably take me a week or so to get a piece of software that did an okay job at doing that comparing that to how long it took me to write this prompt which is about two minutes think of the time saving here I could have spent a week trying to use string matching to solve this problem or I could have spent a couple minutes writing a prop this is just like the core logic of the application this isn't including all the peripherals the user interfaces the boilerplate code and stuff like that but that's the cost savings we're talking about here we're talking about minutes versus days or weeks of software development and so that's the power of prompt engineering and this kind of new way of thinking of programming and software development so now let's talk about best practices for prompt engineering here I'm going to talk about seven tricks you can use to write better prompts and this is definitely not a complete or comprehensive list this is just a set of tricks that I've extracted from comparing and contrasting a few resources if you want to dive deeper into any one of these tricks check out the blog published and towards data science where I talk more about these tricks and different resources you can refer to to learn more about any of these so just running through this the first trick is to be descriptive even though so in a lot of writing tasks less is more when doing prompt engineering it's kind of the opposite more is better trick twos give examples and so this is the idea of few shot learning you give a few demonstrations of questions and answers for example in your prompt and that tends to improve the llm's performance trick three is to use structured text which we'll see what that looks like later trick four is Chain of Thought which is essentially having the llm think step by step trick five is using chatbot personas so basically assigning a role or expertise to the large language model trick six is this flipped approach where instead of you are asking the large language model questions you prompted to ask you questions so it can extract information from you to generate a more helpful completion finally trick 7 is what I summarize as reflect review and refine which is essentially having the large language model reflect on its past responses and refine them either by improving it or or identifying errors in past responses okay so let's see what this looks like via a demo here I'm going to use Chad GPT and it's important to know what large language model you're using because optimal prompting strategies are dependent on the large language model that you're using Chachi PT is a fine-tuned model so you don't really have to break your back too much on the prompt engineering to get reasonable responses but if you're working with a base model like gpt3 you're going to have to do a lot more work on the prompt engineering side to get useful responses and that's because gpg3 is not a fine-tuned model it only does word prediction while chat GPT is a fine-tuned model it was trained to take instructions and then on top of that they did this reinforcement learning with human feedback to refine those responses even further trick one is to be descriptive so let's compare and contrast an example with and without this trick so let's say I want to use chatgpt to help me write a birthday message for my dad the naive thing to do would be to type been to chat GPT the following prompt write me a birthday message for my dad and so it's gonna do that and so while this might be fine for some use cases I don't write messages that are verbose like this and the response is a bit generic you know like Dad you've been my rock my guide my source of inspiration throughout my life your wisdom kindness and unwavering support has shaped me into the person I am today for that I am eternally grateful oh that's very nice I tend to be a bit more cheeky when it comes to these kinds of birthday messages and whatnot another thing we can do is to employ this trick of being descriptive and getting a good response from chat you PT what that might look like is you type in write me a birthday message for my dad no longer than 200 characters okay so now we don't want it to be as verbose this is a big birthday because he's turning 50 so now we're giving more context to celebrate I booked us a boy's trip to Cancun more context and then be sure to include some cheeky humor he loves that so I'm giving jack gbt more to work with to tailor the response to something closer that I would actually write so let's see what this response looks like okay so it's a lot more concise which I like it says happy 50th dad time to Fiesta like you're 21 again in Cancun cheers to endless Adventures ahead hashtag dad and Cancun that's actually pretty funny maybe I want to use this exactly but I could see it as like a starting point for actually writing a birthday message so the second trick is to give examples let's compare prompts without and with this trick without giving examples we might prompt chat gbt as follows given the title of a torch data Science Blog article write a subtitle for it here we're putting in the title as prompt engineering how to trick AI into solving your problems which is the title of the blog associated with this video and then we leave the subtitle area blank so the completion that it spits out is Unleash the Power of clever prompts for more effective AI problem solving yeah pretty nifty let's see what this looks like if we give a few more examples to try to capture the style of the subtitle that we're looking for and so here the prompt is pretty similar but now I'm putting in the title and subtitle for preceding blogs in this larger Series so here put a practical introduction to llms three levels of using llms in practice then we have cracking open the openai python API a complete beginner friendly introduction with example code and then finally we have the same prompt as we saw before so let's see what it spits out now mastering the art of crafting effective prompts for AI driven Solutions well at face value this might not seem much different than the completion that we saw before I kind of prefer this one over this one here and the only reason is because again I don't like verbose text and this is more concise than this previous one here so I think maybe that's what Chad GPT picked up on it's like oh these subtitles here have these number of tokens let's make sure that the next subtitle has about the same number of tokens just speculating but regardless that's how you can incorporate examples into your prompt the next trick is to use structured text let's see what this looks like in action so I suppose this is our prompt for tragedy BT we don't have any structured text here we're just putting in prompt without structured text so we're asking it to write me a recipe for chocolate chip cookies gives a pretty good response gives us ingredients gives us instructions and gives us some tips if Chachi PT was not fine-tuned it may not have spit out this very neat structure for a chocolate chip cookie recipe and so this is another indication of why what large language model you're working with matters because I could be happy with this response here there may not even be a need to use structured text here but still let's see what this could look like if we did use structured text in our prompt here the prompt is a little different create a well organized recipe for chocolate chip cookies use the following formatting elements the key difference here is we're now asking it specifically to follow this specific format and we're giving it kind of of a description of each section that we want so let's see what this looks like so one subtle difference here is that in the completion where we use structured text you notice that it just kind of gives the title and the ingredients and so on this is something that you could easily just copy paste onto like a web page without any alterations well if we go here there's no title which could be fine but you have this certainly here's a classic chocolate chip cookie recipe for you so now it's trying to be more conversational and may have required some extra steps if this is fitting into a larger like automated pipeline but other than that it doesn't seem like there's much difference between the other aspects of the completion one interesting thing is that here the tips are a bit more clear and bolded well here there's just some like quick bullet points next we have trick four which is Chain of Thought and the basic idea with Chain of Thought is to give the llm time to think and this is achieved by breaking down a complex task into smaller pieces so that it's a bit easier for the large language model to give good completions without using Chain of Thought this is what the prompt might look like write me a LinkedIn post based on the following medium blog and then we just copy paste the medium blog text here through some text in here so it does a pretty good job again this feels way too long for LinkedIn post and it feels like it's just summarizing the text that I threw in there but I mean it's not bad this could be a really good starting place but now let's see what this can look like using Chain of Thought instead of just having it write the LinkedIn post based on the text here I'm trying to explicitly list out my personal process for turning a Blog into a LinkedIn post and trying to get the llm to mimic that so here I put write me a LinkedIn post based on the step-by-step process and medium blog given below so here step one come up with a one line hook relevant to the blog step two extract three key points from the article step three compress each point to less than 50 characters step four combine the hook compress key points from step three and add a con to action to generate the final output and then we put the medium text here okay looking at this this seems a lot more reasonable for a LinkedIn post each line is just one sentence it's not way too much text no one likes reading a wall of text or at least I don't like reading a wall of text so this is much more helpful to me in making a LinkedIn post okay trick five is to use these chatbot personas the idea here is to prompt the llm to take on a particular Persona so let's see a concrete example of this without the trick let's just say we want chat gbt to make me a travel itinerary for a weekend in New York city so it spits out something that looks pretty good so now let's see what this could look like with a Persona so here instead of just asking it straight up for an itinerary I say act as an NYC native and cabbie who knows everything about the city please make me a travel itinerary for a weekend in New York City based on your experience don't forget to include your Charming New York accent in your response okay so let's see what this does comparing this response with the other response there seems to be a lot of overlap and maybe there's not a practical difference between these two but it does feel like there are things here that you don't get here start your day with the classic New York breakfast at a local dinner Cafe well this one will just say start with the bagel Central Park stroll Museum and grab a bagel again yeah it's just eat Bagels every single day oh that's funny I like how it injected a bit of humor here yep you guessed it another Bagel fuel of your final day maybe you really have to like read through these to get a sense of the subtle differences but maybe just from this Bagel example this just gives you two different flavors of itineraries and maybe one matches your interests a bit more than the other trick number six the flipped approach and so here instead of you asking all the questions to the chat bot you prompt the chatbot to ask you questions to better help you with whatever you're trying to do so let's see this without the trick let's say you just want an idea for an llm based application you give it that prompt and it's just gonna generate some idea for you here's generating a idea for us edu bot pros and intelligent educational platform that harnesses the power of llms to offer personalized learning and tutoring experience for students of all ages and levels so this could be a great product idea the problem is maybe this isn't something that you're passionate about or that you really care about or this idea is not tailored to your interests and skill set as someone that that wants to build an app let's see how the flipped approach can help us with this so here instead of asking for an idea just straight up we can say I want you to ask me questions to help me come up with an llm based application idea ask me one question at a time to keep things conversational you can see right off the bat what are your areas of expertise and interest that you'd like to incorporate into your llm based application idea I didn't think to say oh yeah maybe I should tell the chat bot what I know and what I'm interested in so we can better serve me and maybe there are a bunch of other questions that are critical to making a good recommendation on an app idea that I just wouldn't think of and that's where the flip approach is helpful because the chatbot will ask you what it needs to know in order to give a good response and those questions may or may not be something that you can think of all up front the seventh and final trick is reflect review and refine and so this is essentially where we prompt the chat bot to look back at previous responses and evaluate them whether we're asking it for improvements or to to identifying potential mistakes so what this might look like is here we have the edu bot Pro response from before let's see what happens when we prompt it to review the previous response so here I'm saying review your previous response pinpoint areas for enhancement and offer an improved version then explain your reasoning for how you improved the response so I haven't tried this so we're both seeing this for the first time it looks pretty similar but since we asked it to explain how it improved the responses it gave us this extra section here so reasoning for enhancements Clarity and conciseness emphasizing personalization enhanced language and then monetization strategies the monetization section provides more detail on viable strategies okay cool well I'm not going to read through this but this prompt or something like it you can basically copy paste this as needed to potentially improve any chat completion so I know that was a ton of content and I flew through that but if you want to dive into any particular trick a bit more check out the Torches data Science Blog where I talk about each of of these a bit more insight resources where you can learn more everything we have just talked about is applicable to both the easy way and the less easy way of prompt engineering but now I want to focus more on the less easy way and I'm going to try to demonstrate the power of prompt engineering the less Easy Way by building out this automatic greater example we were talking about before using the langchain python Library first as always we're going to do some imports so here we're just importing everything from langchain and then here we're going to be using the openai API so that requires a secret key if you haven't worked with the open AI API before check out the previous video that talks all about that there I talk about what an API is talk about open ai's API and give some example python code of how you can use it here we're just importing our secret key which allows us to make API calls here we're going to make our first chain the main utility of Lang chain is that it provides a ton of boilerplate code that makes it easy to incorporate calls to large language models within your python code or some larger piece of software that you're developing and it does this through these things called chains which is essentially a set of steps which you can modularize into these so-called chains so let's see what that looks like the first thing we need is our chat model so here we're going to incorporate open ai's GPT 3.5 turbo the next thing we need is a prompt template so essentially this is going to be a chunk of text that we can actually pass in inputs and dynamically update with new information so for example this is the same prompt we saw from the previous slide for the automatic grader we'll be able to pass in these arguments question correct answer and student answer into our chain and it'll dynamically update this prompt template send it to the chat bot and get back the response to put this chain together it's super simple the syntax looks like this you have llm chain you define what your llm is which is chat model which is the open AI model we instantiated earlier The Prompt is prompt which is the prompt template we created on the previous slide and you combine it all together into this llm chain and we Define it as chain what this looks like in action is as follows we Define the inputs so here we're going to define the question who was the 35th President of the United States of America we Define the correct answer John F Kennedy and we Define the student's answer FDR and so we can pass all these inputs to the chain as a dictionary so we have this questions correct answer student answer keywords and then we plug in these values that we Define up here and then this is what the large language model spits out students answer is wrong so it correctly grades the student answer as wrong because FDR was not the 35th President of the United States however there's a small problem with our chain right now namely the output from this chain is a piece of text which may or may not fit nicely into our larger data pipeline or software pipe line that we're putting together it might make a lot more sense instead of outputting a piece of text the chain will output like a true or false indicating whether the student's answer was correct or not with that numerical or Boolean output it'll be much easier to process that information with some Downstream task maybe you want to sum up all the correct and incorrect answers of the homework and generate the final grade of the entire worksheet we can do this via output parsers so this is another thing we can include in our chains that will take the output text of the large language model we'll format in a certain way extract some piece of information or convert it into some other format as we'll see here here I'm defining our output parser to determine whether the grade was correct or wrong and I just use a simple piece of logic here I have it returned a Boolean of whether or not the word wrong is in the text completion as an example before the completion was the student answer is wrong so this word wrong appears in the text completion this parser here will return false because wrong is in the completion and so this knot will flip that and it'll make it false so as you can see like we haven't automated all the logic out of programming you still need to have some problem solving skills and programming skills here but then once we have our parser defined we can just add it into our chain like this so we have our llm same as before our prompt template same as before and then we add this output parser which is the grade output parser that we defined right here and then we can apply this chain so let's see what this looks like in for Loop so we have the same question and correct answer as before who's the 35th President of the United States and then the correct answer is John F Kennedy and now we're defining a list of student questions that we may have received which are John F Kennedy JFK FDR John F Kennedy only one n John Kennedy Jack Kennedy Jacqueline Kennedy and Robert F Kennedy also with one end we'll run through this list in a for Loop we'll run our chain just like we did before and we'll print the result and so here we can see that John F Kennedy is true indicating a correct response JFK is true FDR is false John F Kennedy spelled incorrectly is true because we specifically said misspellings are okay John Kennedy is true because we're just dropping the middle initial Jack Kennedy's true it's a common nickname Jacqueline Kennedy is false that was his wife and then Robert F Kennedy is false because that's his brother and as always the code is available at the GitHub repo for this video series which is linked down here feel free to take this code adopt it or maybe just give you some ideas of what's possible with prompt Engineering in this way I would be remiss if I did not talk about the limitations of prompt engineering which are as follows like I said in before optimal prompt strategies are model dependent what is the optimal prompt for chat GPT it's going to be completely different than what's a optimal prompt for gpt3 another downside is that not all pertinent information may fit into the context window because only so much information can be passed into a large language model and if you're talking about a significantly large knowledge base that's not something that prompt engineering may be able to do most effectively another limitation is that typically the models we use to do prompt engineering are these like huge general purpose models and if you're talking about a particular use case this might be cost inefficient or even overkill for the problem you're trying to solve and another version of this is that smaller specialized models can outperform a larger general purpose models an example of this was demonstrated by open AI When comparing their smaller instruct GPT model to a much larger version of gpt3 so this brings up the idea of model fine tuning and that's going to be the topic of the next video in this series there we're going to break down some key fine-tuning Concepts and then I'm going to share some concrete example code of how you can fine tune your very own large language model using the hugging face software ecosystem so I hope this video was helpful to you if you enjoyed it please consider liking subscribing and sharing with others if you have any questions or suggestions for future content please feel free to drop those in the comments section below and as always thank you so much for your time and thanks for watching
LRLH_yIxHrI,2023-08-29T19:25:48.000000,"Why I Quit My $150,000 Data Science Job",I've never wanted to climb the corporate ladder I never wanted to be some cxo or director at a big Corporation you were getting paid well you're doing data science and you were part of a great team why would you ever leave that my goal was just to start a business and to learn am I doing what I want to be doing am I spending my time on what I want to spend my time on you can create your ideal job you can create your ideal life hey everyone I'm Shaw and this video is going to be a little different from what I typically post last month I made a pretty significant life change I decided to leave my full-time data science role and to go all in on entrepreneurship and my business I don't really know how this video is going to turn out I don't have slides like I usually do I don't have a structure I just kind of have my story and the reasons why I made my decision here I'm gonna basically talk about the reasons why I decided to make the transition and hopefully people watching this contemplating a similar decision maybe my story and reasoning gives you some helpful perspective in either deciding to stay in your current role or maybe make a transition a little bit of context so I was working as a data scientist at Toyota Financial Services I was part of a big data science group and it was a fantastic experience because never before had I worked with so many data scientists and Technical people prior to starting at Toyota I was in grad school I was getting my physics PhD and there who I worked with was essentially my research Group which was about 12 people and we're all about at the same level you know we're all grad students trying to figure out all this stuff coming into Toyota where there are many layers of experience and backgrounds it was a very rich team to be a part of and you had so many different people that you could walk up and talk to and get a new perspective ask questions learn from and for me learning is very important to me that's a big reason why I make YouTube videos and write articles about data science and other things so other than just like the broader team the team I worked with was awesome I worked with people I admired which not a lot of people can say and I feel like I grew so much and learned so much from the smaller team that I was working on this is probably making it even more confusing like Shaw you were getting paid well you're doing data science and you were part of a great team why would you ever leave that role that situation and so let's get into that the main reason like super big picture that I decided to leave was working in that role did not align with my long-term goals I've never wanted to climb the corporate ladder I never wanted to be some cxo or some director at a big Corporation ever since early days of grad school my vision for myself has always been to start a business to build a team and I eventually realized it's very difficult to do that part-time people who start businesses it's not uncommon for them to be working 60 80 hours a week trying to make that thing work so now imagine trying to do that while working 40 hours a week in a company and then trying to start this business and build this business in that additional 10 or 20 hours you can work realistically in a week so if we were to just take a step back if your goal is to start a business which of the two strategies is better suited to achieve that goal strategy one working 40 hours a week at a company and working on the business 10 hours a week or option two working 40 hours a week on the business and then trying to get money through gigs and other means to support yourself 10 hours a week I just came to the realization that option two made a lot more sense for me so that's reason number one working in the role no longer made sense given my long-term goals and aspirations the second big reason why I left was the same essentially I was too comfortable like I said before learning and growth is very important to me I think of it like this growth is essential to life so just think of a tree what does a tree do a tree grows and if a tree stops growing what is it doing it's probably dying so the way I see it growth is essential to a meaningful life and the thing about growth is that it's uncomfortable the converse of that is if you're comfortable you're not growing you're not learning there's no stress from the environment that's telling you that hey you need to do something you need to learn something but don't get me wrong when I first started the role it was very uncomfortable there was a lot of new things I had never worked corporate before there were a lot of new Norms new people New Concepts new problems it was a lot of learning early on but what happened was as time went on it started to like asymptote to a level of comfort and I'm not saying that I figured everything out and saw evolved data science or solved corporate or anything like that there's still a ton I could learn about being a data scientist doing financial services but kind of taking that step back again the question I have to ask myself is do I want to be a data scientist doing financial services at a big Corporation is this a skill set that I care to get to that expert level ultimately the answer was no because again my goal wasn't to become the best data scientist ever my goal wasn't to become VP of data science or anything like that my goal was just to start a business and to learn looking at the two options on the table corporate data scientists versus entrepreneur which option better aligns with those goals of starting a business and learning and ultimately it was just option two because taking the leap and trying to figure it out with no guarantee of income seemed a lot more uncomfortable than trying to figure it out making six figures this is a really important point because because if humans are good at anything it's adapting to their environment if your environment is risk-free comfortable having more money than you need in your bank account you're going to adapt to that environment on the other hand if you are very uncomfortable and need to figure out how to generate income to pay your bills and basic necessities you're going to be a lot more motivated in the second scenario this is the lesson that I kind of learned observing my parents who came to the U.S from Iran with very little money and opportunity but they just figured it out and it's funny because I was talking to my mom a while back about how life was when we were growing up it's like how'd you do it my dad was working full-time she was working full-time she was going to school and she was raising two kids I'm like how did you do it and she said I don't know I don't know how I did it and I think that's a testament to what we're capable of doing when we have no other options when we're backed into a corner we unlock block a level of performance that we did not know we were capable of and the best way to bring that out is to put yourself in a situation where you have no other option but to make it work so the third big reason and I'll probably just cut it here is the timing seemed right at work there was a bit of a lull we were transitioning projects and so it was less dependencies on me and the work could be better distributed to other people so it felt like a good time to make the transition because I've worked on a lot of different teams in a lot of different situations and I've observed when someone abruptly leaves and people are left like holding a bag and like scrambling trying to figure it out I mean there's always going to be that to some extent there's never going to be a perfect transition but it felt like good time to make the transition without throwing anyone into a bad situation perhaps more importantly I saw an opportunity so a bit more background in grad school I started doing freelance data science work and that's when I first realized that I don't need to work in a full-time role to make income I'm able to go to upwork apply to jobs get gigs that way that was an important point for me because I knew that even if I didn't have a full-time job I can always make money by going to upwork and picking up gigs another thing I started doing in grad school was making YouTube videos and writing blogs doing that for about three years now and to my surprise I've actually seen some traction and grown a small audience on these platforms an opportunity that this unlocks is that I often get people reaching out to me that want to hire me to do data science Consulting work and so I was incrementally seeing the number of reach outs increasing near the end of my time at Toyota so I knew I could make money on upwork I was seeing more traction of inbound leads coming from my content and on top of that I was generating a little bit of Revenue as part of the the medium partners program and YouTube's partners program and so all these elements together you know the content and the freelance it kind of added up to okay I can survive with the income that I generate through content and freelance work but on top of that since my income at Toyota was way more than my expenses I was able to save a Year's worth of expenses so even if I don't make a single dollar for 12 months I'll still be able to survive all these elements I can always go to upwork I got inbound leads coming in I've got a Year's worth of expenses in savings these all contributed to making going all in on entrepreneurship now seem like the right decision and a good opportunity so I cut the reasons there even though there was more that went into the decision just to recap the three biggest reasons why I decided to make the transition to entrepreneurship was one it was more aligned with my long-term goals to do is seem like a much more effective way to grow and develop the skills that I actually care about and want to develop and three the timing and the opportunity just seemed right so I've been full-time entrepreneur for about a month now you might be wondering am I dead yet what's going on how are things going another big surprise to me is that things are going according to plan this was my sketch of a plan leave and then in that first three months make as much content as possible because through making content I get more inbound leads that's good because it's much better if a client reaches out to you than if you apply to a job on upwork as a freelancer just because well one it's kind of less work you don't have to sift through a bunch of job postings and apply to them and go through interviews and all that kind of stuff but typically when clients reach out to you they're typically a better fit with your skill set and mindset just because I think you can pick up a lot lot about someone through watching them talk in a video or reading an article that they write and the goal with that is by the end of Q3 so right now it's August 2023. the goal is to land one single client that came through inbound seems like that's actually going to happen earlier than that someone that I've been talking to wants to move forward and start working together so that's good that's the first three months and the plan for Q4 is okay try to keep up the content but start prioritizing the freelance work a bit more just so you can maintain Financial Security and all that kind of stuff but ultimately for me the goal isn't to just be a freelancer while freelancing is a business and can be rewarding because you have a lot of freedom on the types of projects you take on who you work with where you work how you work etc My ultimate goal is to build a product and grow a business around that product I think a big reason why that sounds so appealing to me is because it feels like a big Challenge and I want to see if I can do it on some level but also I'm a big believer in the value of a team I truly believe that a good team where everyone's working cohesively and moving in the same direction can accomplish anything I've been part of some amazing teams and it's something that is so satisfying and fulfilling I think that Taps into something very fundamental about being human we are incredibly social creatures so if I'm a solo freelancer I miss out on that team aspect but I feel like if I can build a product growing a business around that product will require a team because I obviously can't do everything and so that's a plan the rough timeline is try to get something within two years right now freelance is just to pay the bills I'm freelancing just so I can survive but the main goal is figure out the product figure out that thing that I can feel build a business around so one month then I definitely do not regret the decision maybe I'll feel differently later but right now things are good everything's going according to plan somehow and it really feels like I'm living my dream life we'll see if I feel the same way in 8 to 12 months where the number and the savings starts getting smaller and smaller but really I think the realization for me is that you don't need to be a millionaire to live your dream life you don't need to wait 20 years to live your dream life you can just Live Your Dream Life now maybe it's not a hundred percent of what you envision but even if it's like 20 I think that's worth pursuing because at every step you can just gain an inch and just slowly incrementally get to your ideal life all that to say that my dream life was a lot closer than I may have thought because it doesn't really take a lot for us to be happy once all the basic needs are met then the next obstacle is am I doing what I want to be doing am I spending my time on what I want to spend my time on while some are lucky enough to work in a job where all the work that they do is exactly what they want to work on and what they want to give their attention to and what they want to dedicate their life to I feel the majority of people do not fit nicely into any existing job that's why I feel entrepreneurship is such a natural thing because you can create your ideal job you can create your ideal life based on your goals and your values if it's really important for you to travel all the time you can build a business that allows you to travel all the time if you want to work very odd hours from like 6 p.m to midnight every single day then you can build a business that allows you to do that I truly believe that you can create a role that is a hundred percent aligned and 100 matches your unique skill skill set Ambitions values goals Etc and it's just a matter of going toward that I feel like there are a lot of people that are in a job maybe they don't like it or they just kind of like it but it's not their ideal role it's not their ideal life that they're living and they have this entrepreneurial mindset they want to start a business but there's so many different things that could be holding them back one of the big ones is just doubt and uncertainty and a motivation for me is okay I'm going to do this and I'm going to document the journey and hopefully people who are on the fence and in a similar situation they'll be like this isn't so crazy this random guy on YouTube did it why can't I do it and that is exactly right why can't you do it you can do it we can all do it anyone can be an entrepreneur anyone can be successful it's just a matter of going after it and putting in the work and being willing to be uncomfortable okay so I'll stop there that was a long rant got an hour of footage here we'll see what it gets edited down to but if you've made it to the end I hope you found some sort of value through this rant and monologue that I just went on do you have a similar story similar Journey I'd love to hear it please drop it in the comments section below and as always thank you so much for your time and thanks for watching
jan07gloaRg,2023-08-10T19:48:23.000000,The Hugging Face Transformers Library | Example Code + Chatbot UI with Gradio,hey everyone I'm Shaw and I'm back with the third video in the series on using large language models in practice in this video I'm going to be breaking down a hugging face Transformers Library which is a python library that makes working with open source large language models super easy I'll start by explaining some key Concepts before diving into some concrete example code and then at the end of the video we're going to see how we can spin up our very own chatbot UI using Transformers and gradio and with that let's get into the video so in the previous video of the series we were talking all about the open AI python API and what this API allows you to do is to programmatically interact with open ai's language models so you can build tools or if you want to build some kind of product or service however one obvious downside is that API calls cost money so in some situations where this cost might be too prohibitive we can turn to open source Solutions one way we can do this is via the hugging face Transformers Library which is what I'm going to talk about today so you're probably wondering what is hugging face well it's more than just an emoji on your phone hugging face is actually an AI company and they've become a major hub for open source machine learning in the past few years so there are three key elements to hugging faces ecosystem which is largely contributed to its recent popularity the first one is its models there are hundreds of thousands of pre-trained Open Source machine learning models freely available on hugging face second is their data sets repository so these are open access data sets that developers and practitioners can grab to train their own machine learning models or to fine-tune existing models and finally is hugging face spaces which is a platform that allows users to build and deploy machine learning applications so while these three aspects of hugging faces ecosystem have made developing machine learning models more accessible than ever before or there's still another key element of the ecosystem worth mentioning which is the Transformers Library so Transformers is a python Library developed by hugging face that makes downloading and training machine learning models super easy so while the library was originally developed specifically for natural language processing its current functionality spans all different domains from computer Vision Audio processing multimodal problems and more so just to give you a flavor of how easy it is to get started with the Transformers Library let's look at a concrete example suppose we want to do sentiment analysis you can imagine that there could be a lot of steps involved in doing sentiment analysis so first you need to find the model that is able to do this classification task then you'll need to take some raw text and convert it into a numerical representation that you can pass into the model and then you need to kind of decode the numerical output of the model to get a label for the text input and so while this might sound like many different steps and could be very complicated this can all be done in one line of code in the Transformers library and so this is possible with the pipeline function as you can see the syntax is super simple here so we have this pipeline function we just need to specify the task we want it to do so here we put sentiment analysis and then we pass to the pipeline function a text input so here I put love this and then if we run that the output of this line of code here is a label of positive and a score associated with that label so this makes sentiment analysis super easy but of course sentiment analysis is not the only thing we can do with the pipeline function you can also do summarization translation question answering feature extraction text generation and many many more if you want the full list it's available on hugging faces documentation this is the link down here and I'll also drop it in the description below going back to this one line example here it almost feels like magic because we didn't give it a model we just said hey do sentiment analysis and apply it to this text here we could have been a bit more explicit here and specified the model that we wanted to use to do sentiment analysis and so to do that it's very simple we just specify a model using the syntax here we're using distillber base uncased fine-tuned SST to English so this is actually the default model in the Transformers library that was used before so that's why we have the same exact output but what really makes Transformers powerful is we could have put any one of the thousands of text classification models available on hugging face and so to explore these models we could have gone to huggingface.co models which is a growing repository of pre-trained Open Source machine learning models for things such as natural language processing computer vision and much more so let's see what this looks like so we'll click on here and I'm going to zoom in a little bit we can see here there are currently 200 184 000 models on the platform and if we look on the left here these are for all different types of tasks they're these categories multimodal computer vision natural language processing audio tabular reinforcement learning and so just now we were doing sentiment analysis which is a type of text classification so we can see what other models we could have used here so just zooming in there are over 28 000 models we could have used for text classification but notice we can add additional filters to narrow down the models we want to pick so let's say we want to make sure we can use the model easily with the Transformers library in that case we can just click on this Transformers filter and then it'll narrow down even more so we went from 28 000 models to about 27 000 models so still a lot of different options we can also go further we can specify the data sets we want it to have been trained on so for example if we wanted to be trained on PubMed because we wanted to use this for a medical use case and you know so on languages licenses so I guess this is important if you have a specific use case in mind for example you want to develop a product for commercial use you want to make sure that the license kind of aligns with what you're trying to do and then there's some other filters as well so let's just say that we're fine with any text classification model that is compatible with the Transformers library then we can kind of explore from here right now it's sorted by trending but we can also sort by number of likes number of downloads recently updated so the number one most trending one looks like it's from the bloke and the model name is llama270b guanaco Q Laura fp16 so we can go ahead and click on that and if we click on that we get the model card just to kind of explore this a bit here's the organization or the individual that created the model this is the model name we can easily copy the model name if we want to paste it into our pipeline function we can do that very easily also we have all the different tags for the model here we have the text classification tag it's available in Transformers which is what we filtered on but also this model is compatible with pytorch this is an important point because the models on hugging phase aren't only for the Transformers Library they are also for many other popular machine learning Frameworks so if you're not using the Transformers library but using pi torch hugging faces models repository can still be a very helpful resource so another cool thing about the model card is that there are these like quick start buttons here so we can get a quick start for like fine-tuning the model on Amazon sagemaker this will give you some code to jumpstart your model fine tuning and let's say you don't want to just do text classification you want to do text generation or token classification this will give you some example code to get you started also there's deploy so you can deploy this model using Amazon sagemaker and then also you can use in Transformers so this is the same syntax we saw in the slides where you're just specifying the task you wanted to do and then just specify model and then coming out of that there's just a bunch of general information about the model laid out here now you can imagine that we have these model cards for hundreds of thousands of models on this platform if you're trying to build some kind of machine learning app it's really never been easier to get started you can just use Transformers to load in any of these state-of-the-art open source language models and just start building from there so hopefully you have a little bit of a sense of what you can do with the Transformers Library your next thought might be like how do I get this on my machine and start using it so the standard way to install the library is via PIV and hugging face has a great guide on their website on how to do this and so I'm not going to walk through the PIP installation steps here however I will walk through a conda installation specifically for the example code that we will see in the following slides there's two steps the first step is head over to the GitHub repository and download the HF Dash EnV yaml file so here's the GitHub this is linked in the description below here we have the HF Dash env.yaml file it's a yaml file listing out all the dependencies for the example code and here we of course have the Transformers Library along with all the other dependencies and so once you download that this next step is to execute the following two commands in your terminal or anaconda command prompt depending on the machine you're using two commands here one is you're going to change directories into wherever you have this HF env.yaml file saved and then you can create a new content environment using this command here so conda envcreate dash dash file HF env.yaml it'll probably take a few minutes to download all the dependencies but once that's done you should be good to go and you shouldn't get any kind of Errors running the following example code so while Transformers does more than just NLP these days and the example code here we're just going to focus on NLP tasks first we'll start with sentiment analysis so before we saw something like this where we use the pipeline function to do sentiment analysis and we specified the model but what's different here is instead of just doing it all in one line we use the pipeline function to create this classifier object and then we can take this object and pass in text to it and it'll generate an output like this so if we pass in the text hate this it'll spit out a label negative and a score associated with that label however you're not limited to just passing one input at a time into this classifier object you can actually pass in a list and it'll do a batch prediction for all the elements in that list so for example we have a text list here so this is great thanks for nothing you've got to work on your face your beautiful never change and so we pass these into the classifier and we get the following labels so it says the first one is positive since the second one is positive even though I sent some sarcasm there the third one is negative and then the fourth one is positive but of course there are more models than just this default one on hugging face that we could have used so one example is Roberta bass go emotions by Sam Lowe so the difference with this model and the default model is that the model here has several Target labels that it uses for text classification and so just using the same exact syntax we can create a classifier object using the pipeline function and then we can apply the classifier object to the first element in our text list defined before and so you can see now instead of just positive or negative there are tons of labels so there's admiration approval neutral excitement gratitude Joy curiosity so on and so forth and actually there are even more but I just cropped the image because there's just too many super easy to do sentiment analysis with Transformers and super easy to swap out this model with any other model you like on the hugging face platform another thing we can do of course is summarization so even though this is a completely different task the syntax is very similar so we use the pipeline function to specify the task and the model we want to use and create this summarizer object then we Define the text that we want as input into this object then we pass it in along with some other input parameters so here we're defining a minimum length the maximum length and then we do a couple things to retrieve just the summary text so this is text from the blog associated with this video it's just talking about hugging face and what it is and it takes this couple paragraphs of text and it reduces it to the following two sentences hugging face is an AI company that has become a major hub for open source machine learning they have three major Elements which allow users to access and share Machine learning resources and of course you can chain together multiple objects so for example you can bring together summarization and sentiment analysis by passing the summarize text into our classifier from before and we can generate all these different outputs and so really these become like Lego blocks and you can just piece together very easily very quickly these different NLP tools for whatever particular use case that you're working on another NLP task we can do is conversational text the syntax is slightly different because in a conversation like with a chatbot there's a bit of back and forth but we started with the same exact syntax so we can use the pipeline function to specify the model that we want to use to create this chat bot object and then we can use this conversation object to handle the back and forth between the user and the chat bot how we do this is we pass in the initial user prompt into this conversation object and then we save it as conversation then we can update this conversation object by passing it into this chat bot object and we can print the result and so what this looks like is the user says hi I'm Shaw how are you and then the chatbot says I'm doing well how are you doing this evening I just got home from work then to keep the conversation going we can use this add user input method so here we're adding a follow-up question where do you work and again we pass the conversation into the chat bot and then Auto magically the chat bot will generate a response and it'll update the conversation object and we can print it all out here so follow up where do you work then the chatbot says I work at a grocery store what about you what do you do for a living and all of this is just running locally on my machine no need for API calls no need for cloud resources these models are small enough that it's just running locally okay so while this is giving us very powerful functionality this is a very awkward way to interact with a chat bot so let's see how we can spin up a user interface to make this a bit more intuitive we can actually do this very easily using a library called gradio in just a few lines of code so first I'm initializing these two lists so one list is to store the user inputs and another list to store the chat bot responses and then we Define a function I call it vanilla chat bot here and it has two inputs it has the message which is essentially the user input and the history which is just the history of everything that's happened in this python script it is automatically generated and so in this vanilla chatbot function we use the same exact syntax we used in the previous slide so we have this conversation object we're passing in the message so this is the user input but also we can pass in the context of the conversation so the chat bot knows what the back and forth has been up until this point the way we do this is we pass in the message list and the response list so all this goes into conversation and we pass the conversation to the chatbot who will generate a response appendix response to the conversation and then we will return the latest generated response from the conversation object and then in basically one line of code we can spin up the chat interface with radio they have this chat interface base object that we can readily use so the first input of this object is the function we just defined the vanilla chat bot and then we can Define some other things like the title of the user interface and then a description for the user interface so all this gets stored into this demo chatbot object and then we simply can just launch it if we do that it'll start running locally at this URL here so if it doesn't automatically pop up in your browser you can just copy paste this into your browser also you can create a public link to this chat interface which is pretty cool you know you can spin this up and then you can send the link to someone and they can actually access the application that you made locally so let's see what this demo looks like here's the chat bot running if you run it in Jupiter lab it'll actually spin up the UI in the notebook or you can just click on this which will open a new tab and then you can start talking with the chatbot so let's just talk to the chat bot gotta response pretty quickly hello how are you doing today I just got home from a long day work so this chatbot for some reason always wants to throw it in our face that it had a long day at work and it seems like it wants me to ask what it does for a living so this one might take a bit more time but we'll see oh Works in a warehouse it's pretty boring but pays the bills how about you there's some other things here you can like have it redo its output you can undo you can clear it it gave a different response that's cool I'm gonna cashier to the grocery store isn't the most exciting job in the world so it doesn't really matter what this chatbot does for a living it just doesn't enjoy its work so that's pretty cool however we can take this one step further and instead of Hosting this chat bot locally we can host this chatbot on huggingface Via hugging face spaces essentially spaces are just git repositories that are hosted by hugging face and they allow you to create machine learning applications and so let's see how we can do this go to space's website you know we'll actually find a lot of existing applications so let's see this is a really popular one so this is the open llm leaderboard so we can see them ranked here with all these different metrics and you can filter as you like so that's pretty cool these are just open source machine learning applications that anyone can access so you can actually look at the source code by going over to files here and then you can see how they did it you can also clone the repository you can run with Docker so it makes it really easy to not only deploy your applications but find applications that are already on Spaces to use as a starting point but going back to spaces if we want to create a new space we just go here click create new space you can create a name we'll call it vanilla chat bot license doesn't matter we'll use radio as our SDK we'll make it public since this is hosting the application it needs computational resources so you have a few different options here you can have gpus or just CPUs of course A lot of these cost money so we'll just stick with the free version for this example we'll hit create space next you'll see something like this you it's giving us some nice instructions on how to upload our application to spaces so first it wants us to clone the repository create our app.pi file so this is where our application is going to go and then pushing our code to the repository and so an important note here is you need to add requirements basically what libraries are necessary to run your application when you do to make the push so let's see what this looks like we can clone the repository just copy paste this into terminal and we got it here so I already have the files ready to go in a different folder called my first face and so you can see that they're there if we look in the repo we can see it has a readme file already so all we need to do is add this app.py file and this requirements.txt file so I'll do that on my other screen we do that again we should see those files there okay so we clone the repo we added our app file and requirements file and now we just commit and push so here OBS decided to stop working so I lost audio but you can see me pushing the code to the git repository now I'm waiting around for the image and app to spin up then it finally did spin up and we see that we have this nice interface completely hosted on hugging face spaces the chatbot indeed does work but it was significantly slower here than it was when I was running it locally and since this is publicly available you can actually go and interact with this very same chat bot using the link on the screen and also in the description below so that's basically it I hope this video and demo has been helpful to you and given you a flavor of what's possible with the hugging face ecosystem while it does seem like we covered a ton of information and content we've only really scratched the surface of what's possible with the hugging face Transformers library and broader ecosystem so with that being said in future videos of this series we will explore more sophisticated use cases of the hugging face Transformers Library such as how to fine tune a pre-trained large language model as well as how to train a language model completely from scratch couple other things I'll call out is that there is a Blog associated with this video published in towards data science they're definitely details in there that I probably left out of this video and of course as always all the code that I covered in this video is available on the GitHub repository linked here and in the description below and if you enjoyed this content please consider liking subscribing and sharing with others and as always thank you so much for your time and thanks for watching
czvVibB2lRA,2023-07-28T15:24:51.000000,The OpenAI (Python) API | Introduction & Example Code,hey everyone I'm sure and I'm back with the second video in the series on using large language models in practice in this video I'm going to talk all about open ai's python API so I'll start with an overview of what an API is and other key Concepts before diving into some example code and showing you how to make your very own chatbot so with that let's get into it like I just mentioned this video is going to be all about open ai's python API and the goal of this is to serve as both a complete and beginner-friendly guide so here's what we're going to talk about today I'm going to start with what's an API then I'm going to move on to specifically open ai's python API I'm going to talk about how to get started setting up an account and other things like that and then finally we'll get into some example code going over both the basics and showing you how to make a chat bot with the API so the million dollar question what is an API I feel like this is just thrown around in so many different contexts said people often be familiar with the term but not so familiar of what it actually means and so an API stands for application programming interface and here I'll Define it as a way to interact with a remote application programmatically so while that might sound very technical and very scary it's not let's just consider the following analogy suppose this is you and you've got a craving for some Salvadorian pupusas that you had during your Latin America trip from last summer however you're back home and you don't know where you can find pupusas in your hometown but lucky for you you have a super foodie friend that knows every single restaurant in town and so if you were trying to figure out where you can go find great pupusas you might send your friend a message hey any good pupusa spots in town and then you know maybe a few minutes later your super foodie friend would respond yes flavors of El Salvador has the best pupusas and so basically what just happened here is you sent your friend a request and then a little bit later your friend sent back a response and while this might sound completely irrelevant to programming in Python and what we're talking about here this is essentially how apis work what this can look like in the context of open ai's python API is instead of texting your friend your request you can send open AI a request using Python and then openai will send back a response which you can capture and use for whatever Downstream task you like for example let's say you want to send a API request to one of openai's language models and if you saw the previous video of the series you saw that a core task of language models is given a string of words to predict the very next word and so let's say we send open AI this string listen to your and we want to get back the very next word the response we may receive could be hard and so as we can see this is very similar to the chat GPT web interface but instead of going to their website typing in a prompt in the UI you're essentially doing the same thing but you're sending over your prompt using Python and this API as opposed to natural language and using their web interface and so you might think what's the upside of doing this way this just sounds like the same thing with more steps while there is a lot of overlap there are unique features in the API that are just not available in the easy user interface that is chat apt one example is a customizable system message this is essentially a message that you can use to set the tone for your chat bot basically set the context give it its motivation its task for chat GPT what this might look like is I am Chad GPT a large language model trained by open AI based on the GPT 3.5 architecture my knowledge is based on information available up until September 2021 today's date is July 13th 2020 23 and so you can imagine that you want to make your own chatbot and you don't necessarily want it to be called chachyvt and you maybe you wanted to do something more specific than this and so you can really set that with the system message some other things are you can adjust input parameters such as the maximum length of the response that is sent back from the language model also the number of responses that you get back so in chagy BT you'll type in a prompt and you'll just get one response but what if you want to get 10 responses and compare them programmatically this is something that would be a bit more time consuming using chat apt as opposed to using Python and then finally you can adjust the temperature which is like the randomness of the response generated by the language model but we'll get more into that later in the video also you can process other types of input not just text so images or other file types you can extract helpful word embeddings for Downstream tasks if you go to chat gbt you input text it'll spit text out but let's say you want a numerical representation of the input text so you can use that for some Downstream processing you can also pass an audio to the API for transcription or translation tasks and then finally there's some model fine-tuning functionality built into the API so even though it may seem like more work to use Python to interact with GPT 3.5 or gpt4 you get all these additional features using the API that just are not available with the chat GPT user interface and then it's not just GPT 3.5 or gbt4 that is available to you there are also a bunch of other models and so here's a quick snapshot of them feel free to pause the video if you actually want to read this this is also available in the blog associated with this video if you want to read more about these models additionally there is a full documentation on open ai's website linked here before getting into how to actually use the API I think one thing at the top of a lot of people's minds when it comes to a paid service is how much is this thing going to cost me and so in order to understand that we first need to understand the concept of a token which here I'll Define as a set of words and characters represented by a set of numbers so suppose we have the text the end with a period at the end while you and I as humans can read this and understand it unfortunately a language model or more specifically a neural network does not understand this text directly language models understand numbers so we need to translate this text into a numerical representation so that the language model can actually understand it and do something with it so let's say we translate the end into this list of integers so what we did is we took text and we translated it into numbers but what do these numbers actually mean and so let's say the relationship between the text and the integers here is shown by this mapping here so the word the is represented by 73 a space followed by the word end is represented by the number 102 and then the the period is represented by a six these three things that make up this text are tokens obviously these won't be the only tokens we have we'll actually have a huge set of these words and characters and their corresponding numerical representations and this will make up the set of all our tokens and the vocabulary that we're using and so the reason this is relevant to pricing is that pricing with openai's API is based on the number of tokens sent to the API and the number of tokens returned so that means bigger prompts and bigger responses will have larger costs additionally there are different costs per token associated with different models and you can read more about that on openai's website linked here okay so now let's start to see how we can actually use the API to do something but before we get into the example code we first have to do a few steps to get our account set up and get into a place where we can actually start making API calls and so there are four steps here first we need to make an account second we need to add a payment method third we can set usage limits and then finally we need to get our secret key to actually make the API calls and so for this I'm going to switch over to open ai's website which is here so the URL is platform.openai.com overview and the first thing we want to do is make an account the way to do that super simple go up to this top right corner click the sign up button and then you can create an account using your email address or continue with any of these continue with Google Microsoft or apple if you've used Chad gbt in the past odds are you already have an openai account and if that's the case you can just hit this log in button here and since I already have an open AI account I'll just go ahead and log into mine here we go now that you've made your account or that you're logged in now we can go to the next step which is to add a payment method how to do that is you can click on your profile in the top right corner and then you can click on manage account to add a payment method go to this billing tab here and then then there's this thing called payment methods so you can click on that and add a payment method pretty easily the next step is to set usage limits so this is something I recommend this will help you set hard and soft limits to how many API calls you can make this will just ensure that you don't spend any more money than you expect on API calls there are two options here there's a hard limit and a soft limit the hard limit basically once you hit that number so in my case I put it as five dollars once I hit five dollars any additional API call will get rejected so you will not be charged more than this number here and then there's also a soft limit and basically when you hit the soft limit openai will send you an email hey you've hit your soft limit threshold and so this kind of helps you make sure you're not going off the rails with API calls for a little context I spend a few days playing around with the API writing example code for this video and so on and I spent a whopping three cents so the API calls are pretty cheap unless you're deploying a product where you have many users making API calls on a specific account that's when this number will probably go up a lot but I would imagine for personal use you probably will not hit more than a dollar if you're not really using this API a whole lot and then finally you need to get your secret key and so for that click on this API Keys thing and you'll see a screen like this if you've never done this before you won't see any API Keys here so you need to create a new one and the way to do that is really easy you just click this you can say new secret key I'll give it whatever name you want then you can click this create secret key button and then it's going to create this secret key for you it's important that you copy this and save it somewhere safe and accessible to you because this is the only time you're going to get access to this key just showing that if I hit done the key I just made it just shows me the last four digits of it so I don't know anything else in between so if I didn't copy it then I wouldn't be able to use it so now that we've made our account added a payment method set usage limits and obtained a secret key and we can now finally start start coding so starting with the basics we need to import the open AI python Library if you haven't installed this super simple just pip install open Ai and it should download it automatically in your terminal this line here I'm importing my secret key so what I did is I created a python file called SK dot pi and in there I just Define one variable which is called my SK and I set that equal to my secret key we can see what that looks like here so I have this Jupiter notebook and then I have this SK dot Pi if we open that we have this python file with just one variable my SK and it's equal to a string and so here is where you will copy paste your string I copied it from earlier so I'll just throw it there so you you should never do this you shouldn't just make your secret key visible to people on a public medium like YouTube but I'm just going to revoke this key right after so you won't get the chance to use my five dollars of API credits so this is just like a clean way to do it you just import the key and then you can copy paste whatever key you want in here you can revoke access create a new one paste it here without ever having to change your code and then you can set what API key you want to use using the syntax here so openai.api key you just set it to my secret key alternatively if you don't want to go through all this I've created another file and import the secret key like this you can always just manually paste your secret key right here so we got that set up and now we can make our first API call the way to do this is really simple so we have this chat completion module and then there's this method called create so here we are just passing in two inputs we pass into the model we want to use and when you pass in the messages taking a closer look at this messages thing here messages is going to be a list of dictionaries here we have a list with only one element so this element is a dictionary and then the dictionary has two key value pairs so the first one is we define the role of whoever is saying this message so here here we say user alternatively you could put system to define the system message or you could put assistant to Define some text generated by the chatbot then you define the content so this is essentially what that role said and so here keeping with the listen to your example we're going to pass that in as content from the user essentially this is our request to the API it is all baked into this method here what we get returned back is actually the response from the API and so this will be in a Json format which is essentially like a dictionary so we can print it and we see that it has a bunch of different fields in it there's this ID object created model choices usage so on and so forth most of these are not important to what we're talking about here if you want to know what each of these mean I have a definition for each of these in the blog associated with this video published in towards data science but the only important field for the discussion here is this choices feel because that is what is going to hold the response from the language model in this case GPT 3.5 turbo if we take a closer look we can see that choices will in of itself have an index field and a message field and then the message field will have role and content fields in them basically we just want to extract the content and to do that we can use the syntax here chat completion is this dictionary like object here we can access the choices which is this first field here we can get the First Choice essentially so index 0 then we can access the message of this first choice so now we're here and then finally we can grab the content which is this line here it is very hierarchical but that kind of helps keep all this information very organized so that was our very first API call but this wasn't anything special this is equivalent to going to chat GPT and just typing in listen to your this doesn't make use of a lot of the other features of the API so let's see what else we can do so one is we can set the input parameters Max tokens which will adjust the maximum number of tokens that are allowed in the response from the language model let's say I just want a one word response so what I can do is have the same exact API request but I'll add this Max tokens parameter and set it equal to one if we do that and print it out we actually get the same exact response but notice there's no period so what that tells us is heart is actually a token in the vocabulary of GPT 3.5 turbo and the period is another token let's see what else we can do next we can set this n parameter which controls the number of responses sent back from the language model so in this case we have essentially the same API request as before now we're setting the max tokens equal to two and then we're sending n equal to 5 and so what this will do is return back five chat completion for us we can get those back and we can print them in this for Loop here and then we can see we get heart heart and heart again a new line character I'm guessing then heart comma a new line character Heart comma and so notice these aren't all the same thing and this could be a good thing or a bad thing depending on the use case for example if you're trying to create some AI tool that'll help you in some creative tasks you know maybe you do want these to be a little random and out there on the other side maybe you're automating some business process or standard process and you want it to be very deterministic you want the output to be very predictable and identical every single time it's called and so it turns out we can control this degree of Randomness or predictability using the temperature parameter temperature is this input parameter that essentially controls the randomness of the response and this will take values between 0 and 2. so a temperature value of zero will make the responses very deterministic so very predictable while on the Other Extreme a temperature equal to 2 will make the responses very random and out there to see what this looks like we have the same exact API call from the previous slide but now we're setting the temperature equal to zero so we're making this very predictable and deterministic and lo and behold we get the same exact thing for all five responses heart period heart period heart period basically this is the most probable two tokens that will follow the text listen to your but let's see what happens when we crank the temperature up so let's set temperature equal to two same exact API call and now we get some really interesting chat completions first we get listen to your judgment make sense listen to your advice okay that's good listen to your dot inner awareness interesting listen to your heart so notice we'd still got this most probable outcome so raising the temperature doesn't necessarily mean you get low probability and why old responses it just means that those low probability responses have a higher likelihood of actually showing up in your chat completion and then finally we have gingist which I don't know what that means so listen to your gangist okay so those were the basics of open ai's python API now let's see what a more advanced example could look like the code that we just went through and the code that we're about to go through is all available on the GitHub repository and so this is essentially a code we just ran through and here's the setup our first API call the max tokens the number of chat completions the temperature stuff and now into the demo let's say for whatever reason we wanted to create a lyric completionist system this could be some service where people know the first line of a song but don't remember the lyrics for the rest of the song so they'll just put in the first line and then this lyric completion assistant will fill it in or maybe they're writing their own music and they are kind of getting stumped on what the next line in the lyrics should be so they can use this lyric completion assistant to help them do that creative task with this chat completion API making a custom chatbot is like stupid easy basically all we have to do is Define these messages that will precede the conversation that the user will have with the assistant so there are a couple ways we can do this one we can use the system message to set the context for the chat bot or we can have some example back and forth between the user and the assistant so the chatbot gets an idea of how to respond to user requests or of course we can do both which is what we're doing in this example here we set the system message as I am Roxette lyric completion assistant when given a line from a song I will provide the next line in the song so that's the system message and then we have two example back and forth between the user and the assistant so the user will say I know there's something in the wake of Your Smile the assistant will say I get a notion from the look in your eyes yeah the user will then say you've built a love but that love falls apart and then the assistant says your little piece of heaven turns too dark then we'll will pass in one last message which is the same message we've used throughout this entire demo which is listen to your and then we can see what the chat bot spits out so this code is essentially doing what we did before you know we're making the API call using GPT 3.5 turbo instead of a list with just one dictionary we have this messages list which actually has five dictionaries in it here we set the number of responses to one and we set the temperature to zero so it becomes very predictable here's where a little magic is happening so we're going to print the response from the chatbot but then we're gonna take the response from the chatbot and we're gonna append it to the end of the messages list which will then get fed right back into the chat bot so essentially the chatbot will just loop on as long as we want going off its previous responses and so here's what the output of all that looks like so we passed and listened to your and then the chatbot said heart when he's calling for you listen to your heart there's is nothing else you can do I don't know where you're going and I don't know why but listen to your heart before I tell him goodbye these are powerful lyrics from this chat bot and they actually correspond exactly to the hit Roxette song listen to your heart and whose actual lyrics we can see here but here's a little bonus so let's see what happens when we crank the temperature you know what if The Roxette song went a little differently so here we put temperature equal to zero so we have listened to your I reach into the Shadows summon sweet Elaine pointing all steel values fails if friends remote empty reply new line character image existing long seconds confirm flesh pressed secretly remember Saint talk dying to unfamiliar pieces father blessed speech keeps passing shape raises you travel feeling Shadows thriven body swept Spirit consume so not entirely sure what that means so this could be some you know new genre of Art postmodern techno chat bot AI art or it could just be meaningless gibberish so you can't really tell with this kind of stuff so again example code is available on the GitHub repository shown here and linked down in the description below and so what's next in this video series you know one obvious downside of using open ai's API is that you have to pay for these API calls which if you're building some product or service it may not scale well because if you have thousands tens of thousands or more users these costs can start adding up pretty quickly so in these instances we can turn to open source Solutions where you don't have to pay for API calls you have some other way of Hosting these language models so this is where the hugging face Transformers Library can help hugging face is a AI community that is building the future this is just a screenshot from their website which is also listed here hugging face maintains this python Library called Transformers which essentially makes downloading and training powerful pre-trained language models really easy and so in the next video this series I'm going to be talking all about that and sharing example code and so if you enjoyed this video please consider liking subscribing and sharing with others if you have any questions about large language models or the openai API please feel free to drop those in the comment section below this whole series is part of my Learning Journey and process so I do find a lot of value in the questions that I receive because it prompts me to dive more deeply into the stuff which I really enjoy and as always thank you so much for your time and thanks for watching
4oUOJ37GKYE,2023-07-24T13:50:51.000000,"The more they hurt you, the stronger you get #antifragile",story is the story of the Hydra monster which is the monster that Hercules fought and whenever he would chop off one of its heads three more would grow in its place the Hydra is anti-fragile because the more you harm it the stronger it gets
tFHeUSJAYbE,2023-07-22T14:45:18.000000,A Practical Introduction to Large Language Models (LLMs),everyone I'm Shah and I'm back with a new data science Series in this new series I'm going to be talking about large language models and how to use them in practice in this video I will give a beginner friendly introduction to large language models and describe three levels of working with them in practice future videos in this series will discuss various practical aspects of large language models things like using open ai's python API using open source Solutions like the hugging face Transformers Library how to fine-tune large language models and of course how to build a large language model from scratch if you enjoyed this content please be sure to like subscribe and share with others and if you have any suggestions for me to include in this series please share those in the comments section below and so with that let's get into the video so to kick off the video series in this video I'm going to be giving a practical introduction to large language models and this is meant to be very beginner friendly and high level and I'll leave more technical details and example code for future videos and blogs in this series so a natural place to start is what is a large language model or llm for short so I'm sure most people are familiar with chat GPT however if you are enlightened enough to not keep up with new cycles and Tech hype and all this kind of stuff chat GPT is essentially a very impressive and advanced chat bot so if you go to the chat GPT website you can ask it questions like what's a large language model and it will generate a response very quickly like the one that we are seeing here and that is really impressive like if you were ever on AOL Instant Messenger also called aim you know back in early 2000s or in the early days of the internet there were chat Bots then there have been chat Bots for a long time but this one feels different like the text is very impressive and it almost feels human-like a question you might have when you hear the term large language model is what makes it large what's the difference between a large language model and a not large language model and this was exactly the question I had when I first heard the term and so one way we can put it is that large language models are a special type of language model but what makes them so special and I'm sure there's a lot that can be said about large language models but to keep things simple I'm going to talk about two distinguishing properties the first quantitative and the second qualitative so first quantitatively large language models are large they have many many more model parameters than past language models and so these days this is anywhere from tens to hundreds of billions of parameters the model parameters are numbers that Define how the model will take an input and generate the output so it's essentially the numbers that Define the model itself okay so that's a quantitative perspective of what distinguishes large language models from not large language models but there's also this qualitative perspective and these so-called emergent properties that start to show up when Lang language models become large and so emergent properties is the language used in this paper cited below a survey of large language models available in the archive really great beginner's guide I recommend it but essentially what this term means is there are properties in large language models that do not appear in smaller language models and so one example of this is zero shot learning one definition of zero shot learning is the capability of a machine learning model to complete a task it was not explicitly trained to do so while this may not sound super impressive to us very smart and sophisticated humans this is actually a major innovation in how these state-of-the-art machine learning models are developed so to see this we can compare the old state-of-the-art Paradigm to this new state-of-the-art paradigm the old way and not too long ago we can say like about five ten years ago the way the high performing best machine learning models were developed was strictly through supervised learning what this would typically look like is you would train a model on thousands if not millions of labeled examples and so what this might have looked like is you have some input text like hello Ola how's it going nastabian so on and so forth and you take all these examples and you manually assign a label to each example here we're labeling the language so English Spanish so on and so you can imagine that this would take a tremendous amount of human effort to get thousands if not millions of high quality examples so let's compare this to the more recent Innovation with large language models who use a different Paradigm they use so-called self-supervised learning so what that looks like in the context of large language models is you train a very large model on a very large Corpus of data and so what this can look like is if you're trying to build a model that can do language classification instead of painstakingly generating this labeled data set you can just take a corpus of English text and a corpus of Spanish text and train a model in a self-supervised way so in contrast to supervised learning self-supervised learning does not require manual labeling of each example in your data set the so-called labels or targets for the model are actually defined from the inherent structure of the data or in this context of the text so you might be thinking to yourself how does this self-supervised learning actually work and so one of the most popular ways that this is done is the next word prediction Paradigm so suppose we have this text listen to your and we want to predict what the next word would be but clearly there's not just one word that can go after the string of words there are actually many words you can put after this text and it would make sense in this next word prediction Paradigm what the language model is trying to do is to predict the probability distribution of the neck next word given the previous words what this might look like is listen to your heart might be the most probable next word but another likely word could be gut or listen to your body or listen to your parents and listen to your grandma and so this is essentially the core task that these large language models are trained to do and the way the large language model will learn these probabilities is that it'll see so many examples in this massive Corpus of text that is trained on and it has a massive number of internal parameters so it can efficiently represent all the different statistical associations with different words and an important Point here is that context matters if we simply added the word don't to the front of this string here and it changed it to don't listen to your then this probability distribution could look entirely different because just by adding one word before this sentence we completely change the meaning of the sentence and so to put this a bit more mathematically and I promise this is the most technical thing in this video this is an example of a auto regression task so Auto meaning self regression meaning you're trying to predict something so what this notation means is what is the probability of the nth text or more technically the nth token given the preceding M token so n minus 1 and minus two and minus three and so on and so forth and so if you really want to boil everything down this is the core task most large language models are doing and somehow through this very simple task of predict the next word we get this incredible performance from tools like chat GPT and other large language models so now with that Foundation said hopefully you have a decent understanding of what large language models are and how they work and a broader context for them now let's talk about how we can use these in practice here I will talk about three levels in which we can use large language models these three levels are ordered by the technical expertise and computational resources required the most accessible way to use large language models is prompt engineering next we have model fine tuning and then finally we have build your own large language model so starting from level one prompt engineering here I have a pretty broad definition of prompt engineering here I Define it as just using an llm out of the box so more specifically not touching any of the model parameters so of these tens of billions or hundreds of billions of parameters that Define the model we're not going to touch any of them we're just going to leave them as is here I'll talk about two ways we can do this one is the easy way and I'm sure is the way that most people in the world have interacted with large language models which is using things like chat GPT these are like intuitive user interfaces they don't require any code and they're completely free anyone can just go to the Chad gbt website type in a prompt and it'll spit out a response so while this is definitely the easiest way to do it it is a bit restrictive in that you have to go to their website this doesn't really scale well if you're trying to build a product or service around it but for a lot of use cases this is actually super helpful so for applications where the easy way doesn't cut it there is the less easy way which is using things like the open AI API or the hugging phase Transformers library and these tools provide ways to interact with large language models programmatically so essentially using python in The Case of the openai API instead of typing your request in the chat GPT user interface you can send it over to openai using Python and their API and then you will get a response back of course their API is not free so you have to pay per API call another way we can do this is via open source Solutions one of which is the hugging phase Transformers Library which gives you easy access to open source large language models so it's free and you can run these models locally so no need to send your potentially proprietary or confidential information to a third party and open AI so future videos of the series we'll dive into all these different aspects I'll talk about the openai API what it is how it works share example code I'll dive into the hugging face Transformers Library same situation what the heck is it how does it work and then sharing some python example code there I'll also do a video talking about prompt engineering more generally how can we create prompts to get good responses from large language models and so while prompt engineering is the most accessible way to work with large language models just working with a model out of the box may give you sub-optimal performance on a specific task or use case or the model has really good performance but it's massive it has like a hundred billion parameters so question might be is there a way we can use a smaller model but kind of tweak it in a way to have good performance on our very narrow and specific use case and so this brings us to level two which is model fine tuning which here I Define as adjusting at least one internal model parameter for a particular task and so here there are just generally two steps one you get a pre-trained large language model maybe from open AI or maybe an open source model from the hugging phase Transformers library and then you update the model parameters given task specific examples kind of going back to the supervised learning versus self-supervised learning the pre-trained model is going to be a self-supervised model so it will be trained on this simple word prediction task but in step two here's where we're going to do supervised learning or even reinforcement learning to tweak the model parameters for a specific use case and so this turns out to work very well models like Chachi BT you're not working with the raw pre-trained model the model that you are interacting with in chat GPT is actually a fine-tuned model developed using reinforcement learning and so a reason why this might work is that in doing this self-supervised task and doing the word prediction the base model this pre-trained large language model is learning useful representations for a wide variety of tasks so in a future video I will dive in more deeply into fine tuning techniques popular one is low rank adaptation or low raw for short and then another popular one is reinforcement learning with human feedback or rlhf and of course there is a third step here you'll deploy your fine-tuned large language model to do some kind of service or you know use it in your day-to-day life and you'll profit somehow and so my sense is between prompt engineering and model fine tuning you can probably handle 99 of large language model use cases and applications however if you're a large organization large Enterprise and security is a big concern so you don't want to use open source models or you don't want to send data to a third party via an API and maybe you want your large language model to be very good at a relatively specific set of tasks you want to customize the training data in a very specific way and you want to own all the rides have it for commercial use all this kind of stuff then it can make sense to go one step further Beyond monofine tuning and build your own large language model and so here I Define it as just coming up with all the model parameters so I'll just talk about how to do this at a very high level here and I'll leave technical details for a future video in the series first we need to get our data and so what this might look like is you'll get a book Corpus a Wikipedia Corpus and a python Corpus and so this is billions of tokens of text and then you will take that and pre-process it refine it into your training data set and then you can take the training data set and do the model training through self-supervised learning and then out of that comes the pre-trained large language model so you can take this as your starting point for level two and go from there and so if you enjoyed this video and you want to read more be sure to check out the blog in towards data science there I share some more details that I may have missed in this video this series is both a video and blog Series so each video will have an Associated blog and there will also be tons of example code on the GitHub repository story The goal of the series is to really just make information about large language models much more accessible I really do think this is the technological innovation of our time and there's so many opportunities for potential use cases applications products services that can come out of large language models and that's something that I want to support I think we'll be better off if more people understand this technology and are applying it to solving problems so with that be sure to hit the Subscribe button to keep up with future videos in the series if you have any questions or suggestions for other topics I should cover in this series please drop those in the comments section below and as always thank you so much for your time and thanks for watching
4-Byoa6BDaQ,2023-07-20T14:24:32.000000,"When you’re robust, your environment can’t hurt you #antifragile #resilience",Second Story is the story of the Phoenix which is a bird who when it dies it rises again from its ashes and this highlights the concept of robustness the Phoenix doesn't care if it lives in a stable tranquil environment or a constantly changing environment worst case scenario if the Phoenix dies it'll Rise Again from its actions the Phoenix is apathetic toward its situation
hB27yAkJLC8,2023-07-19T13:57:49.000000,Being fragile means you have more downside than upside. #antifragile #mythology,first story is the sword of dimicles is the story about a servant who sees the life of a king and wants to be in his position so for one day the servant trades places with the King and what he soon realizes is that it's not so great being King because when you have everything everyone wants to take your stuff everyone wants to take your power and the sword sitting above the King's throne is a symbol for the constant threat you are under when you are in a position of power so being King is an example of being fragile because when you have everything you have nothing to gain and everything to lose so any changes will probably not benefit you
Ty2mi994yfE,2023-07-17T16:02:05.000000,How to learn Causal Inference with #python #dataanalysis #datascience,one I use the python libraries called do y that was like one of the first I saw in this space it's a library made by Microsoft what do y and all of these other python libraries that do causal inference they usually have really good documentation so and a lot of times they'll have like techniques that I haven't even heard of I'm like what the heck is this so if you click on the more information in the documentation they usually have a pretty nice write-up on what that technique is they'll link some resources so code documentation is a great starting point
WzL3USLPwmY,2023-07-16T18:31:57.000000,A physicist edited this video #physics  #datascience #curiosity,a long tradition of physicists poking around where they shouldn't be you'll see physicists everywhere economics biology psychology statistics wherever I think a lot of people that I've met that have studied physics have this Natural Curiosity and they tend to pull on the threat of curiosity and just see where it takes them and so that's why I think you find physicists in all sorts of disciplines I identify with that classically trained in physics but I've worked at a car dealership I work at Toyota in financial services I'm really into entrepreneurship I love business I'm fascinated by nature and natural systems and even going back to Isaac Newton he was getting into finance and there's a famous quote from him he said he could predict the emotions of the stars but he can't predict the madness of man so I think there's always that Curiosity of the physicist to try to explain the world through some kind of physical model or at least something like that
ejwVSYKo7jk,2023-06-30T20:19:24.000000,Lessons from Spending $675.92 to Talk to Top Data Scientists on Upwork #freelance #datascience,are my four takeaways I'm spending 675 dollars on upwork talking to top data science Freelancers the first one is to do good work while this might sound obvious this is what allowed a lot of the most successful Freelancers to operate on referrals from past clients alone that's a really good place to be if you're a freelancer second is to find a niche finding a niche makes you a big fish in a small pond also when you built a niche not only do you specialize in a certain type of work and get really good at it but it becomes a lot easier to sell and Market yourself to the people that are looking for those skills the third is to form alliances across the tech stack no one can be a specialist in anything for example I'm a data scientist so I'm really good at data sciency type stuff I can build you machine learning models I can play with data I can spin up some data visualization but there are a lot of other skill sets that are relevant when it comes to using data science in the real world to solve the
_Wjn0gm4g20,2023-06-23T18:10:33.000000,I Spent $675.92 Talking to Top Data Scientists on Upwork—Here’s what I learned,hey everyone I'm Shaw and in this video I'm going to share what I learned talking to top data science Freelancers on upwork [Music] so this journey started about eight months ago for me when I began my search for mentorship I was looking for people on LinkedIn that operated in the intersection of data science and Entrepreneurship well I had a little bit of success with this it mainly consisted of people not responding or ghosting me and I don't take this personally I actually kind of expect it because I feel the ideal Mentor for me is someone that owns and operates a business in the data science space and they probably don't have time to be talking to complete strangers on LinkedIn however game changer for me is when I took my search off of LinkedIn and on to upwork and for those who are not familiar upwork is a platform for Freelancers basically how it works is clients post jobs on the platform and then there are Freelancers that can apply those jobs and submit proposals so the big realization for me is that entrepreneurs are on upwork while on LinkedIn you'll definitely find serious data scientists and other people that work in the data space most of professionals you find on LinkedIn are not entrepreneurs that's simply because of the fact most people are just not entrepreneurs but that's not the case on upwork on upwork everyone's an entrepreneur because freelancing and Consulting is a business so this little switch of just moving from LinkedIn to upwork for looking for mentors unlocked a whole new range of possibilities so this kicked off a series of 10 calls with top data science Freelancers on upwork I searched for the calls into three parts which was past present and future basically what got them into data science what got them into freelancing how do they operate now how do they get clients and where do they see this going for themselves what's their Vision I'm going to structure this video in a similar way and I'm going to walk through some of the key questions I asked in these interviews and summarize and synthesize the responses I got from all the different Freelancers I talked to and if you enjoyed this content please consider subscribing for more videos on data science and Entrepreneurship and if you have any questions please feel free to drop those in the comments section below I I do read all the comments and try to answer all the questions that I received so with that let's get into it so the first question was what got you into data science and one of the most remarkable things that jumped out at me through these interviews is that every single one of the Freelancers had a different educational background so I'm just going to list off some of these backgrounds biomedical engineering industrial engineering biostatistics electrical engineering computer science Finance physics data science marketing AI economics math and MBA and a data science boot camp so I listen to have 12 things there and there are only 10 interviews and that's for two reasons one most people had training Beyond just a bachelor's degree whether that was a graduate degree master's degree PhD or some kind of boot camp or something like that but the second reason is what I said at the top no two of the 10 Freelancers had the same exact trajectory maybe they did bachelor's degree in computer science but then they did a master's in statistics and then someone else may have done a bachelor's in statistics then they got their PHD in biomedical engineering my big takeaway from this is that there is no correct trajectory or journey to get into data science and this theme of diversity no two people fitting into the same exact mold was the theme that I saw coming up over and over again during these interviews so putting this simply it seemed like different was the norm here so the second big question was how did you start freelancing and again in keeping with this different is the norm theme everyone had a unique story to tell but there were still two things that seemed to come up over and over again which was when people were first getting started there was a strong focus on learning and building a reputation and so when I say learning this is more than just the Hands-On keyboard work this is more than just the technical side of data science that people had to learn it was also the business side of freelancing and Entrepreneurship which is you gotta worry about selling yourself you got to worry about finances you got to worry about juggling multiple projects and clients so that's what learning means here in this content and the second point of building a reputation for any business having a history of positive reviews and testimonials makes it a lot easier to get new clients and grow your business and so when first starting out common strategy is to do a high volume of relatively small projects and this helps with both of those points that I mentioned earlier one since you are doing a lot of projects you have the opportunity to do a lot of reps and explore the space and learn as fast as possible and two if you're working with a lot of different clients you in turn have a lot of opportunities to get client testimonials and positive reviews and so while this small and wide strategy seemed to be great for getting started it did not seem to be a common or good long-term strategy which brings us to the next question how do you operate now and so again different is the norm here people were all over the match so we had people freelancing full-time with people freelancing part-time while trying to build a business or generating income some other away we had people working full-time and freelancing on the side there were people working full-time in a contract world as a 1099 employee also there was someone that was transitioning out of freelance into a full-time W-2 role so to me this really highlights the flexibility of freelancing there are so many ways you can fit freelancing into your work in a way that aligns best with your long-term goals but for those who are not seeking full-time work whether that's W-2 or 1099 it was common to try to constrain weekly time commitments to each client and so typically what this would look like was constraining the amount of hours for each client to between 5 to 20 hours a week or 10 hours a week seem to be a pretty good sweet spot and so with this way of allocating time Freelancers would typically have two to three clients at a time the next question was how do you get new clients from these interviews there were three common ways of getting new clients the first and most common was applying to contracts on upwork or other freelancing platforms the second way was inbound leads essentially clients would find the Freelancers and requests for them to submit a proposal so a key driver of these inbound leads was the reputation a freelancer had on upwork did they have a high job success rating did they have a long list of positive reviews and testimonials these were the types of things that would drive that traffic and then the Third Way which was common among the most experienced or those who have been freelancing the longest is operating on referrals alone and so typically this happens when the freelancer has a great reputation but also builds really strong relationships and does really good work to where their clients become their biggest Advocates and so this is really like the ideal state of a freelancer because you don't have to worry about sales you don't really have to worry about marketing you can just focus on doing a good job and building strong relationships and your work will speak for itself and this brings us to the final question I asked in these interviews which is where is this going and again different is the norm here 10 Freelancers gay gave 10 different answers to this question but I'm going to put their responses into three different buckets the first book it will say is to keep freelancing and to scale up their Consulting business these were the people that said they can see themselves freelancing forever and this makes sense you know when you're a freelancer there is so much freedom and flexibility you can work when you want on what you want with who you want where you want and you can turn up or turn down the volume of work you're doing in case you have other priorities you know maybe you want to spend more time with family or you want to take more vacation and so it's not surprising to hear that some Freelancers can see themselves doing this forever also in this bucket I put those who want to scale up their Consulting business and so this seems to happen organically where a freelancer starts getting more work than they can do alone so they start finding subcontractors to help them fulfill this work and this can naturally transition into a more structured and formalized business so the second bucket of long-term goals I'll say is to generate passive income and to build a product oriented company all the freelancing can be very lucrative and you have so much freedom and flexibility at the end of the day as a freelancer you are trading your time for money basically I pay you to do a job for some hourly rate so while training time for money isn't so bad many people will prefer to trade a little time for a lot of money so for example build a product once and sell it many times that's the common thread in this bucket here trading a little time for a lot of money and so that's why I'm lumping together generating passive income and building a product oriented business so some ways the Freelancers talked about doing this was to develop digital products or sell courses online there were others who were actively trying to build trading Bots or building other trading tools for personal use others were considering building software solutions for mid-size companies leveraging The Experience they've gained and the clients that they've worked with to essentially automate a service that they've done for clients in the past so while no one I interviewed had fully made this transition yet it is something I'm really optimistic about and then finally the third bucket was people that said they want to transition into a full-time role while there is a lot of flexibility and freedom when it comes to freelancing there are people that eventually want to transition into full-time roles and there was even a theme of people transitioning in and out of full-time roles they'll work with a handful of clients but then they'll work for one particular client seems to ramp up or they open up a role for them and then they spend some period of time working full-time for that client and then eventually maybe they want to go back to the freedom of flexibility of freelance so they transition out of that full-time role and the cycle starts all over again so I'll read off a few reasons why Freelancers wanted to transition into full-time roles one of the big ones was you can make a bigger impact when you're part of a larger Enterprise and not operating on your own another was when you're part of a larger team working full-time there's more social interaction and opportunity for collaboration a big one was also you get more certainty in income stability and then finally there might be greater opportunities for career growth both in a larger Enterprise where you are working full-time so just to wrap up here there are four key things that I've taken away from these interviews the first one is do good work so you can operate off of reputation and referrals if you don't have to spend so much time selling and marketing yourself this is typically because you have such high demand and that's a really good place to be as a freelancer because you can leverage that to ensure that the work that you do take on is aligned with your long-term goals the second takeaway which I haven't mentioned yet but something that came up over and over again in these interviews is to find a niche the key to the success of a lot of these Freelancers was finding a niche which essentially made them a big fish in a small pond my reflection on this is when you try to Market yourself as everything in anything data science it's often difficult for clients to wrap their heads around how they can use you but if you find a niche and you're very specific on your expertise and your offering it typically makes you a more appealing candidate to those that are seeking those services so the third big takeaway which I haven't mentioned yet but is also something that came up over and over again is to form alliances across the tech stack so me as a data scientist I have a really strong set of skills around data science but there are many other necessary skill sets that come up when it comes to implementing data science Solutions in the real world so a big piece of advice I got from a lot of these Freelancers was to either get exposure to the full Tech stack so you can do every aspect of it you can do the web scraping you can do the data cleaning you can do the database stuff you can do the modeling you can do the machine learning but you can also productionalize your model you can also spin up a website you can also develop some software you can develop some ads so get some exposure to the full Tech stack or form alliances with Specialists across the tech stack so you don't necessarily need to know how to build a website because you have a strong partnership with someone who's an expert in that and ideally I think it's important to do both get exposure to the full text stack and additionally form Alliance is with experts and Specialists and these other things that you're not an expert in the fourth takeaway is to develop a personal brand and so having a strong presence on social media can make it a lot easier to land new clients whether this drives inbound leads like clients reaching out to you or giving you more credibility when you do apply to contracts if you're applying to a contract to do some kind of NLP work and you have a set of videos or blogs on the subject that you can share with the client that really helps close the deal so I hope you found this video valuable and helpful if you have any questions again please feel free to drop those in the comments section below if you enjoyed this content please consider liking subscribing and sharing with others and as always thank you for your time and thanks for watching
NjMD1bGBNqw,2023-05-26T23:04:47.000000,How to Create a Custom Email Signature in Gmail (2024),hey everyone I'm Shaw and in this video I'm going to walk through how to customize your email signature in Gmail so this is a continuation of a recurring theme in my past few videos which is all about leveling up how you present yourself as a professional in the data space so I talked about how to build a free website portfolio using GitHub pages I broke down the resume I used to get me hired in my current full-time role and here I'm going to talk about how to level up your email signature which may sound like a very small part of the story of you as a professional but it is one that I believe is very important so I did a deep dive into customizing email signatures when I started freelancing back in grad school so as a freelancer you're by yourself and you typically don't have a bigname brand to back you up and give you credibility and this is where I feel a solid professionall looking email signature can make a real difference so having a custom professionall looking email signature is an easy way to give yourself credibility and an easy way to share your online platforms and key links to anyone you send an email with with that let's see how to do this all right so here we have Gmail opened so for those who are not familiar you can make custom email signatures in Gmail and so what that looks like every time you hit this compose button to write a new email your email signature is going to pop up there so you don't have to write it out every single time and when you reply to people you can have this come up by default so anytime you send an email to everyone you know you know they have your name they have a link to your website they can have some call to action and they have links to your social media so the way you set up your email signature in Gmail is you got to go to settings so you got to click on this little gear icon here and you'll click on see all settings and then you scroll all the way down to a section called Signature and then you can create a bunch of email signatures here so I have a few on Deck so I got this master one I have one without my socials one with a few socials I have this test one which we can actually I'll go ahead and delete but what we can do to make a new one is just hit this create new button so we'll call this one example Sig as we saw with the email signature earlier you know was this picture and this format at text and these social media icons so while it's easy to you know add text you know put your name here and it's easy to bold it it's easy to add in links here URLs you can add in images and then you can actually link these images uh with the URL one thing that you cannot do with the default settings in Gmail is have these like two columns or multiple columns in your email signature like what we saw before so if I hit compose again you have the head shot on the left and you have all the info on the right and so there's actually a trick in order to get like these two columns or if you want to add a third column or whatever and it's actually pretty simple so I'm going to go ahead and delete this and and what we're going to do is head over to Google Sheets so if you're using Gmail you should also be familiar with Google Sheets what we'll do is we'll open up a new spreadsheet okay so what we're going to do here is we're going to basically create these two columns create the layout for email signature in Google Sheets and then we'll be able to copy paste it over into our custom email signature the way to do this is you select the cell and then we can go over to insert and insert image and we're going to insert image into cell and so here you're going to want to import your head shot so I already have mine ready to go so I'm just going to go and drag and drop this over and we can see the head shot is appearing here and so an important note here is that you need to make sure the head shot is a proper size so here my head shot is about 100 by 100 pixels and you really don't want it much bigger than that in fact if we go over here and we search Gmail signature size we see that the ideal email signature is 100 70 pixels high and 200 pixels wide so we have to be really cognizant about this when we're customizing our email signature and so here I just did a 100 by 100 head shot so just making sure I'm not too tall I think that's going to be the main concern here so I can clean this cell up a bit to make it nice and tight around the head shot if you want to add a bit more space you can always go over here and do the center line and then this will kind of do the resizing automatically for you next we can add some text here so I don't recommend recommend writing out your whole email signature here cuz when I was playing around with this and I was writing out my whole email signature in the cell here and then I copy pasted it over and then the formatting that got pasted over made the email signature too big and it wasn't able to be saved so I recommend just using Google Sheets to set up the formatting and then actually fill out all the content of your email signature in Gmail itself so here I'll just put test just so we can see that some content goes there and let's see I can now let's left align it but let's make it in the center so I think that looks good and then one last thing is if I just copy paste this over it's going to carry with it these like gray lines and it doesn't look very good so we don't want that what you want to do is highlight the cells and you want to add a border but you want to make sure the Border color is white and you do that and then you can click all borders and then you'll see the borders have disappeared from the cell and then when we copy paste it over we don't have those gray marks anymore Additionally you can add this little accent line in between the head shot and the text so I'll just kind of put that in here as well it just like a light gray have some thickness then you click right border and then it adds that little line there so that's optional you know you can really go to town on how you want to do the formatting of this okay but now we have the head shot we have the text we have this line in between and what we can do is just copy these two cells go back over to Gmail and paste that in and so now you'll notice that this head shot looks terrible we actually don't need to keep this head shot here we really really just wanted to get the formatting get these two columns into this signature customizer here so we can actually go ahead and remove the head shot but now you see we have like this other column here and so with that what we can do is we can reinsert our headshot so I'm going to go to insert image and then I'll do upload and then I'm going to drag over the same exact headshot I use so again you know I use 100 pixels by 100 pixels here so just make sure it's a reasonable size if the size is a little big you can go ahead and just hit small and then that'll make it look nice so now with the headshot in we can start adding text so what I had was something pretty simple I just had my name now we can just have your title I like to have my website here and then some call to action so I'm going to put okay so here now we're running into an issue so going back you saw that the text was kind of jumping over to the next line that's cuz I didn't make this second cell wide enough so we're going to going to go ahead and make that cell a bit wider and then we're going to come back delete all this put this back in and just repeat those steps 100 by 100 pixel head shot comes in we'll make it small okay so now we'll try again so put in my name data scientist so yeah so I like to put my homepage in there and then I'll put a call to action book a call so this is a good checkpoint here cool thing is just like with a lot of text editors these days we can make any text a URL so I'm going to go over to my website and I'm going to get this link as well and I'm just going to copy this and you can highlight the text you want to turn into a link you just highlight it and then click this link button and then you're just going to paste in the link you wanted to go to and so now you should see it pop up as a link and then we'll just do that again for book a call there we go and so already this looks like a pretty nice email signature maybe I'll I'll add a space here just to give it a bit more distance from that line but this looks pretty professional so kind of go in the extra mile we can import the social media icons like we saw so kind of going back adding in these social media icons and what's cool about these is that these are actually clickable links when you send the email and so to do that we'll do a similar thing as we did before and we'll insert an image and so if we click this insert image button here and there are actually two ways to do this so the first way is you can import an image as a URL and so you know easy way to do this is just go to like any website so I already have it loaded up in the search bar social media icons for email signature I I found this one was pretty good and what you can do is just go to any website that these images exist and you can just right click on it and copy the image address and if you do that you can come back over here paste it in and you'll see the image pop up and there you go so now the image is in your email signature the other way to do it is to insert the image like we did before which is by uploading it from your computer so I have these like already on my desktop so I'll just kind of go drag and drop them over so there we go and now the image is there and so again whenever we're inserting images into the email signature we have to be cognizant about the size of the image so the head shot I did is 100 pixels x 100 pixels and this LinkedIn icon I made sure it was 24x 24 pixels and that's actually the same as this website over here I thought this was a good size 24x 24 pixels but this website has more 32 64 128 so on also it's pretty easy to resize images if you're on Mac you can use preview and so it's pretty easy typically to scale down images just find it on the web somewhere download it and just make sure it's 24x 24 or 30 X3 whatever you want to make it okay so we'll just repeat this process so I'll go ahead and insert the other links added a space and I'm going to hit insert image again I'm going to upload it again and then I'm going to bring in a few more so here's Youtube upload okay and you know obviously what social media icons you put here will depend on your industry and whatever platforms you're on but you can kind of customize it get the spacing right I just added white spaces in between the icons for the formatting and then what you can do to actually make these clickable links is you can highlight them just like we did with the text hit this link button and then paste in whatever link you want so I'll go back my homepage and grab all these links so grab my LinkedIn grab YouTube grab medium highlight and then finally get sometimes it'll hide behind this thing but you can find that all right there you go and so now we have our custom email signature and when we're done we got to make sure we save changes so we'll click that I'll make a new email I'll send it to myself make the subject line check out my email Sig and so this is actually a different one you can actually choose whichever email signature you want to use so I'll go to the one we just made which is called email Sig and it'll actually automatically change to whatever email signature you want to use so maybe you want to have different signatures for different clientele or different audiences and then I'll just hit send all right so that's it so I hope that was helpful if you were painstakingly trying to figure the out for hours like I was at one point hopefully this made your life a little easier if you enjoyed this video be sure to like subscribe and share it with others if you run into any issues or you have any questions please feel free to leave a comment down below I do read all the comments and try to answer all the questions that I receive and as always thank you for your time and thanks for watching
gp29_P3_lgo,2023-05-06T16:03:12.000000,"My $100,000+ Data Science Resume (what got me hired)",everyone I'm sure and in this video I'm going to walk through the data science resume that actually got me hired and so I've worked as a data scientist in three different Arenas in research in freelance and now in corporate and through this experience I've gotten a good sense of what the day-to-day of a typical data scientist looks like and what it takes to be successful in a data science role so the whole motivation for this video is that I know a lot of people these days are trying to get that first data science job that first job in any industry is always the hardest to get you know it's that old story of you need experience to get a job but you need a job to get experience and so the goal of this video is to talk through the resume that actually worked and got me hired and share the principles and tips that help me craft the resume as well as things that I did outside of the resume to help me land the job and so be sure to stick around to the end because I'm going to talk about five specific things that you can do to increase your value as a candidate Beyond on the essentials of a resume so I want to start by asking what is the goal of a resume and so it might sound obvious you might say the goal of our resumes to get the job or the goal of her resumes to describe my experience the goal of her resume is to say why I'm a great fit for this role so while there are many goals of a resume the one that I found the most clear and helpful is that the goal of a resume is to get the interview and the reason why I think this way of thinking about it is important is because people get so wrapped up in trying to jam-pack every little detail into their resume with the attention of trying to come across as a more attractive candidate but ironically when you put way too much information especially information that's not relevant to the role it actually makes you a less compelling candidate which brings me to my foundational principle of crafting a resume Which is less is more the way I see it the person reading your resume has a finite amount of attention that they're going to give to your resume so you've got about seven seconds to convey not so much who you are as a person and every little detail of your experience but rather your goal in that seven seconds is to be someone interested be someone that they want to reach out to be someone that they want to set up an interview with and so as we walk through my old resume we're going to see this principle of less is more come up through the design and the content of the resume so let's see what this looks like okay so here is the resume so this was for my current role which is a data scientist at Toyota financial services and so you can see this is a very simple resume no pictures no colors just to the point so I'm going to talk through four things that I think were good here so the first one isn't something I included but something I did not include which was this objectives section that you might see on other resumes and so just thinking back to this principle of less is more and that you have a finite amount of the reader's attention do you want to spend that fine finite attention for them to read a paragraph of objectives or do you want to save that attention for some impressive projects you may have done further down in your resume so for me I don't see any utility in having an objective section for data science roles the second thing I think was good here is I added this technical skills section and not only do I have this section I have it at the top of the resume and so I think one thing that's happening is if a human is reading your resume they are looking for specific skills especially in data science they want to know can this person code in Python do they have SQL experience do they have cloud computing experience you know they're looking for these types of things and so don't make them hunt for these keywords because they're going to be so focused on trying to find python in this wall of text here that they're gonna miss everything else so just give it to them at the beginning so they can relax their mind they're like okay great this person can code in Python that's all we use on our team so now with that sense of relief they can actually read your resume with an open mind the third thing is pretty standard which is these bullet points and how you craft these bullet points and so the format that I've been taught and that I found helpful is to start with a strong action verb and then there are resources online that give you like a strong action verb ideas for these types of bullet points you start with the strong action verb what you did and have some kind of quantifiable impact associated with that thing so here I put conducted data collection processing and Analysis for novel study evaluating the impact of over 300 biometric variables on human performance and Hyper realistic live fire training scenarios so that sounds really interesting but I will say one thing that's missing here is the quantifiable impact so what was the result of this work just stop and ask yourself that question of any bullet point you have on here it's like why is this important in a larger context and I would even say if you can't think of a wider impact of any bullet point consider just dropping it and picking up something else that has a higher impact and so if I was to do this over again I would think a bit more carefully about the impact of this bullet point here and here's a corollary to that anytime you can put numbers in these bullet points it just stands out like you might see clickbaity blogs out there like five ways to lose weight fast or seven secrets that will make you a more attractive mate or something like that there's a reason they put numbers in the titles is because our minds are attracted to that when you see a bunch of letters the numbers stand out and it gets your attention and so use numbers especially if you can have a quantifiable impact like here this is a better example analyze marketing and sales reports to inform an inventory acquisition which resulted in a 50 decrease in average inventory age that's a quantifiable impact and that's the type of bullet point you want to have for everything and then the fourth thing is a really small thing but I think it does make a difference in addition to listing your technical skills here bold them in each of these bullet points so this was actually something I picked up from a Google hiring manager when I was in the looking for a job phase of my life in grad school he was actually the one that also told me to put technical skills at the top and then he additionally said both these things because again if the person reading your resume is looking for something specific don't make it hard for them to find it those are the four things that I liked as far as things that I would do differently or improve upon one I already mentioned which is a lot of these bullet points don't have any kind of quantifiable impact they have the strong action verb they have the technical task that I'm trying to convey but there's no impact the second thing that I would improve on this resume is that notice that it's a two-pager and so there's debate on whether a resume should ever be two page should always be one page I'm not going to get into that but specifically here I might consider dropping the talks and Outreach maybe even the awards and honors here and then just have an Abridged version of the Publications just to bring it down to one page and just try to make it a bit more concise just to recap the four things that I thought were good here were no objectives section putting the technical skills at the top so the reader doesn't have to hunt for it having these strong action verb thing then quantifiable impact on the end and then finally building these key skills again so the reader doesn't have to hunt for them and then as far as the two things that I would improve upon is to reevaluate these two bullet points so they do have quantifiable impact and two try to remove some of these sections so this could just be a one-page resume instead of a two-pager another key point with the whole job application process and hiring process is to always ask for feedback no matter what even if you don't get the role ask for feedback in the interviews even if you don't get the interview ask for feedback if you can get in contact with a person and so toward that end when I had that first interview with the hiring manager like that initial interview before you get to the technical interviews and interviews and all that kind of stuff I asked a simple question to the hiring manager which was why me what stood out about my resume what made you think I was a good fit for this role and so I find that a very good question to ask in any kind of hiring process because now you're getting a little bit of insight into their world you're seeing what their problems are and how they see you fitting into that world and so the hiring manager mentioned two things that stood out about my resume that he liked the first was PhD and if you don't have a PhD or you're not pursuing a PhD you might think okay wow that's not helpful at all but I will say in my experience it seems that graduate degrees do open a lot of doors and opportunities so even if we're talking about a master's degree and not a PhD having these Advanced degrees definitely help differentiate you from other candidates and then the second thing that stood out wasn't so much the resume itself because again really all that seemed to stand out was the PHD okay he knows python let's talk to this person but the other thing that that stood out was I had this website here and so we can click on this and it takes you to an actual website having a website a home page an online portfolio is a great way to just stand out as a candidate and so this is being hosted completely for free using GitHub pages and actually in the last video on my channel I have a whole walkthrough guide on how you can spit up a website completely for free using GitHub pages so just having a portfolio website can help differentiate you from other candidates and it doesn't even have to be as sophisticated as this so this took me about a weekend to spend together using this open source drag and drop website builder called Moby rise so I use this Moby rise it's like a drag and drop interface to build the website it's completely free but you don't have to go through all this trouble so GitHub Pages gives you these great themes to spin up a website from a markdown file so still no coding involved whatsoever but the result in like 15 minutes of filling out a readme file you can spin up a good looking website that can also help differentiate you from other candidates and so that was the resume and as we saw based on the hiring manager feedback it wasn't just the resume that stood out it was also this portfolio so this is a great point because you have to realize the resumes and everything there's so much more beyond the resume that goes into the hiring process and everyone has the resume but when you're trying to stand out when you're trying to get the interview a lot of times it pays not so much to be a better candidate but to be a different candidate and what I mean by that is it pays to stand out to distinguish yourself as a candidate for a role and so we already talked about one way to do this which is a website portfolio but even beyond the resume and Beyond the portfolio there's still more that can be done and so what I'm talking about here is networking so a great way to do this is LinkedIn if you have a role that you're interested in go on LinkedIn and try to find people that work at that company try to find people that work on that team try to find people working in HR are at that company and just try to make contact with a human even though it's not going to work out every time that 20 of the time that you actually get connected with someone and start talking to someone that's in the company this can have a very significant impact on your probability of getting the job so if you're sitting there and you're like Shaw you're not telling me anything I don't know I have the resume I have the portfolio I reached out to 200 people in the last three months on LinkedIn and still I'm not getting anywhere so if that's the case a lot of times What's Happening Here is you need more experience you need more development as a data scientist which brings us back to the dilemma I mentioned at the beginning of this video I need experience to get a job but I need a job to get experience but I don't think that's the case necessarily I can think of five things you can do to get more experience as a data scientist and make yourself a more attractive candidate for these data science roles and so the first one that might sound obvious which is education and so this is a lot of go-to's for people you're applying to data science jobs with a bachelor's degree and you're not getting anywhere maybe get that master's degree maybe get that certification if you have the certification master's degree and it's still not working out for you maybe go for another certification or maybe go for a PhD but I'll just caveat this if you're getting a degree just to get a job I don't think that's going to be the right answer most of the time the second thing you can do is independent projects if you don't have enough experiences as data scientists get the experience by coming up with projects go find some data on the web build a web scraper and gather data and start building interesting data sets so you can do interesting data science projects and put that on your resume put that on your portfolio the third thing that you can do that I found very helpful is to make content write blogs on medium publish them in towards data science make YouTube videos like this where you're explaining data science topics where you're explaining your data science projects and you're displaying your competency and people have something tangible they can click on on your portfolio or you can refer them to with a link and Beyond just being something you shared with people sometimes these blogs and videos they take on a life of their own and people start coming to you asking you questions about this stuff and maybe even offering you jobs the fourth thing you can do that seems to be coming more and more popular these days isn't so much doing more data science projects or going back to school but just hosting on LinkedIn and so I've seen a lot of people working in data roles or still in school building a following and building an audience on LinkedIn and through that growth they're getting a lot of visibility and this leads them to getting job offers and so this is interesting because you don't necessarily need to be developing your skills further you just need to be showing off the Knowledge and Skills that you already have and then the fifth option is you can start freelancing and so I really like this option because if your goal is to get a full-time data science role when freelancing on a website like upwork you're getting a lot of interview reps and the time it takes you to apply to 10 full-time roles you can apply to a hundred upwork roles so you're getting 10 times the experience writing resumes writing cover letters doing interviews than you do otherwise and so you can really develop that skill set quickly and another benefit of freelancing you get experience by working on real world projects that aren't just for education but are making an impact for your clients and is putting money in your pocket and so I hope this video was helpful for you and trying to land that first data science role and break into the field if you enjoyed this content please consider liking subscribing and sharing with others if you have any questions about data science or career stuff please feel free to drop those in the comment section below I do read all the comments and try to answer to all the questions that I received and as always thank you so much for your time and thanks for watching
D9CLhQdLp8w,2023-04-24T21:30:03.000000,How to Make a Data Science Portfolio With GitHub Pages (2024),everyone I'm sure and in this video I'm going to walk through a super easy way to make a portfolio website without any coding using GitHub pages so it's about that time of year people are graduating graduating college maybe you're graduating grad school and now it's time to enter the workforce and if you're like a lot of people you've probably heard of data science and how data scientists are doing all these cool things you know they're building chat gpts they're building models they're using machine learning and AI to solve business problems and create impacts and this sounds like a really fun and exciting field to be a part of as someone that works in data science a big problem for a lot of data scientists is not so much knowing your stuff like the technical side of things but it's the ability to show your stuff and sell yourself and this is just something that you never really do When developing the skill set to become a data scientist but if you're trying to get a job whether that's full-time at a company or you want to go into freelancing or Consulting being able to sell yourself is a critical part of the process so one thing that I found super helpful in getting data science work is having a website portfolio that employers can go to to see my experience my projects and just get to know me but the problem is I'm a data scientist I can build you a machine learning model but don't ask me to build you a website because it's not going to be great so I'm totally incompetent with HTML CSS any kind of like web dev type of stuff and if you're a data scientist it's likely that you're in a similar boat but lucky for us our friend GitHub has this built-in functionality for generating and hosting websites completely for free without requiring any kind of web development experience whatsoever so GitHub Pages makes this spinning up a website super simple and I use this functionality a lot in grad school in spinning up websites for my own portfolio and for projects that I was working on quickly running through here what the steps are you have these two options you can build a website from scratch and just host it on GitHub or you can go a much easier route where GitHub will generate you a website automatically from your readme file using a package called Jekyll And so here all you gotta do is fill out your readme file with what you want in your portfolio and then Jekyll will take that text and generate a website based on the theme that you choose not too long ago like one two years ago this was like really really stupid simple just looking at this website it's a pages.github.com just a few steps so you go to the repository settings and then it used to have this Theme Chooser built in so you just say like which branch you have your readme file in and then you just like choose a theme and then you have this user interface this GUI where you could just click which theme you wanted and it would just automatically generate the website so when I first wanted to make this video I'm like okay yeah it's gonna be super simple show people how they can make a website completely for free free without any web development experience whatsoever but when I came to spin up my website this button was not there anymore this kind of let me down a rabbit hole trying to figure out what happened couldn't find anything anywhere no one's made a YouTube video about this no one's written a Blog about this github's documentation was not very helpful to me however after messing around with it for a couple hours I finally figured it out and now I'm gonna walk through step by step how you can build your website using GitHub pages and this built-in functionality so this is what the final product looks like so it's a really clean design cool looking website you have your picture here you think about your name like whatever job you're looking for I didn't do any kind of coding whatsoever I just typed up a markdown file and this was automatically generated so I was able to throw this together in less than 30 minutes because a lot of the stuff it's just like copy pasting from your LinkedIn or your resume or something similar and so this is the final product now I'll walk through the steps to build this so first step go to your GitHub Pro profile if you don't have a GitHub profile go ahead and make one it should be straightforward and a good idea if you're trying to get a job in data science here at our GitHub profile we're going to go ahead and click repositories and you can see this is the example portfolio I made this past week but we're not going to go there we're going to start over from scratch and click new and then we should get a screen like this so create a new repository repository name so we have two options here option one is if you want this portfolio website name to just be your GitHub username.github.io you just type that in right here so Shaheen T is my username GitHub dot IO this will appear in the search bar and this will be your website name I already created a website portfolio using this website name so it's not going to work for me but if you haven't done this already then it should work for you the second option is you can give it any repository name you like so it could just be portfolio well it could be data science it could be data scientist you know whatever you want and what's going to happen in the search bar for example if you make your repository name portfolio what's going to appear in the search bar is going to be your GitHub username.github dot IO slash whatever you make this repository name so if I make a portfolio it'll be Shaheen T github.io portfolio Okay so we'll go ahead and name it portfolio we don't need a description here and it's probably best that we don't have one but we're going to initialize our repository with a readme file so we'll click that this is gonna make it really quick and easy to spin up our website I'm not going to have a git ignore we're not going to add a license that's not necessary so just to recap we make our repository name either our GitHub username.github.io or whatever other name that we like so just go in portfolio here don't need a description and just be sure to click read me and then we're gonna create repository and there we go so now we have the super Bare Bones repository so we already have a readme file here but we're going to need to add one more file called a config file so to do that we can just go to add file create new file and we'll call it config.yml and so now that GitHub got rid of this choose theme button in the interface so let me go back now that they got rid of this choose theme button here and you can't have this super easy user interface for picking your theme you have to do it using this config.yaml file and so this is actually super simple and it's not much more difficult than this interface so I'm going to kind of jump ahead and this is the config file for the example that I've already put together we can just go ahead and like copy paste this so title would be at the top left corner of the portfolio website so you can just put your name here logo we can actually comment this out for now because this is going to be the relative path of the image you want on that left hand side so let me just go back so this is where the title appears this logo relative path is where this image is located in your GitHub repo but since we're starting fresh I'm going to comment that out if you want to add text below here so if we added a description to our repo it would appear here or we can overwrite that by manually putting a description here so show downloads true it's just giving you these options to download the zip the tar or go to the GitHub so I put true for my example portfolio if people want to steal the code but for yours I mean this is optional if you want people to be able to download your code you can make it false or just comment it out and then finally the key thing is this theme so instead of being able to click on the theme we have to manually kind of type it out using this syntax here so I went with this Jekyll theme minimal so that's what we're seeing here it's super Sleek super nice I think it makes a lot of sense for a portfolio but if you don't like that for every reason there are a bunch of other supported themes for example let's click on architect and we can see what that looks like so that looks like this a little different different design but again this will be generated automatically based on your readme file in the GitHub repository so I'll share this link it's pages.github.com themes and I'll share it below if you want to explore the other theme options okay so we have our title we're going to have show downloads we're going to just comment this out we don't really need that and then we're just left here so really right now all you need is the title and the theme that you want to choose you go ahead and hit commit changes and so now you have two files in your repository you have your readme file and your config file and now you can just start adding stuff in here so like data scientist education work experience uh data scientists full of points here we go big impact project one big impact project two let's see what else is good to have here education your work experience what else do I have projects yes that's important so projects see uh and then so on and so forth so you can start just building some stuff out I'm just doing this so when we spin up the website something appears so just through something really simple together here and so still we just have a readme file in this simple config.yaml file and then the last step is to go to settings here we're in settings then we're going to scroll down to pages and then we have this section here build and deployment so here we're going to leave the source as is we're going to deploy from a branch and then under Branch we're just going to hit Main and then we're going to keep it as root because we want GitHub pages to look at the readme file and the config file in our root directory and we're going to hit save so now notice that we didn't have that super simple GUI to select our theme and then if you hit add a jackal theme you get some instructions here but I got tripped up on theme theme name because this isn't the right syntax here so this uh typo here it's uh made this like really straightforward task like a two-hour task for me so you can put just Minima or minimal here you have to put Jekyll Dash theme Dash Minima so if you were in a similar boat and you were struggling for hours like me it's just a simple syntax issue and that's why the website's not working anyway going back so if we go here to actions we see it's already been built but if we came back earlier we would have seen this is like processing and something's happening but we built Pages was built and deployed so if we click over here to deployment we can hit this view deployment option here and it'll take us to our website so oh look at that then you can look in your search bar what the website name is so for me it's shaheent.github.io portfolio and then if we look over here we see our readme file built out we have data scientists which was in the title so I'll go back so we can compare so we have our readme file here so data scientist education work experience projects EEG band discovery that looks a little different but looks better here on the website and then we just build it out and so we can kind of do it on the cooking shows where I don't walk through the process here and just kind of jump to the final product and so going to this example portfolio repo here are a few simple things to do one I'll definitely add a assets slash image path and then dump all the images you want to use in your portfolio here going back next big thing is so we'll go ahead and edit this readme and so configuring a readme file is a lot easier than building a website with HTML you can just copy paste this or clone the repo and use this as a starting point you can customize the structure you can start completely from scratch whatever makes sense for you just a couple things you know you can add links here so this is the standard GitHub syntax you just say what you want the text to appear as and then this is what the link is that people click on if you want to add images you put the title of whatever image and then you put the relative path to the image so again it's in this assets image subdirectory and then that's the file name and then so on and so forth so really go to town on this you know you can spend probably hours just tweaking and fine-tuning it but this is hopefully a nice jump start and it'll get you something pretty close to a final product okay so I rambled on a bit at the end there but hopefully this is helpful you know it's hard to get that first job and bring to the data science field so I hope this kind of accelerates that process for you and if you enjoyed this video please consider liking subscribing and sharing the video with others if you have any questions about like building a portfolio or like getting a job data science you know feel free to drop those in the comment section below I do read all the comments and I try to respond to all the questions that I receive and as always thank you for your time and thanks for watching
4vvoIA0MalQ,2023-03-18T23:34:38.000000,Dimensionality Reduction & Segmentation with Decision Trees | Python Code,hey everyone welcome back I'm Shaw and in this video I'm going to continue the series on decision trees and talk about a couple of applications so in the last two videos of the series we talked about how we can train predictive models using decision trees so in the first video we just talked about models employing a single decision tree then in the second video we expanded this idea to tree ensembles so if you haven't already be sure to check those out because we're going to build upon those ideas in this video the whole point of the discussion today is that we can use machine learning models specifically decision trees for more than just making predictions so this is what I'll call Next Level uses of decision trees not because they're anything profound or groundbreaking but because they go beyond this obvious task of just using a machine learning model to make a prediction and I think for those who are just getting started in data science it may be easy to think that all there is to data science is getting some data training a model and making predictions and that somehow through this process there's going to be immediate real world impact in value and the reality is it's not so straightforward so what I really like about data science is the critical thinking and the creativity that's required to use these tools and techniques to solve real world problems and provide value and so that's all I mean by Next Level so toward that end in this video I'll talk about two ways we can use decision trees for more than just making predictions so the first is reducing predictor count and the second is called predictor segmentation starting with the first one reduce predictor count so this goes back to the previous video of the series where we talked about tree ensembles where we stitch together a bunch of decision trees which made our machine learning model more robust while decision tree ensembles give us so many great things like I was talking about in the previous video all these great things come at a cost which which is that tree ensembles are a bit of a black box so we know what we put into the tree Ensemble and we can see what comes out of it but what happens in between is a bit of a mystery and this is a problem for a lot of machine learning models we may get this great predictive performance but a lot of times it's not so easy to interpret what's happening under the hood however we don't have to use our tree Ensemble to make our final predictions rather what we can do is take the feature importance ranking from our tree Ensemble model and use that to inform a simpler set of predictor variables and so this is all motivated by Occam's razor which is a very popular idea and in this context what it implies is that simpler models are better so let's say that we have our true Ensemble model and it has 30 predictor variables to it but following the logic here what we want to do is take these 30 variables and just keep the handful of variables that are most important so not only does this help us in interpreting what the model is doing but in a lot of cases it can actually lead to an improvement in predictive performance so walking through what this might look like we take our tree Ensemble and we spit out our feature importance ranking then what we can do is take the top predictor train a machine learning model from it so this could be a decision tree could be a logistic regression model linear model neural network it really doesn't matter what kind of model we use it for we use the one predictor to develop a model and then we assess that model's performance then we can do the same exact thing for the top two predictors and now we have a model with two variables we grab its performance metrics for three models four models and so on and so forth so we just keep doing this until we have all the predictors in our original tree Ensemble model once we go through this process we can plot a chart that looks something like this we're on the x-axis we have the number of variables included in the model and then on the y-axis we have some performance measures so here I put AUC just as an example each of these points corresponds to a different model and we can see the gain and predictive performance here so for example we kind of see that we have pretty big gains until we hit about three variables so what this tells us is maybe we don't need all six variables and we can just get away with using three of them without a major loss in predictive performance and so the upside of that like I mentioned earlier is that a model with three input variables is a little easier to interpret than a model with 6 or 60 input variables so now I'm going to walk through some example code of doing this so here I'm going to use the same data set that I used in the previous video this series so there's a bit of overlap between the code here and from the previous video so I'm not going to spend too much time on any recurring details but if you want to learn more check out that video and also the code is available at the GitHub linked down here and Linked In the description below First Step as always is importing modules this is a lot of the same stuff as the example code from the previous video with the only addition of this import here where we're bringing in a logistic regression model from sklearn and then as always we're going to load in our data next we're doing some data prep this is very similar to what we saw in the previous video this is something new where all I'm doing here is since Y is a Boolean variable meaning it can only take values of zero or one all I'm doing is switching the meaning of zero and one so originally zero meant the tumor was malignant and one meant the tumor was benign and all I'm doing with this transformation is switching one means the tumor is malignant and zero means the tumor is benign and the reason I'm doing this is that later down the line when it comes to interpreting what the coefficients of our logistic regression model mean it's just a bit more intuitive to talk about things in terms of risk of breast cancer as opposed to the opposite which would be like safety from breast cancer and then this should also be a review where we're using smoke to balance our imbalance data set so we have way more benign cases than malignant cases so all smote is doing is synthetically over sampling the minority class and then we're using this train test split function to create our training and testing data sets Okay so now we can train our random Forest so this is one of the tree Ensemble methods we saw in the previous video so we can fit that model with just a couple lines of code so next we have something new everything up until this point we basically did in the previous video but now we're kind of going into some novelty so all we're doing here is pulling the feature importances from our random forest model and then we're sorting them in descending order so what this looks like is this where these are all the names of our features and these numbers here quantify their relative importance and so now what we can do is exactly the process I was describing before or retrain a model using the top predictor and assess its performance and we train another model using the top two predictors assess the performance top three sets performance so on and so forth so what this looks like in code could be something like this where we're initializing all these lists to store our classifiers and then to store our different performance measures we can ignore this I equals zero here this is just left over from an earlier version of the code what we're doing here is for I corresponding to the number of elements in this series we're going to go through one by one and do the following block of code so what we're doing here is listing the feature names up until I plus one we train our logistic regression model using this line of code and then we're just appending things to these lists from before so we're pending the classifier to the classifier list we're appending the AUC value for the training data set and then we're appending the AUC value for the testing data set if this is confusing and complicated don't worry what matters is this final result which is just like what we saw before which is our number of variables plotted on the x-axis and then the performance of the models on the y-axis so this this red dashed line is the AUC value for the random forest model we trained originally so this is a tree Ensemble model that uses all 30 predictor variables but what's really remarkable is that once we hit five variables the logistic regression model actually outperforms this more sophisticated model that uses six times as many variables and then you can see after five variables the logistic regression models just keep getting better and better but let's say for our purposes we really value being able to interpret what the model is doing as well as the model's accuracy so let's say once we beat our random force model we're satisfied so that's the model we're going to use and then since logistic regression is a linear model we can easily interpret the relationship between our predictor variables and the target variable by looking at the model coefficients and so the bars here are just showing the coefficient values looking at worst perimeter which is about 0.3 the way to interpret this is a unit increase and worst perimeter translates to a 0.3 increase in the log odds that the tumor is malignant so I know that was a mouthful making that a bit more qualitative as the worst perimeter increases the probability that the tumor is malignant also increases so now we kind of have the concrete quantification of the interaction between our predictor variables and the probability that the tumor is malignant and so there's a small technical detail here that I don't want to spend too much time on but I talk about more in the blog and it has to do with the resampling since we use mode to synthetically oversample the minority class we can't immediately translate our logistic regression model outputs to probabilities and that's just because the y-intercept for our logistic regression model is biased due to the over sampling and so there's a simple fix there we can just adjust our y-intercept to make it not biased and then everything works perfectly and so if you want to learn more about that check out the blog okay next we have predictor variable segmentation and this actually goes back to the first blog in this series where we used a decision tree model for sepsis survival prediction and there the final decision tree we had looked like this and what's interesting here is even though we had three predictor variables the vast majority of these splits are only using age so here the initial split is splitting on ages less than or equal to 58.5 years and then 44.5 78.5 56.5 67 86 so this is really interesting what this is indicating is that when it comes to sepsis survival age is the most important risk factor we have in our data set and so the other ones we had were the sex of the patient and also the number of previous sepsis episodes and so sometimes in cases like this where there's one predictor variable that has this outsized impact on our Target it can make sense to do segmentation on that predictor variable and what that means is all we're doing is taking this continuous variable age and we're going to partition it into discrete sections so kind of looking at this visually let's say we have ages on our data set ranging from zero to a hundred all segmentation does is split these ages into some number of subcategories so let's say we want to split it into five subcategories and then the result looks like this and so what you can do now is instead of training a decision Tree on all of your data you can train separate decision trees for each age group and what this can translate to is better model performance especially if there are systematic differences between these age groups which requires separate model development so the question is how do we come up with these segments so we can definitely do it manually so we just kind of look at the data and say okay let's do this age group and that age group or use some kind of subject matter expertise but another way we can do it is using a decision tree so this picture here is showing how we can come up with these segments using a decision tree you notice that age is actually being split into these different sections based on the sepsis outcome of dead or alive but maybe we wouldn't want to use this decision tree directly because it has this other variable involved in the splits so what we can do is train another decision tree model but now instead of using the three predictors of age sex and number of sepsis episodes we can just use the one variable we care about which is age so now I'm going to walk through what that looks like I'm going to use the same data set from the first video of this series and then as always we're going to start by importing our modules so these shouldn't be anything new next we're going to load in our data just like we did in the first video and then we're going to do some some data prep so here all we're doing is keeping the variables of age and the sepsis survival flag and now we're going to do a little bit of data prep so what we're doing here is we're grouping the data based on age so you can imagine that we have all these different patients and there can be multiple patients of the same age so all we're doing here is reshuffling the data to have only unique age values but then for each of these unique age values we're going to have a percent of patients that are alive and we're going to name this column percent alive and then on the flip side we can take 1 minus the percent alive and create a new column called percent not alive and so the result of that is a data frame that looks like this so now we only have unique age values starting from zero going all the way up to a hundred and then for each age value we have the percentage of them that are live and the percentage of them that are dead next all we're doing here is grabbing the variable names and creating separate data frames for our input and Target variable so here the predicted variable is age the target variable is going to be percent not alive and as a first pass to the relationship we can just plot them against each other it's on the x-axis we have age and on the y-axis we have percent not alive so as percent not alive goes up that's the indication that the risk of sepsis increases so you can see around midlife there's this clear uptrend of the percent of patients that are not surviving their sepsis episode but before that this risk is relatively low and stable and so just looking at this plot we could probably chop up this data into any number of segments based on this risk so maybe we would do like zero to 40 40 to 60 60 80 100 like whatever but this is just us eyeballing it and it'll be interesting to compare this intuition to what the decision tree is going to spit out okay moving forward we can now train in our decision tree model so here we can Define our number of bins by controlling the maximum number of leaf nodes in our decision tree regressor and the reason this works is that as we saw in the first video a fully grown decision Tree on this data is just massive so you can virtually have any number of bins that you like and it'll work and then finally we just fit our data to the decision tree and now with the decision Tree in hand we can go in and grab all the split values in an automated way so this code is a bit involved so I won't spend too much time on it but for those who are curious you can take a look at it here and it's also available at the GitHub repository linked here but the final result looks something like this so here we have the same plot from before where we have age and years plotted against the percent not alive and so this is qualitatively pretty similar to what we were talking about before like maybe we would have put one here and then adjusted the rest but this is kind of a tricky problem because if I shift this border from here to here to make this first bin look a little better now this bin may not look as good because now you're mixing together these lower risk patients with these higher risk patients and then from a treatment standpoint that may not make a whole lot of sense that's one of the upsides of using a decision tree and leveraging that greedy search to Define these bins because it already is doing that tricky optimization for us and then as a final note I'll just say to take all these with a grain of salt just because the decision tree spits out these optimal age buckets this may not translate well to treatment strategies and so as opposed to just taking this as gospel this is more of a starting place and may just serve better than just arbitrarily drawing lines for these different age groups okay that's it so if you want to learn more be sure to check out the blog published on medium and Linked In the description below feel free to steal the code from the GitHub repository and apply it to use cases or projects that you're working on if you if you enjoyed this content please consider liking subscribing and sharing your thoughts in the comments section below I do read all the comments and I find all the questions and feedback that I receive very valuable and as always thank you for your time and thanks for watching
ZaXpMou55lw,2023-03-03T14:22:24.000000,10 Decision Trees are Better Than 1 | Random Forest & AdaBoost,hey everyone I'm Shaw and in this video I'm going to continue the series on decision trees and talk about decision tree ensembles so instead of just using one decision tree a tree Ensemble combines a collection of decision trees into a single model so with that let's get into the video so like I just said a decision tree Ensemble is a collection of decision trees which are combined into a single model so if you recall from the previous video we saw decision trees where a way we can make predictions through a series of yes or no questions and they look something like this you start at the top node here and just follow the arrow heads based on predictor variable values which eventually lead you to your final prediction on the other hand a decision tree Ensemble will look something more like this so now instead of a single decision tree we have multiple decision trees each giving a prediction and then we can combine these predictions together to give us our final estimation and the key benefit of a decision tree Ensemble is that it generally performs better than any single decision tree alone and we'll kind of touch on why this is the case a little later in the video but first i'm going to talk about two different types of decision tree ensembl the first one is bagging which is short for bootstrap aggregation or bootstrap aggregation bootstrapped bootstrapped aggregation first short for bootstrapped aggregation and then the second one is called boosting which is isn't short for anything starting with bagging here the idea is to train a set of decision trees one at a time by randomly sampling with replacement so what the heck does this mean so we'll just walk through this one step at a time say we start with our training data set T KN and each of these blocks here represents a different record or example in our training data set so we have record one we've record two 3 four five what we do here is we create another training data set by randomly sampling T not with replacement what this might look like is this where we randomly pick five records from the original training data set and so notice record number three actually shows up twice in this training data set this just follows from sampling with replacement so basically all that means is every time we pick a record from T for T1 we will replace it before making a second pick so we have this new training data set T1 we can just do the same thing and we get T2 so let's say this this time we really got a lot of twos through this random sample and then so on and so forth and then let's say the nth training data set looks something like this so now notice instead of just a single training data set we have a collection of training data sets which allows us to train a collection of decision trees so that could look something like this and then just like we saw on that first slide we can combine the predictions from each of these decision trees to produce our final prediction so one of the most popular machine learning algorithms that uses bagging is called random forest and I'm not going to get into all the details of that in this video but for those who are interested be sure to check out the blog published in towards data science where I give a few more details about random Forest additionally there's a really nice paper by the creator of the random Forest algorithm Breeman who is very well known for his work on decision trees and decision tree ensembles so I definitely recommend reading that paper if you're into that kind of stuff the second type of decision tree Ensemble we're going to talk about uses something called boosting so boosting is completely different than bagging so here we will recursively train decision trees using an error-based reweighing scheme no worries if this doesn't make any sense we're going to walk through what I mean by this one step at a time so again imagine we start with this training data set T KN but now we're going to introduce this concept of weight and this is exactly what it sounds like essentially we can give different different records in our data set more weight or more importance when it comes to developing our model so we'll just start with T KN in such a way that all the weights are equal so every record every example is equally important and then we can use this training data set to create a decision tree we'll call it hot but now we can create another training data set based on the performance of this decision tree and so that might look something like this so notice the different color here all this is showing is that records 1 and four were correctly classified in this binary classification problem we're trying to solve while records 2 3 and five were incorrectly classified and so what we can do now is we can decrease the weight of Records 1 and four and increase the weights of 2 three and five and then with that we have this new training data set T1 and we can train a new decision tree we'll call it H1 and then we can repeat this process we evaluate the predictions of H1 we see which records were correctly predicted which records were incorrectly predicted update their weights accordingly create another decision tree and so on and so forth and we can do this for however long that we want now notice again we have a collection of decision trees and we can just aggregate the predictions of these decision trees into a single estimate the first technique that really introduced this idea of boosting is called adaboost or adaptive boost and so when people are talking about boosting they're typically talking about a process similar to what we see in adab boost which is essentially what I walk through here just explaining some of the details of it more here so basically all that adaboost does is it combines each of these decision trees into a linear model and weights each of the decision tree predictions based on this Alpha value and then the alpha value is just proportional to the decision tree's performance and here what I've written out is the specific reweighting scheme used in adaboost so notice that incorrectly classified records will get a weight update proportional to this value while correctly predicted records will have their weight updated proportional to this Factor over here ever since adab boost was introduced in the mid 90s there have been two major Innovations around this idea of boosting the first of which is called gradient boosting so instead of talking about the specific reweighing scheme and details of adab boost gradient boosting just provides a more generalized framework where you can take any differentiable loss function and Define its gradient and Define some boosting strategy from it so the second major Innovation comes from a library called XG boost which basically makes the gradient boosting idea much more scalable and computationally efficient through a set of different heuristics and so that's all I'm going to say about those I talk a little bit more about grum boosting and XG boost in the blog associated with this video and I also have the original reference for those ideas in the blog as well so now coming back to this question of why are decision tree ensembl better than just single decision trees and if I were to summarize everything into a single picture it would be something like this we're essentially going away from point estimates toward population estimates so what I mean by this is instead of just having a single number as our prediction from our decision tree we now have a population of predictions from our decision tree Ensemble which has these three main benefits that I'm going to talk about now so the first key benefit is that decision tree ensembles are much more robust to the overfitting problem than single decision trees so if you saw the previous video of the series we saw that overfitting is when your machine learning model essentially over optimizes to a single training data set in such a way that when you try to apply it to new data it doesn't work as well this turns out to be a pretty big problem for just single decision trees but this problem for a lot of cases tends to go away when you start aggregating groups of decision trees together the second key benefit of decision tree ensembles are more robust feature importance rankings so importance rankings are a critical output of any decision tree based method and so these can be based on things like Information Gain or out of bag error if we're talking about random forest or any number of different ways that we want to Define importance some of these quantities that we can use to define importance are only possible through tree Ensemble based approaches so one example is outof bag error defined in the random Forest algorithm so I won't get into all the details of that if you're interested I talk a little bit about it in the blog but all that to say is that Tre Ensemble based approaches not only open up more ways of defining importance but now kind of going back to this idea of population estimates we're not just relying on the importance of our features from one view of the data essentially from one decision tree but through having a wide collection of decision trees our importance rankings can become more robust and then finally the last key benefit of decision tree ensembles is through population estimates we now have a pretty straightforward way to quantify our confidence or uncertainty in our models predictions and anytime you want to use your model in the real world or there are physical consequences for your model it's good to have some measure of confidence or uncertainty just so you know your exposure while if you just have a point estimate you don't really know the confidence of your prediction it could be zero uncertainty or it can be infinite uncertainty so that's another case where population estimates are very beneficial okay now we're going to jump into some example code so here we're going to do breast cancer prediction using decision tree ensembles like always we're going to use the sklearn python Library which is one of the most popular machine learning python libraries there is and then the data that we're going to use for this example comes from the UCI machine learning repository so the first step is we import our python libraries so just kind of running through quickly we have pandas to help Wrangle our data numpy to do some math mat plot lib will help us make some nice visualizations SK learn data sets so this is the data set while it's originally from the UCI machine learning repository sklearn has this data set readily available for us and this is my short apology for using a toy data set and not wrangling a data set from The Real World but the point of this is to focus on the tree ensembles and not the data preparation step so I hope you'll forgive me next I imported smoke so this is optional I have it commented out for the results we're going to see here but if you're interested head over to the GitHub uncomment this code block and you'll be able to see what the results are doing here and if you recall we use SME in the previous example to balance our imbalance data set and then finally we import a whole bunch of other things from sklearn this handy function to create a training and testing data set decision tree classifier and then we import all the different tree Ensemble approaches that we've talked about so Random Forest addab boost and gradient boosting and then finally we import three different evaluation metrics for our decision trees basically what we're going to do in this example is we're going to train four different models using these four different approaches and we're just going to compare their performance sklearn makes it super easy to import the this toy data set just one line of code we have it in a pandis data frame and then it's always a good practice to plot the histograms of your data here are all the predictor variables we have at our disposal and then here is our Target variable so this kind of goes back to the imbalanced data set idea we see that there are a lot more cases where the breast tumor is benign as opposed to malignant and so while we could apply SME here to synthetically over sample the minority class we're not going to do anything here and see how the 4 different models hold up okay next we Define our predictor and Target variables so this is basically grabbing everything but the last variable name in our data frame this is grabbing the very last variable name in the data frame and then this is just creating two data frames based on the variable names and then with that we can easily create our training and testing data sets here we use a 8020 split SK learn makes this super easy a bit of a warning with this next block of code because I just inherently refuse to copy and paste code over and over again I use what I've heard referenced as automatic code generation and basically all that's going to happen here is instead of explicitly writing the python command out and then copy pasting changing one thing copy pasting changing one thing and so on you can Define your python command as a string and then use this handy execute function to execute the command so while this might conceal kind of like what's going on here this is just a much cleaner way and convenient way that I found to write code and I'm sure there are going to be some programmers out there that are going to yell at me for doing this but I haven't run into any major issues writing code this way so be curious to hear other people's thoughts on this while it might conceal kind of what's going on here I have everything printed out so essentially what's being dynamically written here is this single line of code so all that's happening is four different models are being created using the four different things we imported from SK learn so decision tree classifier is our lone decision tree the random Forest classifier uses random Forest add a boost and then gradient boosting so all these different models are initialized in this clf data structure and then each of these are stored in a list so now we have a list of models as you can see here the automatic code generation gets even worse here because we have a lot of combinatorics happening so we have four different models we have two different data sets and we have three different performance metrics we want to Define for each of these cases so this code block may not make a whole lot of sense but I printed everything that's being dynamically written here so let's just look at these first three lines so all that's happening is we're going one model out of time for the models in our list and we're going to apply it to the training data set and get a prediction and then we're going to compute the Precision recall and F1 for this model applied to the training data set so that's what these three lines of code do here and then we do the same exact thing with the same exact model but now for the testing data set so we get a prediction compute the Precision recall F1 score and we just append everything to the same list and so on for each model the results get stored in this performance dict it's just a dictionary that we initialized here where the keys are the different model names and the values are all the performance metrics relevant to that model and then after all that this dictionary gets is all filled up for all four models for all three evaluation metrics and for both data sets and we can just convert it all to a pandas data frame and so if this is all confusing and doesn't make any sense don't worry about it doesn't really matter what matters is this final output here where we can just simply look at all four of our models and all the different performance metrics that we have and just compare them together so we can see that all four models performed perfectly on the training data set so we can see the Precision recall F1 score for the training data set is one but the real test is looking at the performance metrics for the testing data set and so in this context we can use as a rule of thumb the difference in performance between the training and testing data sets is indicative of overfitting put that more simply the smaller these values are the more overfitting that model is showing based on that her istic we can see that the decision tree classifier seems to be overfitting most because it has the worst performance when applied to the testing data set so on the other side of it random forest and gradient boosting seem to have the best performance when looking at the F1 score a close second is adaboost which has F1 score of 0963 and so these results make sense they agree with this story and intuition that tree ensembles are more robust to overfitting than single decision trees alone so two things I did not explore in this example which are obvious next steps are looking at the feature importance rankings for all four models and then addition additionally doing some kind of uncertainty estimate for each of these models okay so that's basically it if you enjoyed this video and you want to learn more be sure to check out the blog published in towards data science Linked In the description below again feel free to steal the code from the GitHub repository referenced here also please consider liking subscribing and sharing your thoughts in the comment section below and as always thank you for your time and thanks for watching
B6a64wdD7Zs,2023-02-18T17:30:14.000000,An Introduction to Decision Trees | Gini Impurity & Python Code,hey everyone I'm Shaw and in this video I'm going to be giving a brief introduction to decision trees so we might ask ourselves what are decision trees and put very simply a decision tree is something we can use to make predictions via a series of yes or no questions so let's look at a concrete example so let's say we want to predict whether I'm going to drink tea or coffee and to make that prediction we can use a decision tree like the one shown here and so the way this works is we start at the top of the decision tree here and we just answered the following yes or no questions so first we ask is it after 4 pm and if yes we follow this Arrow here and if no we follow this Arrow here so let's say yes it is after 4 pm then our answer is I will drink tea but if the answer was no we would follow this arrow and end up at another yes or no question so this one asks if I got more than 6 hours of sleep last night if the answer is yes we go with t once again but if the answer is no we're going to go with coffee so this is a very simple and straightforward way we can make predictions using just two pieces of information namely the time of day and the amount of sleep I got last night and so just to talk a little terminology because it will come up later these rectangles that we're seeing throughout the decision tree are called nodes and we have different types of nodes so for example the node that sits at the top of the decision tree is called the root node over here in green we have what is called a leaf node and these are nodes that I don't ask any yes or no questions and where we can assign our prediction and then we have splitting nodes which ask yes or no questions but are not the root node and so another way to think about decision trees is graphically and I personally find this a much more intuitive way to think about things so here on the right we have the example from before where we have two pieces of information namely time of day which we're plotting on this x-axis and the amount of sleep I got last night which we're plotting on the y-axis so another way we can represent what the decision tree is doing is by partitioning this predictor space meaning the space defined by R2 predictor variables time of day and hours of sleep partitioning this predictor space into different sections and assigning a label to each section so what this will look like for splitting on 4 pm and 6 hours of sleep is something like this so here's 4 pm we draw a line here and then here's six hours of sleep we draw another line here and now we just look at the leaf nodes for each of these splits and assign a label to each section so intuitively this is all a decision tree is doing it's taking the predictor space splitting it into the different sections and then assigning a label to each section so now that we have a basic understanding of what decision trees are and and an intuition for how they work a natural question is how can we bring this into practice namely how can I use a decision tree in the real world and so this is a great question and as it turns out we can use decision trees in practice by developing them from data so put another way we can learn decision tree structure from data so I'm going to walk through an example here just to give you a qualitative sense of how this works and I'll just kind of start with the disclaimer that there are many ways to grow decision trees from data but what I'm going to describe here is a widely used methodology all right so before getting into it I need to introduce the concept of Genie impurity and so I'm just throwing the equation up here for completeness and for those that I think in terms of math and just describing what this is it's saying that the genie impurity of a sample s like this sample right here is equal to 1 minus the sum over p i squared and so Pi corresponds to the probability of the I class looking at this example here we have two possible classes tea or coffee so the genie impurity of this sample here would simply be 1 minus the probability of t squared minus the probability of coffee squared and if that doesn't make any sense no worries we can think of the genie impurity in terms of its extremes namely its minimum and maximum value so visualizing this we have minimum impurity whenever every class in our sample is identical so either every class in the sample is T or every class in the sample is coffee on the other end of the spectrum we have maximum impurity when each class is equally likely and so for those of you familiar with information Theory or the concept of entropy you'll notice that this quantity Genie impurity is actually proportional with information entropy okay so you might be saying Xiao why are you talking about this Genie impurity what does this have to do with decision trees and I'm glad you asked that because we can use the genie impurity to learn decision tree structure from data and so the goal when growing decision trees is to use our predictor variables to split our data such that the overall Genie impurity is minimized so essentially growing a decision tree is an optimization problem and in the following slides I'm going to walk through a popular way of doing this so just putting aside our minimum and maximum impurity just as a reference and then consider this data set on the right where each of the rows or records of this data set are represented in this sample over here so each icon in this box corresponds to a different record in this table so let's say like this icon corresponds to the first row with this T this coffee icon corresponds to the second row and so on and so forth so again our goal is to use our predictor variables time and amount of sleep to split our data such that we minimize the overall Genie impurity a sort of Brute Force way of doing this is evaluating every possible split option that we have in our data set for example consider time we can just go through one record at a time and split based on each value we observe in this table so for example we take this first value of 721 am and we can split our data based on time being less than or equal to 721 am and the resulting split would look like this so here we have a sample with just one record this first one here and then we have everything else in this other sample here and then we can evaluate this split option by Computing its Genie impurity so basically what I mean by that is we calculate the genie and purity of this sample and the genie impurity of this sample and then we take their weighted average so here we just have one class so that is actually minimum impurity of zero and then this one is a bit of a mix so it's going to be pretty close to maximum impurity and then we will wait the average by the number of Records in each sample so this one will have a very low weight because it only consists of one record and then this one will have a very high weight because there are a lot of Records in this box over here so this split will give us a number corresponding to its genium Purity and then we can just continue this process and so now we split on 8 47 am calculate the average genium Purity we split on 9 30 am calculate average Genie impurity and so on and so forth for every possible value of time in this data set and then we do the same thing for amount of sleep we look at our first option which is 5.5 we get something like this 5.9 595 and so on and so forth for every single value we observe in our data set and so let's say after doing this and calculating all these average genium Purity values we discovered that the split option of sleep less than or equal to 6.75 hours is the optimal value this gives us the smallest Genie and purity of all the different split options that we observe in our data set and so now notice this node over here is pure it has minimum impurity so it doesn't really make sense to split this sample further but on the left hand side we still have some impurity in this node and we can do additional splits so let's do that here so now we have a smaller data set so instead of starting with all the data in our table we just have a subset shown by this smaller table over here and then we just repeat the same exact process as before we evaluate every split option let's say that after evaluating all the split options we discover that splitting on time less than or equal to 145 PM gives us the smallest Genie impurity and now notice again we have a pure node here but we still have some impurity here so we can just keep splitting the data until every single node is pure so meaning every single node just has a single class in it and it has Genie impurity equal to zero so at first while this might sound great you might think oh we can have a perfect classifier we can have a decision tree that is absolutely perfect however this is not such a great idea because this brings up a very well-known problem in machine learning known as the overfitting problem and overfitting is when you learn a machine learning model based on some data set but your model becomes over optimized on the data set it was trained on and when you try to apply that model to new data that it's never seen before you'll find that your model is actually very inaccurate and so instead of allowing our decision tree to grow without end and become hyper optimized to our data set we can control the growth of our decision tree using what are called hyper parameters put simply hyper parameters are values that constrain the growth of our decision tree and so just to look at a few examples let's say we have this original decision Tree on the left hand side but we find that it doesn't generalize well to different data sets and we actually want to tune it to this simpler decision Tree on the right hand side so in order to do this we can actually use hyper parameters and so here I'm going to show three different hyper parameters we can use to go from this original decision Tree on the left to the tuned decision Tree on the right so first we have the maximum number of splits so in the original decision tree you see that we have two splits happening but we could have easily constrained the size of this decision Tree by setting the max number of splits equal to one another hyper parameter we could have used was the minimum Leaf size so in the original decision tree we have a minimum Leaf size of two but if we would have set the minimum Leaf size to something like five this additional split could have never happened and then finally we could have controlled the number of splitting variables so in the decision tree from the previous slide we split on both hours of sleep and time of day but if we set the number of splitting variables to 1 decorative constrained our decision tree to the sex and so the key point is hyper parameter tuning can help avoid this overfitting problem and improve your decision trees generalizability so its ability to perform well on new data and then as a final note although this is a very widely used way to develop decision trees this is not the only way to develop decision trees and I talk a little bit more about alternative strategies for developing decision trees in the blog associated with this video so if you're interested in that be sure to check that out okay so with the theoretical Foundation set let's dive into a concrete example with code and data from The Real World so here we're going to do sepsis survival prediction using decision trees and so here we're going to use the scikit-learn python Library which is a very popular machine learning library in Python and then we're going to use a data set from the UCI machine learning Repository all this code that I'm going to walk through here is available in the GitHub repository which I will also Link in the description below the first step is we're going to import some helpful python libraries so pandas is going to help us with formatting our data numpy is helpful for doing some math and calculations we use matplotlib to make visualizations we import several things from sklearn and then finally we're going to import this smote function to help balance our data set which we will talk about here soon okay so with our libraries imported we can read in our data set so with pandas this is just one line of code and the CSV file used here is available at the GitHub repo as well as two additional CSV files that can be used for validating our decision tree okay so with our data read in we can plot the histograms for every variable in our data set so here we just have four variables the age of the patient whether the patient is male or female the number of sepsis episodes that the patient has experienced and then finally the outcome variable which is an indicator of whether the patient survived or died and so the first thing that we should notice here is that we have a imbalanced data set so what that means is we have a lot more patients that survive than that diet well this is a good thing from a human perspective this is not a good thing from a model development perspective because if we train our decision Tree on this data directly basically what will happen is that our decision tree will overestimate the a live class and underestimate the dead class and so one way we can correct this is using smote which stands for synthetic minority class over sampling technique I think I got that right and it's basically a way to over sample the minority class to make it more Equitable with the majority class and ultimately reduce bias in our decision tree so this is pretty straightforward so here we're just grabbing the predictor variable names and the outcome some variable name here we store the predictor and outcome variables into two pandas data frames namely X and Y and then finally with just one line of code we can use smote to over sample the minority class and then we can plot the results using matplotlib and look at that we have a more balanced data set all right so now that we have balanced our data set we can create our training in testing data sets and so basically the point of this is our training data set will be used to grow our decision tree and then the testing data set will be used to evaluate its performance and so here we use the 80 20 split so 80 of the data is used for training 20 is used for testing and then with that growing the decision tree is very straightforward we can do it with just two lines of code as we do here so first step is we initialize the decision tree classifier and then the second step is we fit our decision tree to our data and then that's it so we have our decision tree we can take a look at it using this built-in functionality inside like it learn and this is what it looks like needless to say this is a very big decision tree and it's hard to think that a doctor or any medical staff will be able to interpret this decision tree to extract anything meaningful but let's just put that point aside for now and evaluate our decision trees performance and so the way we could do this is using a confusion Matrix and so just looking at this one on the left what this is showing is the number of troop negatives true positives false positives and false negatives so in other words this is just comparing the decision trees predictions to the ground truth and so I don't want to get into too many details of interpreting confusion matrices and whatnot for this discussion I'll say when it comes to confusion matrices you generally want to maximize the diagonal elements and minimize the off diagonal element what that means is we want our predictions and the ground truth to agree as much as possible and we want them to disagree as little as possible possible so we see for both the training and testing data sets the performance seems reasonably well and then another way to evaluate performance is using three different metrics namely Precision recall and the F1 score which are defined by these equations on the left here so Precision is basically the number of true positives scaled by the sum of the true positives and false positives recall is a similar kind of thing but it is scaled by the number of true positives and false negatives and then the F1 score is the harmonic mean of the two and so in this case I'd say the Precision is something we care more about than recall because in this context we probably care more about false positives than false negatives and so the reason being is a false positive corresponds to the case where the decision tree predicted that the patient would survive and they did not and so for using this decision tree to quantify patient risk then there's a lot more downside to predicting that a patient would survive five that didn't then predicting that a patient would die who doesn't and so clearly which one of these metrics you want to look at and care about is highly context dependent sometimes you care equally about false positives and false negatives sometimes you care more about false negatives sometimes you might care more about false positives like the case here and so which metric you use to evaluate your model depends on the problem and the context you're looking at and then here's a handy function available in the GitHub that generates all this okay so coming back to this massive decision tree from a couple slides ago this brings up once again the overfitting problem so while this decision tree might work reasonably well on the data set here a decision tree that looks like this is prone to overfitting meaning it may not generalize well to new data sets and So to avoid this problem we can use hyper parameters and so here we're just going to use one hyper parameter which is the maximum depth and so here we're just going to set that equal to three so we ensure that the decision tree doesn't get too many branches setting this is super easy with sklearn we just pass this input argument into our decision tree classifier and then we fit our model just like we did before and then outcomes our tuned decision tree and so plotting out the decision tree it looks like this and already we can see this is much more interpretable we can actually read the text here and so just kind of looking at all these different splitting nodes we're seeing the age predictor is appearing a lot so this is indicating that age seems to be a very important risk factor when it comes to sepsis survival prediction and then also right here we're seeing that sex is playing a role which is a little surprising that we're not seeing number of episodes and surely if we were to increase the max depth or do some other hyper parameter tuning we would see that variable appear in additional splits okay so our hyper parameter tuned decision tree seems more comprehensible but how does it perform and so we can once again look at the confusion Matrix and those three performance match tricks and surprisingly the Precision is actually a little better for this hyper perimeter tune decision tree than the fully grown decision tree but notice that the recall in F1 scores are significantly lower than what we saw before but I would say in this case we may not care about that because Precision like I was saying before might be the metric that we're really trying to optimize in this context because we likely will want to wait false positives more than false negatives so that was a tremendous amount of information if you still want to read more check out the blog published in towards data science on medium be sure to check out the GitHub repo and steal the code and train your own decision tree model with your own hyper parameter optimizations and if you enjoyed this content please consider liking subscribing and sharing your thoughts in the comment section below and as always thank you for your time and thanks for watching
CTu8JNLq5ZU,2023-01-28T15:06:57.000000,5 Reasons Why Every Data Scientist Should Consider Freelancing,are you feeling stuck you got the degree you got the job you're working in data science you're killing it but for some reason you don't feel like you're progressing in life you're not growing you're not developing the skills that you want to develop or maybe you haven't even gotten the job yet you're desperately applying the jobs you're getting scared all the tech companies are laying people off what am I supposed to do about this how am I supposed to get a data science job with just a bachelor's degree with my Master's Degree with my PhD even if you resonated with any of these random points I just spat out at you then this video is for you hey everyone I'm Shaw and as someone who has worked in data science as both a freelancer and a full-timer I just wanted to share a little bit of my experience for those pondering and reflecting on their data science journey and trying to figure out where they want to go with it toward that end in this video I'm going to break down five reasons why every data scientist should at least consider freelancing so for my view these points are beneficial to anyone in data science but perhaps especially so for those just getting started in my personal experience freelancing accelerated my development as a data scientist and it played a big part in helping me get my current full-time role as a data scientist and even if your goal isn't to get a full-time gig at a company freelancing can serve as a main source of income or even give you insights into different industries that might help you develop a new business or a minimum viable product so if you like this content and you want to see more about data science productivity entrepreneurship go ahead and hit that subscribe button right now so the YouTube algorithm will know that you want to see this face on your computer screen and with that let's get into the Five Points okay so the first reason why every data scientist should at least consider freelancing is to work on new problems one of the greatest powers of data science that I personally just love so much is that data science is so often context agnostic so what do I mean by that basic basically all I mean is you can take one method one Technique One Piece of code and apply it to multiple different use cases so like a very simple one is logistic regression you can use logistic regression to solve binary classification problems which comes up in credit risk modeling will the person I'm giving a loan to pay me back or analyzing customer retention what's the probability that our customer will continue our services next month or even marketing analytics what's the probab ability that a user will buy our product after they watch our ad and so these are completely different context we're talking about credit risk and financial services lifetime customer value with retention analysis and then we're talking about ads and marketing with that last Point completely different context but you can use a single data science approach to solve all of these problems and these are just three off the top of my head there are countless use cases and applications for logistic regression or any common data science approach and so all that to say when it comes to freelancing you have the opportunity to use this basic toolkit in a wide range of contexts so when I was freelancing I was working as a graduate research assistant in the physics department but through my freelance work I gained exposure to different fields so one example was trying to classify sepsis subphenotypes so basically subtypes of sepsis using unsupervised machine learning techniques that I've used in count other contexts so that's one thing to consider when thinking about freelancing and data science you have the opportunity to work on different problems leveraging your experience and the skills that you've acquired in new contexts and it kind of enriches your understanding of those tools so every time you use a technique in a different context it helps you build an intuition of what else you can use that same method for so reason number two is developing soft skills so when you're freelancing you're really on your own you have to figure out how to put yourself out there how to get clients and how to communicate with a diverse set of people and so when you're freelancing you're trying to find gigs you often have to network you have to talk to people you have to connect with people and this really forces you to develop your soft skills you know sending out cold messages email etiquette talking to people at networking events reaching out to people on LinkedIn responding to potential clients reaching out to you because they see some work that you've done or they came across your profile while on some freelancing website like upwork or Fiverr and so all these interactions all these reps really allow you to develop these soft skills that you may just not have as much opportunity to develop in a full-time role where the work is more delegated to you and is more stable and you're typically interacting with the same handful of people constantly as opposed to in a freelance role you're constantly interacting with new people and brushing up on those skills so the third reason is fine-tuning your pitch and this has some overlap with the second reason it comes down to your ability to communicate and connect with people but fine-tuning your pitch is really about selling yourself and what I mean by this is fine-tuning your resumés your cover letter or proposals and your interview skills so this is another area where full-time roles and freelance gigs have a big difference and It ultimately just comes down to timeliness so for the full-time role you know you submit your resume and cover letter you spend all this time on it but it's not uncommon to not hear back from that application for weeks or even months sometimes so it's really hard to kind of gauge how effective your resume and cover letter were at conveying your skill set and experience but on the flip side in freelancing the time scale is just much faster if you apply to a gig let's say on a site like upwork the feedback is typically much quicker if someone wants to work with you you will a lot of times hear from them within a few days and if you don't hear back from them in a few days that probably means they moving forward with other candidates or the job's no longer relevant or something like that and so ultimately what this means is in freelancing as opposed to full-time roles you really can get a lot of reps in on your resume and cover letter and get much faster feedback and so what this allows you to do is fine-tune your resume and cover letter to convey your skills and experience more effectively and this is something I definitely benefited from so I was freelancing in grad school so constantly fine-tuning my resume and my cover letter and then eventually when I graduated and decided to apply for a full-time role my resume and cover letter went in a pretty good spot and I could just leverage what I'd learned from freelancing to apply to the full-time gig so I would say to anyone trying to break into data science you know you just graduated or you're about to graduate I would recommend freelancing even if you don't get any gigs and don't get any work through it at least you get these reps you get to fine-tune your resume and your cover letter and hopefully your interview skills through chatting with potential clients and you can leverage this experience and these reps and the feedback for applying to a full-time gig but overall the skill set of selling yourself being able to communicate your skill set and how your experience and skills are relevant to solving other people's problems is a very valuable skill set to have and essentially being able to sell yourself and your ideas is something that will be valuable in whatever context you find yourself in reason number four is flexibility and autonomy and so so this is one of the greatest benefits of freelancing and I feel one of the main reasons why people are so attracted to it in freelancing you essentially choose what you work on because you choose the clients that you work with and moreover freelancing gigs are typically on a much shorter time scale than full-time roles so you could be working with a client on a month-by-month basis and it could be going great for 6 months but then at a certain point the work May no longer be relevant or getting overloaded on contracts with a handful of other clients and then you have the option and opportunity to reduce your workload or refocus your efforts toward a specific type of work and also you don't just get to choose what you work on but you typically get to choose where you work how you work when you work and this is something that a lot of people have value in people who greatly value their autonomy their flexibility their freedom you know maybe they don't want to be bound to a certain city they want to be able to travel and you know this was something that got big during Co you know people were getting gigs online or during remote work they were living for months in different countries you know living in Europe or South America or Asia or something like that and so for people that that lifestyle is appealing to them freelancing is a great option for that okay so the fifth reason is networking and so I kind of touched on this before but here what I'm specifically talking about is building new relationships and new connections and so through my freelancing I've met a wide range of people that has given me insight into Worlds that I didn't even know existed so I've worked with medical doctors clinicians with people working in Special Forces military police officers business people you know so many different walks of life and backgrounds and has really enriched my own experience and my understanding of the world which I find a lot of value in relationships and learning from people is something I give a lot of weight to so this is one aspect of freelancing that I really enjoy okay and if those five reasons were not enough to make you consider freelancing in data science I've got two bonus tips to share so the first bonus tip is money so even if you don't really care about developing your technical skills expanding your experience and Horizons developing your soft skills building new relationships what else did I talk about fine-tuning your pitches your ability to sell yourself all these different reasons you could always just do it for the money and most of the time freelancing gigs are much more lucrative than full-time roles so just speaking from my personal experience before I entered into my current full-time role I had two offers on the table I had the my current role and I had a essentially a contractor role which could have been full-time and just comparing the pay of the two roles the contract role paid almost twice as much as my full-time gig I would say 80% yeah paid about 80% more than my full-time gig which is a lot of money I'm just saying that to give you an idea of how much more you could get as a freelancer as opposed to a full-time role and to those who are saying like oh freelancing is so great why didn't you take that why didn't you take the money and that was just a personal decision for me when I had graduated I had done the freelance stuff I'd worked in research but I never worked at a large company as part of like a big data science team the biggest team I had worked with was my research team which was about 12 people and I work on a data science and analytics team that's I want to say like 100 people if not more and so for me the reason I went with the full-time role is because it was a new experience for me it was also the opportunity to learn from other data scientists and data analysts that have been working in the field much longer than I have and then the last thing I'll say about the money is that you know the great thing about freelance is that you can customize the freelance workload so you can definitely be a full-time freelancer and reall the benefits there but if you're just trying to make some extra cash on the side and you have a full-time role you can probably just pick up one or so contracts every so often if you just want to make some extra cash on the side and the second bonus tip is that freelancing gives you options if you have freelancing experience or you've done it in the past you always have that option on the table so say you're working full-time and you want to make some extra cash you can always just go to freelancing or kind of given all the recent Tech layoffs it's kind of a scary thing you could just wake up one day and your full-time employer says we don't need you anymore or we don't see the value in data science anymore and they lay you off now what are you going to do well if you're freelancing on the side or a freelance in the past you have an immediate thing you can fall back on until you can either work up your client base or find another full-time role okay so that's basically it so in this video I give you five reasons why every data scientists should at least consider freelancing with two additional bonus tips and so if you enjoyed this content you want to read more take a look at the blog associated with this video published in towards data science on medium if you enjoyed this content please consider liking subscribing and sharing your thoughts in the comments section below and as always thank you for your time and thanks for watching
O72uByJlnMw,2023-01-14T15:08:23.000000,Causal Effects via Regression w/ Python Code,hey everyone welcome back my name is shahen and today we're going to continue the series on causal effects and talk about causal effects via regression this will actually be the fifth video in a larger Series so if you're just joining in now check out the other videos to get a larger context of what we're talking about here so the title of this talk is causal effects via regression so we might ask ourselves what is regression and regression is very simple it's a very widely used technique and Science and business economics progression is simply a way to learn relationships between variables using data and one of the most popular regression techniques is linear regression which you may have even seen back in high school in linear regression we use data to develop a linear model between two or more variables so a super simple example is if we have a variable X and we want to find its linear relationship with another variable y we can just Express the relationship through this equation so y = Theta x + B we know X and Y so through the regression process we actually learn values for Theta and B that best fits our data and so the result of a regression technique is what we call a model so a model is essentially something that we can use to make predictions so this is an example of a linear model so if you give me a value for x X I can make a prediction about the value of y so where the causal effects come in is if we have a linear model like this and we have a variable X and a variable y we can interpret this coefficient Theta as the causal effect of X on Y and so this is an important note here because if you've been watching previous videos of this series you might notice that this is a fundamentally different way to define causal effects as we've seen in past videos so namely before we defined the causal effect through the average treatment effect which was essentially the difference in mean outcomes for two different groups a treatment group and a control group here where xal 1 is representing the situation where someone receives a treatment like taking a pill and x equal 0 represents the situation where someone does not receive a treatment or does not take a pill for example so that's an important thing to know so in this video I'm going to talk about three regression based techniques for estimating causal effects so the first is linear regression which we've kind of already touched on but I'm going to talk more about in the next slide we have a more sophisticated technique called double machine learning and then finally we have another popular technique called metal Learners so for linear regression as we've seen here is where we train a linear model to predict the outcome variable Y in terms of the treatment variable X so again we have a linear model y = Theta x + B and so just defining some of these things Y is our outcome variable and X is our treatment variable so X could be whether someone takes a pill or not and Y can be an outcome such as headache status or something like that and then like I mentioned earlier we interpret Theta to be the causal effect of X on Y and then here we can interpret B to be an error term and so this is a very simplified view of the causal effect of X on y but we can go a step further and include confounders in our linear model so what do I mean by that so in this context a confounder let's call it Z is something that influences both X and Y so the way we can handle this situation is we can include our confounder Z in our linear model for y and then we can additionally write out a linear model for Z in terms of X or equivalently a linear model of X in terms of Z we can just move some terms around and get that equivalently but what this allows us to do is plug in our equation for Z into our equation for y and get a new equation for y that does not include our confounder and this gives us a different equation for our causal effect so no longer is our causal effect simply the coefficient of x in our linear model for y but it's got this extra term here which is representing the impact of the confounder and so this even still is a simplified example and for those who are interested there's a really nice book chapter by Andrew Gilman and Jennifer Hill called causal inference using regression on treatment variables which I will link in the comments and you can find the link in the blog associated with this video and I highly recommend this chapter it was a really helpful resource for me okay so linear regression is a very simple and easy to understand technique with which is helpful when using it in practice however the downside of using it is that since it is such a simplified technique it may not accurately capture the relationships between our variables of Interest so for example if X and Y are related by a non-linear relationship even a quadratic relationship or cubic or sinusoidal or whatever it might be and so for these situations we can turn to more sophisticated techniques and one such technique is called double machine learning and so the idea a here is we're going to estimate the treatment effect using a three-step process in which we develop two separate machine learning models and so double machine learning is a bit of a daunting technique at least it was for me when first hearing about the idea I came across the paper it's like 70 pages and just kind of reading through it's a lot of math a lot of abstract ideas but upon reading through the paper a couple times and then watching a really nice talk given by the first author uh Victor uh I'm not even I'm going to butcher this a really nice talk by Victor on YouTube which I can also link down below this whole process that may seem very daunting and complicated on the surface can be broken down into a simple three-step process so the first step is we train two regression models the first we can call the outcome model and the second we can call the treatment model so what this notation is representing f is a machine learning model that takes in covariant values and estimates the outcome value associated with those covariates and similarly G hat takes in covariant values and estimates the treatment value associated with those covariants so we trained two machine learning models fat and G hat and we can use any machine learning model we like we can use linear regression we can use neural networks we can use decision trees you know whatever we like which is one of the big powers of this approach okay so the second step is we compute residuals so essentially what that is is the difference between the True Value Y and the estimated value from fat and similarly the True Value X and the estimated value G hat and then this gives us our residual values u and v for each model respectively and then using these residuals we can compute the treatment effect directly and so we use this equation here which at first may seem like it's a lot going on but here we have essentially just three things we have our residual value for the treatment model V here and here we have our residual value for the outcome model U which is this thing and then we have our ground truth treatment values that we observe and so I is just indexing our samples and then n is the number of Records in our main sample so one important detail which I kind of left out of this three-step process but is critical to doing double machine learning is a process process called crossfitting so what we do in crossfitting is essentially we split our data into two subsets we have our main sample and auxiliary sample and then we use the main sample to train one of our models and then we use the auxiliary sample to train our other model and then once we do that we switch so if we used our main sample to train fat and our auxiliary sample to train G hat we switch we then use our main sample to train G hat and the auxiliary sample to train fat and so through each each of these steps like step two and step three these will give us a treatment effect and then so we can aggregate these two treatment effects to give us a final causal estimate this may seem like a unnecessary extra step but it's actually critical because some of the assumptions that it allows us to make it actually helps make this equation simpler than it would be otherwise and so if you want to learn more details about that I talk a little more about it in the blog and then the authors give a more detailed explanation in the paper that you can find on the archive here so the final regression based approach that I'm going to talk about are the so-called metal Learners so what these do essentially is use regression models to simulate unobserved outcomes and estimate causal effects so there are several types of meta learners but in this video I'll just talk about three which are the T learner or two learner the S learner or single learner and the X learner so what do these things mean so we'll start with the t- learner so the process of using t- Learners to estimate causal effects can be broken down into a two-step process so we start with our treatment group data so yxal 1 is representing outcome values for people who received treatment and ZX equal 1 are the covariates for people that received treatment and then similarly we have our control group which are the outcomes for people who did not receive treatment and covariates for people who did not receive treatment okay so once once we have our two groups our treatment and control groups the first step is to train two machine learning models that is estimating the outcome Y for the treatment group in terms of their co-variance and then we similarly estimate the outcome value for the control group in terms of covariant so this gives us two machine learning models fat and Gat and then the last step is we compute the causal effects so here we Define the individual treatment effect which we actually saw in the first video of the series titled causal effects but defining it in this metal learner framework we have the individual treatment effect of the I record is equal to the outcome value estimated by the treatment group model for the I record minus the estimated outcome value from the control group model for the I record saying it another way this is the estimated outcome for the I individual if they receive treatment minus the estimated outcome value of the individual if they do not receive treatment and so this difference gives us the individual treatment effect which we can then aggregate in this way to get the average treatment effect and then going a step further we can Define the conditional average treatment effect which is not defined for any particular individual but for a particular subpopulation defined by the covariant values that we plug into this equation okay so next we have the S learner so with s Learners we don't need to split our data into to a treatment and control group we can just use all data from all units and then we use this data to train a single model we'll call it fat which estimates the outcome variable y using both the treatment variable and our covariates so notice we're not restricting the values of the treatment variable here it can take all different levels so one upside of this is that we're not restricted to Binary treatment variables 01 we can actually have multi level treatment variables here okay and then in a very similar way we can compute the effects so we defined the individual treatment effect in a similar way as we did with the t- learner so the individual treatment effect for the I record is defined as the estimated outcome when the i unit receives treatment minus the estimated outcome when the i unit does not receive treatment and so instead of having two separate models one for the treatment group and one for the control group we can modulate our outcome value pred itions by manually setting our treatment input to be one or zero or whatever treatment value we like and then we can aggregate individual treatment effects using the average treatment effect exactly how we did before and then we can Define the conditional average treatment effect in a very similar way as before but now notice we're manually setting our first input value as one for the first term and zero for the second term and finally we have the X learner and so this has over with the t- learner process we saw before but it takes things a step further so first we train two models one for the treatment group and one for the control group just like we did with the t- learner then we impute outcomes and compute the individual treatment effects and so we have like this Crossing term here we use the control group model to impute outcomes so these are unobserved outcomes for the treatment group and so since the treatment group got the treatment we know their outcomes given treatment but we didn't observe their outcomes in the reality where they didn't take treatment and so that's what this model G allows us to do it allows us to simulate these unobserved outcomes and compute the individual treatment effect which we're calling di1 here and then we do a similar thing for the control group so since we observed what the outcome values are for the control group given that they don't take treatment we simulate what they would have been if they did receive treatment using our treatment group Model F and then the next step is we train models to specifically predict our individual treatment effects and so we'll call these models to hat one and to hat zero and so this will use covariant values Z to predict our individual treatment effects D1 and d0 and then finally we can Define our conditional average treatment effects estimator by combining tow hat one and Tow hat zero using a weight function W so in the paper by consel at all they suggest using a propensity score for this weight function W and we talked about propensity scores in a earlier video of this series which I will link down below for those who are interested and then here the conditional average treatment effect is equivalent to to so that's just how we Define it and so if you want to learn more about Metal Learners I found this resource very helpful the paper titled metal learners for estimating heterogeneous treatment effects using machine learning available on the archive I also talk more about this in the blog post associated with this video okay so now we're going to dive into a concrete example with code so this is a example that I've probably overused at this point but we're going to estimate the treatment effect of grad school on income so here we're again going to use the python doy Library which you can find at this link here and then the data source is the UCI machine learning repository so here's the data source it's a open access data source so you should be able to grab the data from here or additionally I have a kind of pickled version that is a pandas data frame available on the GitHub okay so first step is we import our modules and load in our data and so importing pickle so we can bring in our data Eon ml to do the double machine learning stuff and I think the metal learner stuff but we'll find out soon and then doy to do a lot of the heavy lifting for the causal effect calculation and then we import things from sklearn to help us train our machine learning models okay so this is a critical step specific to the dwide library we have to Define our causal model which essentially here is defining our treatment variable or outcome variable and our confounders or common causes and then we Define an es demand which is a recipe that tells us how to define our causal effects with this ground set we can go through each of the three techniques that I discussed earlier so we start with linear regression and the doy Library makes it super easy we essentially can get the linear regression console estimate with one line of code so we input our s demand and then we just say the method that we want to use so we use linear regression and then back door and if you want to learn more about the back door Criterion I talked about that in a previous video of this series but through that the result of this is the average treatment effect based on linear regression is 0.297 which is kind of in the ballark what we've seen in example analyses using this data set next we have double machine learning the very sophisticated and complicated approach relative to others but here we just again can get the double machine learning estimate through one line of code so we have our estimate and then we're defining our method name which is double machine learning which we're grabbing from the econ ml library and then we're defining our method parameters so here I just use linear regression for everything because the data here are so simple we only have three variables two of them are binary so linear regression seemed to give the best results here I would say double machine learning is a bit of overkill for this simple example but I think it would have a lot more utility in more complicated contexts and then here we see the average treatment effect is very similar to always out the linear regression approach which isn't surprising since we use linear regression for all the sub models here and then finally we have the X learner so this is one of the metal Learners we talked about and again it's the same type of thing we put in our estimand and we Define our method name which is from the econ ml Library X learner here and then we use a decision tree for the two submodels in our X learner and then that spits out an average treatment effect of about 0.20 okay so there's a bit of variation and I think that's one of the upsides of libraries like doy is that we can very easily and programmatically try a suite of different causal effect estimates for our data set and then we can have a wider distri distribution of average treatment effects or causal effects that we can use to inform our analyses so that's it for now if you enjoyed this video please consider liking subscribing and sharing with others if you have any questions comments or suggestions for future videos please share those in the comments section below if you want to read more about this subject check out the associated blog on medium additionally feel free to steal the code from the GitHub repository link below and as always thank you for your time and thanks for watching
-5c1KO-JF_s,2022-12-15T13:33:49.000000,Smoothing Crypto Time Series with Wavelets | Real-world Data Project,hey everyone welcome back my name is Shaheen and in this video I'm going to be talking about how we can smooth Financial time series data using something called wavelets so this is a little different than past videos that you may have seen on my YouTube channel what I usually do is videos in more of like a lecture type format where I start talking about Theory and Concepts and then usually finish up with a concrete example with code but here I wanted to do something a little different a real life use case came up when an old friend reached out for some help on a project that he's working on and I thought it was such a wonderful example of what data science looks like in the real world so in this video I want to walk through the background and the problem that my friend was running into and walk through my thought process and how I arrived to the final solution for this problem so I'm going to go through this Jupiter notebook which kind of has step by step the the whole story of what happened in the this specific project and you can find this Jupiter notebook along with the data used in this example in the GitHub repo that is linked Below in the description so if you want to follow along or use these solutions for a problem that you're facing feel free to download the code there okay so let's get into it so like I mentioned the whole setup for this problem was my friend approached me because he was running into an issue in a project that he's working on so the project was actually a tool that he's developing to automatically trade cryptocurrencies and he has some strategy for executing these Buy sell decisions that I don't fully know the details of but he reached out to me because his strategy was all based on time series data that looks something like this and so the problem that he was facing is that his strategy was starting to give undesirable results due to the volatility of this signal so I mean just looking at this we we see like these crazy fast oscillations in the signal so if your goal is to develop a automated trading system you want it to be robust in a wide range of situations and so I can see just looking at how noisy this time series is how a strategy might break down so this brings up a really good point it highlights a mistake that a lot of data scientists make early on in that they're almost too eager to jump into the the coding jump into the technical details jump into the math the techniques the algorithms and don't take the time to take a step back and really ask themselves what is the problem we're trying to solve why are we interested in this problem what is it that we are truly after or what is it that the client is truly after so kind of in line with that my initial thoughts from this ask is what is this data it's very noisy and it turns out to be you know quite a specific signal that's being extracted from cryptocurrency prices it's something like the time average z-score over the past 45 minutes of a currency's trading price so if that doesn't make any sense don't worry it's not going to be critical to the larger point of this use case okay so my second thought was why do you need to use this particular time series so my initial thought is if you're relying on a signal for your strategy and that signal is fundamentally unreliable or noisy are there other data that we can look at and use for our strategy and so in this case there probably are however my friend had already developed his strategy that had worked well for him in the past based on this data so that's the reason he wanted to stick with this particular signal so even if these questions like in this case didn't change the direction of the analysis it's good to make sure you have an understanding of the context and the bigger picture and where your collab operator is coming from and asking for your help with a data science project after some conversation understanding what this data is getting a better idea of why he wanted to use this particular time series and at least in the short term kind of ruling out alternative strategies my first thought was well why don't you just apply a moving average so that's a very simple way to take a noisy signal like this and make it more smooth and so that brings us to the first solution and as it turned out this was a solution that my friend had already tried and it was still not giving him the results that he was looking for and so you know without knowing too many details about his technique there are just three possible issues that kind of jump out to me with this solution so looking at this orange signal here we still see that there are rapid changes in particular sections so while this section looks pretty smooth we can look back a little bit and we can see we still have of time ranges where there's rapid oscillation and that is conceivably something that'll make a automated Buy sell strategy very unstable the second thing is there are time shifting artifacts when we do moving averages so for example we can see that we have a peak in the original signal right here however in the moving averaged in the smoothed version of the signal using the moving average that Peak has actually shifted slightly to the right and so we have these time shifting artifacts anytime we do a moving average and it's kind of like this trade-off you know you can make your moving average window very small and you'll have less of these time shifting artifacts but the signal is going to stay noisy so we can actually look at that a little bit so if I made the window size 10 and run this we can see that the orange signal sticks pretty closely to the blue signal we don't have the time shifting artifact as much then on the other end we can make the window size very large and now we have a really smooth signal but now the time shifting artifacts are very large so this is likely not desirable especially if you have a program that's executing Buy sell decisions automatically it might be a little late to the party and then the third issue is the optimal window size for the moving average is likely Dynamic so just as we saw like this window size dramatically changes how our smooth signal looks and anytime you have to fix a parameter like this it's easy to let's say over fit to a example time series that you use in development so in other words what that means is we might pick a window size of 25 and agree that it looks good on this data but then let's say tomorrow we get new data and then 25 doesn't give us the desired outcome for these reasons the moving average is likely not going to be a optimal solution for this specific use case and so the first thing that came to mind which was a complete failure which is funny because I was so confident that this would solve the problem that my friend offered to he offered to pay me for my time and I was like oh no no this is going to take like five minutes like don't even like it's not even worth it's not even worth it like the coffee you just bought me is uh is equivalent to the amount of effort I'm gonna spend on this problem and I was wrong and thinking through it a little more it kind of becomes obvious why this would fail so this is a great example of when expectations and intuitions can fail like the the gut reaction the gut instinct can a lot of times mislead you when you're in kind of new territory and so uh let's kind of walk through the solution so at a high level the strategy is we have this signal let's fit it to a polynomial so we're talking about like x x squared x cubed x to the fourth so on and so forth and we're just gonna fit it to enough polynomials that we capture the properties we care about of the signal and don't capture the noise essentially and so this was a complete failure so here we fit it to 15 polynomials so what that means is we had a constant term x to the first power x squared x cubed x to the 4 all the way up to x to the 15. we just do a polynomial fit and then we plot the fit and compare it to the original signal and so while this well the polynomial fit is a lot smoother than the original signal it doesn't capture any useful information from the underlying signal so this was a complete failure so in the face of this failure I had another thought I thought of course polynomials aren't going to work here polynomials want to shoot up to infinity or shoot down to negative infinity and here we clearly have a signal that is oscillating around zero so when I think of something oscillating around zero I naturally think of Sines and cosines which brings up the concept of a Fourier transform if you're not familiar with the Fourier transform is one of the most powerful and widely used techniques in Signal processing Math and Science and so basically what we're doing conceptually is we're taking a signal so our blue signal here and we're going to decompose it into a set of Sines and cosines of different amplitudes and frequencies and so notice this is very similar to what we did with the polynomial fit so in the polynomial fit we take the signal fill it fit it to a set of polynomials you can conceptually think of what we're doing here as taking the signal and fitting it to a set of Sines and cosines the result of that captures a lot more of the underlying signal but we still have a lot of noise this actually has even more noise qualitatively than the moving average solution so if I were to pick between the moving average solution and this solution I would pick the moving average because it is both simple and qualitatively better and so solution number two attempt number two was yet another failure and so now I'm starting to think more polynomials didn't work they smoothed the signal but they kind of smoothed it so much that we lost all the useful information the Fourier Transformer may have captured the signal but there's still a lot of noise is there something that's in between these things and that's what inspired using the third technique which is the wavelet transform what we're doing here is the same idea as with the polynomial fit and the Fourier decomposition but instead of fitting our signal in terms of polynomials or in terms of Sines and cosines we're going to fit our signal to a set of wavelets which are essentially these wave-like things that are localized in time and if you want to learn more about the wavelet transform and the 40 transform I have some videos on that I have some blog posts on that so that should be a good primer if you're completely lately unfamiliar with the technique and so here we do the same concept we kind of fit our signal to a family of wavelets and then we just drop these higher order these more detailed coefficients of the wavelet transform and something remarkable happens in that we have a kind of Goldilocks smoothing situation you know it's not oversoothed like the polynomial fit but it's not too noisy like with the Fourier transform fit and so I shared the solution to my friend he seemed really excited about it really happy with it so he's testing it out in his platform now and hopefully it works out well and so there's one key Point here that is one aspect of data science that comes up a lot that gets me really excited which is that so often data science techniques and solution are application or context agnostic so for those of you who did see the video I posted on the wavelet transform you might notice that the steps that we ran through to smooth this financial time series is essentially the same exact process we used to detect our peaks in ECG data it's a great feeling when you can spend a lot of time working on a project and then somewhere down the line like two years later you come across a different project and you can basically copy and paste code from a project you did in the past even if the context is completely different like with analyzing ECG data versus Financial time series data okay so that's basically it I've also created a user-defined function to implement this strategy and that is also available at the GitHub repo and then here's a zoomed in version of the time series so how would you approach this problem leave your ideas and suggestions in the comments section below if you enjoyed this video please consider liking subscribing and sharing with others and as always thank you so much for your time and thanks for watching
ASU5HG5EqTM,2022-12-02T13:54:12.000000,Causal Effects via DAGs | How to Handle Unobserved Confounders,everyone welcome back my name is Shaheen and in this video I will be talking about causal effects via dags or in other words directed acyclic graphs so this video is part of a larger series on causal effects in the previous video of this series we defined causal effects using something called the do operator and then this led us to the concept of identifiability so that's where we're going to start our discussion today so identifiability is all about answering this question can the causal effect be evaluated from the given data so one way we can Define causal effects is via the average treatment effect or ate for short and in the previous video we defined the ate using this equation and so I don't want to spend too much time on it because I broke this down in the previous video but here we have an expectation value for an outcome variable y given an intervention in a treatment so that's what this you 2 operator is representing subtracted by the expectation value of the same outcome but for a different valued intervention so from this view the question of identifiability reduces to the following can the Interventional distribution be expressed in terms of observational distribution so on the left here we have an Interventional distribution which we discussed in the previous video and this is what we're seeing in our definition of the average treatment effect and on the right here we have a observational distribution so a key point is in the real world the data we collect is usually an observational distribution so expressing these Interventional distributions in terms of the things we actually measure is what identifiability is all about and so we actually saw an example of this in the previous video where we had this dag on the left directed acyclic graph representing the causal connections between different variables in our problem and we saw that we could express this in Interventional distribution in terms of observational data alone and this is the result that we saw what allowed us to do this in the previous video is that this is a special type of causal model it is a so-called markovian causal model so these are models that satisfy two conditions one they have no cycles and two there are no unmeasured noise terms that simultaneously cause two or more variables so we can make this a bit more concrete with some examples so the first one here this is an example of a causal model or dag that is not markovian because it has a cycle as we can see here we have another example of a non-markovian causal model so even though there are no Cycles we have this unmeasured variable X which is simultaneously causing two variables A and B but then on the far right here we have a markovian model because there are no cycles and we have unmeasured variables but they are not simultaneously causing two or more other variables and so a key point about markovian cause models and why we care about them is that if our causal model is markovian then the causal effect is always identifiable and we saw this in the previous video via the truncated factorization formula however Things become much more interesting when we have these unmeasured unobserved confounders so let's take a look at a concrete example of this suppose we have this markovian model but then suppose Jimmy forgot to turn on the Z2 sensor so actually the Z2 variable is unobserved and so now this model is not markovian because we have an unobserved variable that simultaneously is causing two other variables we can equivalently express this model by removing Z2 and connecting Z3 and Y with a bi-directed edge and so we started with a markovian model but now with just missing information about one variable the model is no longer markovian and we can see that in two ways the middle one has this unobserved variable that causes two or more other their variables and then this far right model actually has a cycle in it because we have this Loop here but as we saw in the previous video Even though this model is not markovian the causal effect is still identifiable in this case and so identifying causal effects of non-markovian models brings up two graphical criteria the backdoor Criterion and the front door Criterion basically these are graphical tests that we can apply to a directed acyclic graph either markovian or not to answer the question of identifiability or in other words evaluate whether the causal effect can be evaluated from the data that we are given so I'm going to Define each of these criteria and give a concrete example for each but first we need to Define two concepts the first concept is that of a backdoor path so a backdoor path between two nodes say X and Y is any path that starts with an arrow pointing into X and terminates at y so looking at this example on the left here all the back door paths in this dag are listed here so we have X to Z1 to Z3 to Y X to Z1 to Z3 to Z2 to Y X to Z3 to Y and then X to Z3 Z2 to Y notice that when talking about backdoor paths the only arrowheads would pay any attention to are the ones going into X the second concept is that of blocking and so a path p is said to be blocked by a set of nodes zi if and only if two conditions are satisfied the first is p contains a chain and this is an example of a chain here or a fork and then this is an example of a fork here such that b this middle variable is an element in the set z i and so this is what we might intuitively think of as blocking we have some flow of information so to speak from a to c via the chain or the fork and if we remove b or adjust for it we essentially block the flow of information between a and C the second point is that P contains a collider or in other words an inverted Fork such that b in any of its descendants are not in z i this point is a little less intuitive at least it was for me when first learning about this so the way to think about this is that a and C in a inverted Fork are statistically independent however if we condition on B we generate a statistical dependence between a and c and so this is also known as berkson's Paradox so basically all the second point is saying is that we are not creating a statistical dependence between a and C by conditioning on it or including it in our set z i and so here are some examples using the dag on the left here so Z3 blocks the path X z3y so this one because it is the center node in this Fork here another more interesting example is that Z1 blocks this path here because again we have a fork but notice that Z3 does not block this path because Z3 is an inverted fork or in other words a collider so if we were to include it in the blocking set it would essentially open up a statistical dependence between X and Y okay so with those two concepts of a back door path and blocking we can finally Define the back door criteria so a set of nodes zi satisfied the back door Criterion relative to X and Y if no node in x i is a descendant of X so essentially no node in this set zi that we're considering comes after X and two zi blocks every backdoor path between X and Y so the way we can evaluate this for this example tag on the left here is we first write down all the back door paths between X and Y so we have X the Z1 Z3 to y z E1 Z3 Z2 to Y and so on and we choose a set of nodes that block each of these four backdoor paths and so in this example we have two sets that satisfy this condition we have Z1 and Z3 so we see Z1 blocks this path as does Z3 Z1 blocks this path but Z3 does not block this path Z3 blocks this path and Z3 blocks this path and then almost trivially the set Z1 Z2 and Z3 also satisfies the backdoor Criterion and so a set of nodes that satisfies the backdoor Criterion is called a sufficient set and once we've identified our sufficient set we can immediately write down the Interventional distribution using this equation here where I is indexing our variables in our sufficient set so here we can either use Z1 or Z3 or Z1 Z2 and Z3 and this is exactly what we saw in the example from the previous video we initially had all three variables in our X expression for the Interventional distribution but then magically we were able to remove Z2 so hopefully now you can see how that trick was done and it's because Z1 and Z3 satisfy the backdoor Criterion so there's really no need to include Z2 in this expression so next we have the front door Criterion a set of nodes say Z I satisfy the front door Criterion relative to X and Y if these three conditions are satisfied one zi intercepts all directed paths from X to Y to all back door paths from X and each element in zi are blocked by the empty set and three all back door paths from z i to Y are blocked by X so there's a lot to chew on here so we're just going to go through each condition one by one for this example on the left here the first condition is zi intercepts all directed paths from X to Y so the way we can evaluate this is by writing down every single directed path from X to Y and just inspecting which sets of variables inner intercept each of these paths as it turns out for this example it's really simple we only have two directed paths from X to Y so we have X to Z1 to Y here and then we have X to Z2 to Y so really there's only one set that satisfies this first condition which is the set Z1 and Z2 but we're not done yet even though this set satisfies condition one we still need to check it against the other two conditions so looking at condition two we need to evaluate whether all the backdoor paths from X to z i are candidate set of variables are blocked by the empty set so the way to do this is we write down all the backdoor paths from X to each variable in z i so here we have the back door paths between x and z i and here and then on the right hand side we have all the back door paths between X and Z2 and so just looking at this backdoor path we have X to Z3 to Y to Z1 we see indeed that this Factor path is blocked by the empty set because we have a collider in y and and then actually looking at every other of the back door pads the collider at Y is present in all of them so indeed all the backdoor paths between X and Z1 and x and Z2 are blocked by the empty set that's a check mark for condition two and then finally condition three we need to evaluate whether all the back door paths from z i to Y are blocked by X so again we just write out all the backdoor paths between each variable in z i and y so just looking at this path our way of z i to X to Z3 to Y and then we can immediately see that X indeed does block this path because we have a chain where X is the center and there are no colliders and this is the same exact story for actually all of these backdoor paths we see X is either a chain or a fork and so this satisfies our definition of blocking from before okay so all three of these conditions are satisfied by our candidate set Z1 and Z2 which means we can write down the Interventional distribution directly using this equation and so this equation has a bit more going on than what we saw for the backdoor Criterion because we have two different summations so to make things a little more concrete I'll write out this first summation for the example we have here so since this set satisfies the front door Criterion we can write the Interventional distribution like this so notice we write out this sum the first term is for Z1 and the second term is for Z2 and this x here will correspond to the X on the left hand side so this will be a fixed value based on the intervention that we are looking at but notice we have this other X this x Prime and what this is denoting is the summation over all possible values of X so for example if we had a binary treatment variable and X could take values of zero or one then we would do this summation over both possible values of X so we would write this out with X Prime equals zero and then X Prime equals one and so this equation would go from two terms to four terms okay so that's pretty much it there is a Blog associated with this material so if you're interested in learning more check that out there are some more details there that I didn't include in the video and if you enjoyed this content please consider liking subscribing and sharing so if you have any comments or questions please share those in the comment section below I find those very helpful for my own personal development and Learning Journey and as always thanks for your time and thanks for watching
dejZzJIZdow,2022-10-22T13:30:09.000000,Causal Effects via the Do-operator | Overview & Example,hey folks welcome back this is the third video in the series on causal effects in the last video we learned how to estimate causal effects with observational data via something called a propensity score while these approaches help us in the face of measured confounders there's still the problem of unmeasured confounders in other words variables that bias our estimate that we do not observe in our data in this video we will see how we can resolve this problem of unmeasured confounders to do this we will need to re-evaluate how we think about causal effects so with that let's get into it everything I'm going to talk about in this video concerns connecting observational data to Interventional data in other words connecting data that we might passively observe to data that we might measure more carefully through a randomized controlled trial or something similar and so this distinction between observational studies and Interventional studies was made in the previous video but just as a quick recap of what we're talking about here say we're trying to quantify the causal effect of a pill on headache status what a observational study would look like to investigate this is we passively observe a population of people with headaches some of them take the pill some of them don't take the pill and we just observe their natural outcomes with no intervention or influence on who takes the pill and who doesn't on the other side of this we have an Interventional study which is more akin to a randomized controlled trial where we carefully pick a population of people at random and then randomly split them into two groups an experimental group and a control group would give the experimental group the pill and we don't give the pill to the control group and then we can compare their outcomes so clearly observational data take less effort to measure while Interventional data take more effort to measure but as has been said in the past there's no such thing as a free lunch so even though I observational data are easier to capture takes more effort to estimate causal effects from them and even still it may not be possible to estimate causal effects from them we saw one way we can estimate causal effects from observational data in the previous video where we talked about propensity scores and in this video we're going to talk about an alternative strategy to doing this and so first we're going to take a step back and re-evaluate our definition of the average treatment effect which is what we've been using to quantify causal effects in this series formula one here is how we Define the average treatment effect in the context of a randomized controlled trial so what we have is this expression here and just quickly running through this e is denoting the expectation value y denotes the outcome variable X denotes the binary treatment variable x equals 1 means the subject took the pill x equals zero means the subject didn't take the pill so this first term is the expected outcome of the treated group because all the these people took the pill the second term is the expected outcome of the untreated group because they didn't take the pill so this is a very simple expression and if we have a randomized control trial we can obtain the average treatment effect in a very straightforward way but there's an implicit assumption here that people may not realize if just seeing this equation on paper or on a website or something so the assumption is that the treatment status is statistically independent of all other factors so that's why this equation is true for a randomized controlled trial that's the whole point of randomization so that who is in the experimental group and who's in the control group has no statistical relationship with any other factors but now we're going to look at an alternative way to formulate the average treatment effect and to do this we're going to use the do operator which we first saw in the previous video on causal inference so here we have a very similar thing we have this expected outcome of the treated group and we have the expected outcome of the control group but the only difference here is we now now have this do operator in each expression with the do operator represents is a physical intervention as opposed to on this side in Formula One this notation does not distinguish between a intervention and just a passive observation and so what we gain by expressing the average treatment effect in this way is that we are explicitly denoting that the treatment status is statistically independent of all other factors and so in the context of a randomized controlled trial these two formulas are equivalent but this isn't true in other situations in other words in an observational study if you use this equation without being careful about statistical dependencies you're likely going to get biased estimates of causal effects because there are likely systematic differences between the people that received the treatment and people that didn't receive the treatment okay and then as a final note in the previous video we talked about these propensity score based methods and there we were using this equation and we were using observational data so there the implicit assumption is that the propensity score based methods approximately remove the statistical dependence between treatment variables and measured covariates so that's what allows the propensity score based methods to work is that we're taking care of these other statistical dependencies okay so more on this do operator so the do operator again is a way to denote interventions and so this brings up another observational Interventional Distinction on the left here we have an observational distribution so this is what we saw on the left hand side of the previous slide in other words this is what we saw in Formula One it was the probability of Y given the variable X is observed to be X naught while in Formula 2 we had an Interventional distribution so that was the probability of Y given the variable X is artificially set to X naught I talk more about the do operator in the video and causal inference so check that out also this is a very good reference this is an introduction by Judea Pearl who invented the do operator and it's a big figure in the space of causal inference so often we don't measure Interventional distributions directly so what this would look like is doing the Interventional study doing the randomized control trial in other words doing the physical intervention and capturing the statistics however Formula 2 this alternative definition of the average treatment effect relies on the Interventional distribution so if we want to use this more general formula for the average treatment Effect one that's not just true in randomized controlled trials but is true everywhere it would be good if we can translate any Interventional distribution into observational distribution so translating the thing that we want to estimate in terms of things that we actually measure and that brings up the question of identifiability and so identifiability is all about answering the following question can the Interventional distributions be obtained from the given data so in the context of a randomized controlled trial we likely already have the Interventional distribution because that's what we painstakingly went through the process of measuring but in all other cases where we just have observational data what identifiability is all about is expressing the Interventional distribution in terms of observational distributions and so Pearl and colleagues developed a systematic three-step process for answering this question of identifiability okay so the first step in this three-step process is to write down the causal model and so taking the example from the previous video where we were estimating the causal effect of going to grad school on income this was the causal model that we had assumed age causes grad school and income then grad school causes income then the second step is to evaluate something called the Markov condition and so this consists of two parts the first is asking is the graph a cyclic so basically do any Cycles exist in this graph so if we start at Z and we follow the arrowheads there's no way we can get back to Z if we start at X we follow arrowheads there's no way to get back to X and if we start at y there's no way to get back to Y so indeed this graph is a cyclic the second part is that all noise terms are independent so basically the noise terms for age would be all factors that cause age that are not captured in our causal model similarly the noise terms for grad school would be all other factors other than age that drive someone's probability of going to grad school and then the noise terms of income would be all other factors other than age and grad school that have a causal effect on someone's income and so assuming that these noise terms are independent means that there's no cross connections between these external factors is this true in this case that is questionable but for the sake of this example we'll just assume that it is okay and finally step three we express the Interventional distribution in terms of observational distributions and so we can do this using the wonderful truncated factorization formula which expresses any Interventional distribution in terms of observational distribution so notice we have a do operator on the left hand side and there's no do operator on the right hand side okay but still on the left hand side we have Z and to estimate the causal effect of X on y we just want to isolate y in this conditional probability here so the way we can get rid of Z is by summing over it so basically setting Z equal to every value in our data set so Z equals one two three four five all the way up to you know 100 whatever we might have and then just evaluating this summation okay so now that we have the Interventional distribution in terms of observational ones we can compute the average treatment effect using our formula two from before and so all we do is just plug in one here stick it over here plug in 0 here stick it over here and then just compute the average treatment effect and so the truncated factorization model I showed in the previous slide was just for that specific example but it's a much more general formula as you can see here so I'm not going to spend too much time on this but this is from the introduction by Pearl and I also talk a bit more about this in the blog associated with this video so now the whole motivation of this video from the propensity score video was that propensity score based methods don't account for unmeasured confounders they only handle confounders or covariates that are included in our data so what can we do about unmeasured confounders so using everything that we've talked about in this video we can compute the average treatment effect even when we have data missing from our data set so we're going to run through this systematic three-step process again but now for a more complicated example so this is taken from the introduction by Pearl so suppose we have this causal model we have our treatment X our outcome Y and we have three covariates okay so step two is we evaluate the Markov condition we can see that this graph is acyclic there are no cycles and we will just assume that the noise terms are independent and then step three we'll Express the Interventional distribution so again we do this using the truncated factorization formula which is a lot longer now because we have five variables instead of three and then to isolate y on the left hand side we can just sum over variants but now suppose that Z isn't measured it's missing from our data set we can run through the same process again we write down the causal model we evaluate the Markov condition we express the Interventional distribution first with the truncated factorization formula then summing over covariates but now the problem is we don't measure Z2 we don't know these probabilities so what are we supposed to do so now we can just apply a little bit of magic and out comes the Interventional distribution without Z2 on the right hand side so what just happened here so what we just did there was the result of this expression here so this is the Interventional distribution via the parents of the treatment so on the left hand side we have the Interventional distribution of Y given the intervention in X and on the right hand side we just have observational distributions in terms of x y and the parents of X so the key inside here is we only need the parents of X to estimate its causal effect and so again this is from the introduction by Pearl referenced down here so that was a lot of information so I'm just going to try to recap some key points the first is given a markovian causal model with all the variables measured we can use the truncated factorization formula to express any Interventional distribution in terms of observational ones and thus we can express any causal effect using our new formulation the average treatment effect the second key point is that if we have unmeasured confounders we can always simplify the truncated factorization formula given a markovian causal model to include only the parents of X but now what if we can't measure the parents of X and so what that brings up is this notion of alternative covariate selection that's going to be the topic of the next video in this series so there we're going to further explore this notion of simplifying the trunk the factorization formula to only include variables that we measure or in other words to exclude the variables that we don't measure and then we'll get further insight to the magic that we saw a couple slides ago so there's a Blog associated with this video there's some details in there that I may have left out in this video and I'll drop a friend Link in the description so you can access the blog even if you're not a medium member and if you enjoyed this video please consider liking subscribing and sharing this content if you have questions please share those in the comments section below a big reason why I make these videos and write these blogs is for my own learning process and development and the questions that I receive is a huge help in this whole learning process and as always thanks for your time and thanks for watching
dm-BWjyYQpw,2022-10-14T13:37:55.000000,Causal Effects via Propensity Scores | Introduction & Python Code,hey folks welcome back this is the second video in a series on causal effects in the last video we learned some theoretical Concepts that underlie causal effects however there were questions surrounding how to translate this Theory into practice in this video we will resolve these questions with a set of practical techniques for estimating causal effects these techniques are all based on something called a propensity score we will conclude the discussion with a concrete example with python code and real world data so with that let's get into the video so in the last video of this series we were talking about estimating causal effects and to estimate causal effects we need data but not all data are equal and so here I'm going to distinguish two types of ways we can obtain data so the first is data from what I'll call an observational study so an observational study insists of passively measuring data without intervention in the data generating process so as an example the causal effect of taking a pill on headache status would an observational study might look like is we passively observe a population of people with headaches some of them taking pills some of them not taking pills and just doing an analysis trying to quantify the causal effect of that pill on headache status the second type of way we can obtain data is from what I'll call an Interventional study and what this consists of is an intentional manipulation of a data generating process for a particular goal so an example of this is a randomized control trial which we talked about in the previous video so if we were interested in studying the same thing the effect of a pill on headache status what a randomized controlled trial would look like is we pick a group of people from a larger population of people with headaches we split that group into two subgroups the first of which we give all the people a pill and then the other subgroup we don't give them a pill and then we can evaluate the causal effect so like I mentioned in the previous video Interventional studies or randomized control trials or something like it is one of the most common ways to quantify causal effects but this comes at a cost it takes a lot more effort and care to collect data through a randomized control trial than it is to just passively observe natural behaviors of people so it would be very advantageous if we could compute causal effects using observational data because it is a lot easier to obtain but there's a problem here in observational studies there could be systematic differences between people that take the pill and don't take the pill that can bias your estimate so for this example where we're just passively observing people with headaches not controlling who takes the pill and who doesn't take the pill there could be a variable that we might not be measuring such as age that could be a confounder because someone's age could drive their behavior to take a pill like kids probably won't be taking pills because their parents won't let them adults might be more likely to take pills than kids because they don't need permission from anyone to take a Tylenol and then that could also affect headache status kids might be less prone to get headaches than adults and so this introduces a systematic difference that can bias our causal effect estimate so one solution to this problem is the propensity score and so a propensity score aims to solve this problem of systematic differences by estimating the probability a subject receives a treatment based on other characteristics so essentially what we do is we include additional variables to our treatment and outcomes so our treatment variable could be takes pill or not our outcome could be headache status and then we can collect other variables that might influence treatment status or headache status such as age income sex or some other variables and so a common way of computing the propensities score is a two-step process first we train a logistic regression model so what that consists of is taking your covariates basically any variable that's not your treatment or outcome and so let's say here we have age income and sex and then we set as our Target variable the treatment status so in other words we're using the covariates to predict treatment status and logistic regression is just a way to connect a set of predictors to a binary Target once we get the logistic regression model the next step is to use it to generate the propensity score so what this might look like is we take a subject with a set of covariates here we have age income and sex we pass them into the logistic regression model that we just developed and then out comes a probability of treatment or in other words a propensity score and so now we can do this for all our subjects and we have a propensity score for every single subject in our data set with a propensity score for every subject in our observational data set we can use the propensity score in different ways to help estimate unbiased causal effects and here I'll be talking about three different propensity score based methods for doing this and so these methods are matching stratification and inverse probability of treatment waiting okay so starting with matching in the simplest case what matching consists of is creating treated untreated pairs with similar propensity scores what this might look like is we have an observational data set so we've just passively observed People's Natural behaviors when they have a headache let's say five people took a pill seven people didn't take the pill and we have propensity scores for each of these subjects and notice that there are people that actually took the pill that have a relatively low probability of treatment and conversely there are people that didn't take the pill that have a relatively high probability of treatment so this is good because it helps us in doing the matching process so one way we can do this is called one to one unmatching without replacement say we pick this subject with a 93 percent probability of treatment and what we do is match this subject to a subject in the untreated population with the most similar propensity score so we can just look at all these subjects we see 73 is the closest to 93 out of all these participants and we just match these two together and then we can take this subject with the 80 propensity score and then we do the same process excluding this participant here with 73 and then we pick this participant with a 57 percent pick the closest one in the untreated population so on and so forth so now we have a so-called matched sample and this is reminiscent of what we might see in a randomized control trial we have two groups of equal size one group took a treatment the other group didn't take the treatment so with this match sample we can compute the average treatment effect just like we did in the previous video so conceive of probably there are two ways we could go about this the first way ate stands for average treatment effect defined in the previous video e is the expectation value which is essentially an average Y is denoting the outcome variable one is indicating a treatment status of one meaning they took the pill zero is indicating a treatment status of zero meaning they didn't take the pill and I is just indexing these pairs in the Matched sample so kind of walking through this I equals one let's say it corresponds to this matched pair so this person corresponds to this term and then this person corresponds to this term we take their difference then we look at this pair we take the difference in their outcomes then we look at the next pair look at the difference in their outcomes and so on and so forth and now we'll have five values corresponding to the difference in outcomes for each of these pairs and then we can just take their average and then that'll give us an average treatment effect alternatively we can use the Expression we had for a randomized control trial so that's what RCT means here so this is is the average treatment effect in a randomized controlled trial so it's just slightly different instead of taking five differences and Computing the average here we will look at all the participants in the treated population look at their outcome and compute the average then we look at the outcome for the untreated population and then we take their difference so these are two alternative ways we can compute an average treatment effect with a matched sample so here there are a lot of details that I glossed over and I don't want to spend too much time on it I'll just refer you to this nice paper by Austin where he dives into details on optimizing the Matched sample basically how we match the took pill population with the didn't take pill population and then also Alternatives and matching so here we did one-to-one matching but you can also do one-to-many matching so essentially there what you're doing is instead of pairing each individual in the treated population with one subject in the untreated population you can match many untreated subjects to a single treated subject and this is all in the paper for anyone who is interested so we'll move on to stratification so in stratification we split subjects into groups with similar propensity scores so again let's say we have our observational data set five people took the pill seven people didn't take the pill and then what we do is what is called rank ordering so basically we order the subjects from lowest to highest propensity score so this is like what you do in elementary school math take the subjects from smallest propensity score to largest propensity score and then what we can do is split them into groups so here we split the subjects into four equal sized groups and then what we can do is compute the average treatment effect in each of these groups don't worry too much about the notation here so G is just indexing each group so we can have group one group two group three group four and then p is just indexing the people that took the pill and is indexing the people that didn't take the pill and we can compute the average treatment effect for each group like this so looking at this term this is the expectation value the average outcome for the people that took the pill let's say in group one so let's say G is equal to one and then this for g equals one we look at the average outcome for the people that didn't take the pill and since it's just one person we just look at this one person's outcome and then we take the difference and then we have the average treatment effect for group one we can do the same thing for group two then for group three group four so we have four average treatment effects and then we can go a step further and compute the average of these four averages and get an overall average treatment effect all right the last method I'll talk about is inverse probability of treatment waiting or iptw for short and the first two methods were kind of similar where we were clumping together people with similar propensity scores and comparing average outcomes between people that took the pill and didn't but in iptw we do things a little differently so here instead we use the propensity score to derive five weights from which the average treatment effect can be computed directly so what this might look like is we have our same observational data set five people take the pill seven people didn't take the pill we have propensity scores for all 12 of these individuals then we can convert these propensity scores into weights and so how we Define the weight will depend on whether the subject hooked the pill or didn't take the pill if they took the pill we just do one divided by the propensity score so 1 divided by 0.23 is about 4.35 1 over 0.93 is about 1.08 so on and so forth but if they didn't take the pill we do one divided by 1 minus their propensity score and then we can get weights for each of the subjects okay and then the next step is we use these weights to aggregate the outcomes for the treated and untreated populations so we weight each outcome so p is indexing again the people that took the pill we do this for every subject in the treated population we multi put these together we add them all up and then we divide by n which is the total number of subjects so in this case it would be 12 and then that gives us the aggregated outcome for the treated population then we do the same exact thing for the untreated population which will give us the aggregated outcome for the untreated and then we can use these values to estimate our average treatment effect so here we can just simply take the difference between the aggregated outcome of the treatment with the aggregated outcome of the untreated now we're going to do a concrete example in Python so here we'll compute the average treatment effect of going to grad school on income and for those of you keeping up with the causality videos I've been putting up this example will be very similar to what we saw in the causal inference video this code and much more is available at the GitHub which I will link in the description below so first we import our libraries so we have pickled just to import our data do y is going to help us do the causal effect estimation and then we have numpy to just do some math then we load in the data we have this pickle file which is a pandas data frame also available at the GitHub repository and then this data comes from the UCI machine learning data repository which I will link in the description as well okay next we Define the causal model so this isn't really necessary for the propensity score based techniques but this is a standard procedure in the do y Library we'll get a better sense of why the library is set up in this way in future videos of this series but for now we can just view this as picking out or labeling which variables in our data set are the treatments the outcomes and the covariance so our treatment is the variable has graduate degree which is a Boolean variable indicating whether the subject went to grad school our outcome variable is greater than 50k indicating whether the participant makes more than fifty thousand dollars or not and then we have the common causes which here we can just view as the covariates and we just have H as the single covariant and so what this causal model looks like is this this is identical to what we saw in the causal inference video and then we can compute the causal effects using these three different propensity score based methods so here we're just creating the S demand which isn't so important here but we will see why this is very important in future videos of this series and then I create a list of the names of each of the propensity score based methods so we have matching stratification and waiting then I initialize two data structures a dictionary and a list to store the causal estimates for each of these approaches and then we just compute each causal effect in a for Loop so just one by one for each of these elements in this list we're just going through and Computing the causal estimate and then storing those in both the dictionary and the list that I created earlier so the result of all this is for matching we had an average treatment effect of 0.136 for stratification we had 0.25 and for inverse probability of treatment weighting we had an average treatment effect of 0.331 so it's interesting to see that all three methods even though they're all using the same data give different causal estimates so one thing we can do is just aggregate these and take their average and so the average of all three methods is 0.24 for those of you who recalled the causal inference video where we did an identical analysis but there instead of using propensity score based methods we use something called a meta learner and we got a similar result so they're the average treatment effect was 0.2 and here we have 0.24 so very similar so how we can interpret this result is going to grad school will increase the probability someone makes more than fifty thousand dollars a year by 24 okay so before we jump with joy and say that we can compute causal effects from any observational data set I would share a word of caution so the whole point that people go through the trouble of randomized control trial is that they can handle both measured and unmeasured confounders through the randomization process you can mitigate systematic differences between your treated and untreated populations but with these propensity score based methods we can only hope to handle measured confounders in the example that we just ran through we had only a single measured confounder which was age but there are conceivably other variables that could impact both someone's probability of going to grad school and someone's probability of making more than fifty thousand dollars a year things like parental income or field of study or work ethic and so on and so forth so these propensity score based methods won't be able to account for other confounders that are not included in the propensity score model and so this may not be such a big deal when you know what you need to measure and you have the ability to measure it but the situations where you know what you need to measure but it's very tricky key to measure so for example let's say we're considering work ethic as a confounder to both someone's probability of going to grad school and someone's probability of making more than fifty thousand dollars a year it could be challenging to quantify work ethic and measuring that for each of your subjects so that brings up the problem of unmeasured confounders and so in the next video of this series we will see what we can do about unmeasured confounders okay and lastly there is a Blog associated with this video linked here and I'll also link it in the description there are some details in the blog that I didn't discuss here so for those interested feel free to check that out and if you enjoyed this video please consider liking subscribing or sharing the content if you have any questions please share those in the comments section below I definitely learned a lot from the feedback I get in the comments section and as always thanks for your time and thanks for watching
BOPOX_mTS0g,2022-10-07T12:23:26.000000,Causal Effects | An introduction,hey folks welcome back this is the first video in a new series on causal effects causal effects go beyond statements like a causes B and quantifies how much a change in a will change B in this video we will explore a few Core Concepts that underlie causal effects which will serve as a solid foundation for future videos and with that let's get into it one way we can think about causal effects is going Beyond answering questions of why to answering questions of how much so for example we might ask why did my headache go away put another way what is the reason my headache went away what caused that and maybe what happened was I took a pill so the pill caused my headache to go away but asking how much would be what if I took two pills how much does a unit of treatment so how does an additional pill affect my headache and so that's what causal effects are all about answering this question of how much and so before diving into the discussion on causal effects I'm going to touch on two pieces of background information the first being the types of variables that we see in estimating causal effects and so the first type of variable we have is the outcome which is the variable we're ultimately interested in so using the same example from the previous slide headache status another type of variable we have are treatment variables and so these are the variables that we change in order to influence the outcome so from the same example the pill was the treatment and so we took a pill to try to influence our outcome variable of headache status and then we basically have everything else which we call the covariates and then this could be age weight exercise level how often someone takes headache pills or how adapted they might be to them so on and so forth and so the other piece of background information that we're going to talk about is this potential outcomes framework this is a big idea in the space of causality and it's an approach to estimating causal effects through what if questions so what this might look like is we have scenario one which is what actually happens so let's say I have a headache and I take a pill and then my headache goes away that's scenario one and then we can ask what if I didn't take the pill would I still have a headache what situation would I be in so that's scenario two so we can use this way of thinking this scenario one scenario two way of thinking to formulate causal effects and so in this video I'll talk about three different types of Castle effects listed here and kind of embedded in all these different types of causal effects is the potential outcomes framework okay so first we have the individual treatment effect and so this basically quantifies the impact of treatment for a particular individual so just like we were talking about before scenario one let's say me personally specifically I have a headache and I take a pill and then my headache goes away but then we can imagine this what if scenario scenario two where I have headache but I didn't take the pill I kind of turned back time to the moment just before I took the pill and just didn't and then let's say in that scenario my headache got a little better and so then we can formulate this individual treatment effect or ite for short as the difference between these two outcomes putting this into an equation Y is representing our outcome variable headache status X is representing our treatment status so x equals one represents taking a pill scenario one and then x equals zero represents not taking the pill so scenario two and then what this is representing is the outcome variable when I took the pill versus the outcome variable when I didn't take the pill so we just take the difference and that's the individual treatment effect but if we're trying to investigate the efficacy of this pill we're probably not just interested in how effective it is on me but how effective it is on the larger popular population and so that's where we get into the average treatment effect which is the expected impact of treatment for a population so here we're going to do the same exact thing as before but instead of just for one person we're going to look at a bunch of people so scenario one we have a bunch of people with headaches we give them pills and we measure their outcomes scenario two take those same people and we do the what if scenario and then we measure their outcomes and then just like we did for the individual treatment effects we take a specific person's scenario one look at the outcome subtracted by their outcome in scenario two and then we just take the average for this Rod population we can represent this in an equation form where e is representing the expectation value and then I is indexing the subjects in this study so what this means is we look at the I participant we see what their outcome was in the scenario one and we subtracted by their outcome in scenario two and then we compute the expectation value which is just a weighted average is what we're seeing here okay so we have this way of formulating causal effects through the individual treatment effect and average treatment effect but you know a big question here is how are we supposed to compute this when we can only really observe one of these two scenarios in practice we don't have all the information that we need to compute the individual treatment effect or the average treatment effect and so that brings us to the individual treatment effect in rcts or randomized controlled trials and so this is how we can compute an average treatment effect experimentally so imagine that we pick out a population of people at random and then we randomly split them into two groups so we'll call one the experimental group and we'll call the other the control group and then we can do a similar kind of thing as we did before we give half the people the experimental group a pill when they have a headache and we measure their outcomes then we have the other half of people in the control group and we don't give them a pill and we measure their outcomes but now what we do slightly differently is we compute the average outcome for each of these two groups so with the average outcome for the experimental group and then we have the average outcome for the control group and then we can compute the average treatment effect in the context of this randomized control trial as the difference between these two average outcomes so this is probably the most common way that the average treatment effect is computed in practice through randomized controlled trials or something similar and so how we can represent this in an equation is instead of having just a single expectation value now we have two expectation values and then J is indexing the experimental group and K is indexing the control group so this is much easier to measure in practice than the previous equation because here we're just comparing two numbers as opposed to doing a bunch of subtractions and then Computing the average and then finally we have the average treatment effect for the treated or similarly for the controls so what this is is the expected impact of treatment for the treated population or the control population and so how we can express this mathematically the average treatment effect for the trade-in also called ATT we can write it down like this so here we have again the expectation value which is just a weighted average we have the outcome variable y i is indexing our participants so this is the outcome variable for the ith participant when they take the treatment minus the outcome variable for the ith participant when they don't take the treatment so we can take that difference but now we have this conditional statement so basically what this is saying is what is the average treatment effect but conditional that the participant actually received the treatment and so in the textbook by Morgan Winship they have a nice discussion about this and an example where this might come up the ATT the average treatment effect for the treated is let's say you're trying to measure the causal effect of Private School versus public school on SAT scores or something so what average treatment effect for the treated and what that study might look like is you take students that are already in private school so the treatment variable is in private school you split them into two groups where one of them you keep them in private school and the other group you send them to public school and then you can measure their SAT scores compute the average and then compute the difference conversely for the same example you can do the average treatment effect for the controls you do a very similar study where you split your population into two groups the treatment group and the control group but the only difference here is instead of taking a population of private school students for your study you take a population of public school students for your study and so generally these values these average treatment effects will be different for the two different populations so if your kid already goes to private school school you're probably more interested in the average treatment effect for the treated as opposed to the overall average treatment effect so in this video we talked about three different types of causal effects which are all based on this potential outcomes framework which essentially works by comparing the outcomes of what you actually measure to a what if scenario that you don't measure but there are some practical concerns with this way of doing things so how are we supposed to deal with all these counter factual terms how are we supposed to deal with all these what-if scenarios in practice so we saw that we could do a randomized control trial but are we limited to just doing those kinds of studies what about observational data randomized controlled trials and things like it are expensive not just in cost but in effort you need researchers you need protocols you need irbs it takes a lot of effort to manage these kinds of studies however observational data this is just data that we can passively observe this kind of data is much more prevalent than data from randomized controlled trials it would be very advantageous if we can use this data to compute causal effects and then finally what kind of software tools are available for this stuff so in the next video this series I'm going to touch on all these different questions and discuss three different kinds of practical techniques for computing causal effects from observational data and so if you enjoyed this video please consider liking subscribing or sharing this content if you have questions please share those in the comments section below I learn a lot from the questions I've received in the comments section and as always thanks for your time and thanks for watching foreign
5zeqo-R12vk,2022-10-01T02:01:57.000000,How to STAY dumb,in a lot of situations we might feel the need to look smart while this may work out in the short run there's a big problem if you're so busy trying to look smart you kill your chances of actually getting smarter it all comes down to this when someone trying to look smart comes across something that doesn't make sense they act like it does on the other hand someone trying to get smart will ask a question so you have to decide do you want to look smart or Get Smart
WgmMK5fS0X0,2022-09-27T19:09:20.000000,How to do MORE with LESS - multikills,multitasking isn't a good strategy how do we solve this problem trying to get more done in less time one idea is we can seek guidance from our bird hating ancestors and try to kill two birds with one stone at first glance this might sound like multitasking but it's not multitasking is like when you do two things around the same time while killing two birds with one stone is getting two things done with a single action
gazeatME3dI,2022-09-26T22:07:08.000000,How to be more ANTI-FRAGILE,only certainty in life is uncertainty and unfortunately for us uncertainty doesn't seem like something that's beneficial but what if there was a way we could gain from uncertainty in his book anti-fragile author Nasim Nicholas taleb defines something anti-fragile as something that benefits from things like uncertainty harm and disorder
aD9ereUJBII,2022-09-24T20:12:19.000000,The Achievement TRAP,be careful with achievements it probably sounds silly since achievements are usually thought of as good things but they can be dangerous the more we focus on what we've achieved the less we focus on what we need to do now when we think we're so cool for hitting a goal or winning an award or being the best at something it's easy to get lazy about the habits that helped us achieve that thing in the first place don't fall into the achievement trap no matter what you've achieved there's always room for improvement
6A4qwVOkPF0,2022-09-23T19:42:12.000000,"It’s not DANGER behind the fear of conflict, but uncertainty",so now let's talk about fear the fear of conflict like many other fears doesn't usually stem from any kind of actual danger what it really comes from is the unknown consider the example of driving to work driving is probably one of the riskiest and most dangerous things we do on a daily basis but no one's afraid to commute to work that's because we do it every day it's familiar and things that are familiar aren't scary so in a similar way just like how every time you drive and you survive it becomes less scary every time a team goes through a conflict and survives conflict becomes less scary
W6TkOTsI7vM,2022-08-26T13:30:31.000000,What Is Data Science & How To Start? | A Beginner's Guide,i get a lot of data science questions naturally through my blogs and youtube videos but i've also been getting questions that are a bit more meta the questions are more career oriented people ask me how can i get into data science what do i need to become a good data scientist so on and so forth so the fact that people are reaching out to me with these kinds of questions is probably a good indication that there are a lot of other people out there with similar kinds of questions so that's the inspiration for this video here i'll give my take on what is data science and what is the best way to get into it so i prefaced this whole discussion with this is just my impression as someone that's worked in data science for the past few years in both an academic and now in a business setting and data science being the huge field that it is there are certainly a wide range of different manifestations of what it might look like across different organizations businesses etc however this is the kind of mental model that i've arrived at about data science and the different associated roles through my experience so we'll start with this first question of what is data science and to me data science fits into a bigger picture which includes a variety of different roles and responsibilities that all center around this core of data well again there might be different labels and details for these types of roles the way i see it there are generally five distinct roles in this data space to list them the first is data engineering second is data analytics third is data science which is what i do and the main topic of this video the fourth is machine learning operations and engineering and then the fifth is data management all these archetypal roles and responsibilities in the data space all serve this bigger data pipeline so data engineering is the starting point and then that goes all the way to ml operations engineering and then data management kind of sits on top of this whole thing okay so the first role data engineering is mainly focused on taking data information from the real world and bringing it to a place where data scientists data analysts can do something with that information and as a data scientist i love data engineers because they make my life a lot easier so every data analyst and every data scientist's dream is to just have a curated data set where everything is good to go the data is clean the data is pre-processed and all they have to do is make a visualization or develop a model so on and so forth so good data engineers are very valuable asset probably a big reason why it's such a big job these days okay so the second role i mentioned was data analyst and typically what a data analyst does is take data that is made available to them through whatever means and typically will generate interactive dashboards or visualizations graphs plots they will typically tell stories with data they'll make slides they'll give presentations to decision makers and business partners and stakeholders and a data analyst will typically use tools like microsoft excel tableau power bi and some kind of interactive sql application so a lot of these tools don't require much programming experience so in that regard a data analyst role is typically less technical for lack of a better word than a data scientist rule and probably the biggest difference between a data analyst and a data scientist is that a data scientist will do a lot more programming than a data analyst and so that brings us to the third role which is the data scientist what i do a data scientist will do a lot of the same things as a data analyst we will create visualizations we'll tell stories we'll communicate our findings but like i mentioned what i spend most of my time on is programming so i use python and that seems the most popular these days in the data science space but there's also r which is pretty big in the statistics community there's matlab which is big in the academic or scientific community and then there's also julia which is a kind of new up-and-coming programming language so it's not just a matter of data scientists use different tools than data analysts to do the same thing data scientists also will typically focus on developing models so what's a model a model is basically something that lets you do predictions you give it some information that you can measure and it will give you a prediction about something you care about that you can't necessarily measure right now so this could be something like given someone's past credit history what's the probability that they won't make a credit card payment in the future models are definitely a powerful tool in science in business in finance and sales and basically any industry that has data and has things that they would like to predict models are going to be helpful to that industry which brings us to the fourth role in this data space which is machine learning operations and what this role typically does is takes the model that the data scientist develops and deploys it into production so it doesn't matter how amazing and insightful your model is if it can't be used and deployed in the real world it's not going to have any value what this might look like using the same example from before what's the probability that someone will make their credit card payment based on past credit history so it's great if i develop a model and it's sitting on some server somewhere or sitting on my local machine or whatever it might be but how does that help the people making the decisions of whether to increase someone's credit card limit or to approve a new credit application so that's where the machine learning operations will be valuable the fifth and final rule sits on top of this whole pipeline so we start with the data engineers who take the data and information from the real world and bring it to a place where analysts and data scientists can use it the data analyst will typically take the data that is there and visualize it look at it in different ways and create stories and slides from it the data scientists will do a lot of the same things but we'll take it a step further in taking that data and developing models with it then finally the machine learning engineer will take that model and deploy it into production so this fifth role which i labeled as data management kind of sits on top and what this role focuses on is handling the metadata the meta information so not so much what the values in a specific data table might be but where's the data table located what variables are in this data table what do these variable names mean what is the description of each variable what data types are they using where is it coming from where is this data being pushed to eventually kind of keeping track of all the data about the data which is very important so this ecosystem like all these different types of roles are critical to make sure the whole process is working effectively like you can't just have data engineers you can't just have data scientists you can't just have machine learning engineers you really need every kind of piece of the puzzle to have data make an impact in your business or your research or whatever it might be as a final caveat even though i've presented these five different roles as beautifully distinct and specialized responsibilities in reality it's not like that at all there's a lot of bleed over there's a lot of overlap so just speaking from my experience as a data scientist i do the dating engineering work i do the data analytics work i'll do the machine learning development i'll do the data management stuff and that's just the reality of things so while it may be the case at some organizations that these roles are very specialized so a data scientist will only focus on doing data science type things or a data engineer will only focus on data engineering type things for the most part there is bleed over so if you want to get into any one of these roles it's good to have some knowledge and experience doing the other roles in this space okay so now we'll talk about the second question which is how can i get into data science so first preface this again that this advice is definitely biased by my experience and my data science journey so there are certainly other paths to get into a data science role but these are the tips that i would give to anyone that comes up to me and says that they want to get into data science i have three tips for anyone trying to get into data science so the first thing to do is get a technical graduate degree so probably 80 to 90 of the people that i work with in data science they either have a master's degree or a phd in some technical field so what do i mean by technical there's a lot of flexibility here so i have a phd in physics i've seen phds in math phds in statistics masters in statistics master's in data science and data analytics any training that has a focus on math science programming will be helpful to getting a job and while you don't necessarily learn all that you need to know to do data science in these different programs there's typically a good amount of overlap so in physics you learn a lot of math and science and you do a lot of modeling and so that's a pretty natural transition into data science but second and probably the most practical is that it's just something to have on your resume so the reality is when people are looking at resumes for data scientists they're getting flooded with applications and there just has to be a way to just narrow down the applications to help make a decision and typically the first filter that people use in picking data science applications is do they have a graduate degree and in what and so again i'll highlight even though this is the background of the bulk of people i see in data science they have a technical graduate degree there's certainly other people working in data science without them maybe they'll just have an undergraduate degree or maybe they got an undergraduate degree and did like a data science boot camp so it's certainly not the only way into data science but it is the bulk of what i've seen okay so the second tip i would give to anyone trying to get into data science is to learn the basics and what are the basics so i would say there are three kind of core things that you need as a data scientist one you absolutely need to know how to program i would definitely recommend to learn python the second basic basic thing you need as a data scientist is the solid mathematical foundation and so specifically what this includes is linear algebra statistics and calculus those are the three core mathematical topics that you need to be comfortable with and then the third basic thing you need as a data scientist which i personally think is the most important but is often the one that's focused on the least is communication if you want to be a good data scientist you need to be able to present technical information to a non-technical audience you need to be able to translate your insights into something practical into something that has implications for the real world and the reason i think it's the most important is it doesn't matter how profound or impressive your findings are if it doesn't make sense to people if no one understands what you're talking about it's going to have absolutely zero impact it's going to provide absolutely no value to anyone so again second tip is learn the basics learn how to program probably python's your best bet get the solid mathematical foundation and work on your technical communication okay so the third and final tip that i would give to any aspiring data scientist is to do it i think the best way to learn is by doing so the best way to learn data science is by doing data science and so what this might look like in the early days is just finding some data and doing a project ideally using real world data so not like curated and ready to go data sets that you might find on kaggle for instance but go online try to find data sets try to find data that is not clean use a web scraper to pull information off a website reach out to your local network do you know anyone doing research that is working with real world do you know anyone that owns or works in a business that is working with real world data try to get access to that data and come up with some projects that might be helpful to that person so if you want to get good at data science you got to do data science so try to come up with at least one or two data science projects and do it from a to z do the data engineering bit do the data analytics do the data science bit and do the machine learning engineering in other words take the data from the raw source and bring it into a place where you can analyze it create visualizations look at the basic statistics try to tell a story with the data identify a problem identify a project then develop a model do something that has real world impact that provides value and then try to deploy it maybe make a website maybe make an api so people can actually use your project and then i'll also say a lot of times it's not just about what you know but about what you show like let's say you do the whole shebang you do it all perfectly you get real world data you look at the data you get the basic statistics you make visualizations you develop a model and then you deploy that model and it has tremendous real world impact and it provides value but if you don't present that information no one's going to know about it so if you're trying to work in data science you want to get hired by someone to do data science you need to not just know your stuff but show your stuff show people what you've done what you can do and it's going to increase your chances of success so don't just do a data science project and keep it a secret make a portfolio put it out there and show people what you can do okay so that's basically it so if you watch this and you still have questions please feel free to drop additional questions in the comments section below the questions are really helpful for me i feel like i learn a lot about the people that watch these videos through the questions that i see in the comments section so i greatly appreciate those i hope you found some value in this video and after watching it you know what to do next to start your data science journey or you realize you don't want to be a data scientist at all and you're going to open a coffee shop or something either way i hope it was helpful and as always thanks for watching you
5ezFcy9CIWE,2022-06-16T13:50:15.000000,Persistent Homology | Introduction & Python Example Code,hey folks welcome back this is the final video in a three-part series on topological data analysis or TDA for short in this video I'll be talking about another specific technique Under the Umbrella of TDA called persistent homology the big idea behind persistent homology is finding the core topological features of your data that are hopefully robust to noise I'll start with a brief discussion of key points surrounding persistent homology and then dive into a concrete example with code of how to use it and with that let's get into the video there are many layers to persistent homology so I'll try to start super simple and build things up in a way that hopefully makes some sense like I've mentioned throughout this series TDA is all about looking at the shape of data so let's go back to preschool and talk about shapes or more precisely polygons like the ones shown here but not all polygons are equal there is one that is special and the reason it is special is because it is the simplest polygon we can construct the triangle and one neat thing about triangles is that we can use them to make any other polygon for example a square is really just two triangles stuck together a pentagon can be made from four triangles like this and a star is just the same Pentagon but with five triangles coming out of it so one thought is if we want to analyze the shape of our data maybe we can break it down into a bunch of triangles well as it turns out this is essentially what what we do in persistent homology but with one technical detail since most data sets live in more than just two dimensions that's to say we have more than just two variables flat two-dimensional triangles may not capture the full richness of our data's shape don't worry like most things mathematicians have generalized the notion of a triangle to any number of dimensions and they call these generalized triangles simplexes so the triangle that we know and love is called a two Simplex since it lives in two dimensions a line segment is the simplest shape we can construct in one dimension and it's called a one Simplex similarly a tetrahedron is called a three simplex and a point is a zero simplex and so on for all the other dimensions so just like a collection of triangles can make any two-dimensional polygon a collection of simplexes can approximate just about any complicated high-dimensional shape that may underly our data and so since you'll probably see it elsewhere the technical name for a collection of simplexes is called a simplicial complex and this is a key Concept in persistent homology okay so this gives us a clue as to how we can take unstructured Point clouds in other words data sets and translate them into shapes so now let's talk about how we might compare shapes together no matter how different or complicated they may seem so one way to do this is by looking at holes for example these three objects shown here we have a Taurus a loop and a coffee mug so while these may appear to be very different shapes they have something fundamental in common they all have a hole and this is like the joke that a topologist looks at a coffee mug and a donut and sees the same thing the reason being that one can continuously transform one into the other for the fadas out there this is called a homeomorphism but the fundamental thing here is the number of holes so one way we can characterize and group shapes together is by counting holes and just like before when we generalize triangles into simplexes we can generalize holes as well we can think of cavities as holes in 3D and we can think of singly connected components as holes in 1D and so these generalized holes form the basis of what are called homology groups and these give us a formal way to characterize different shapes so when we talk about homology we are essentially just talking about holes okay so now that we've talked about constructing shapes with generalized triangles and characterizing those shapes via generalized holes we can finally talk about persistent homology and the first step in persistent homology is to convert data into a simpal complex to see this consider a data set I.E a point cloud like this and one way we can construct a simplicial complex out of this is by drawing n dimensional balls around each point and since our data here is two-dimensional we just draw circles around each point which might look something like this so at the center of each of these gray circles we have a point we can form one simplexes I.E line segments by connecting the data points whose corresponding circles overlap which might look something like this and so now we have two shapes we have our original Point Cloud which is indeed a simpal complex where each point is a zero simp Lex and the shape we just constructed made up of both zero and one simplexes and then we can compare these two shapes by looking at their homology more specifically by counting the number of connected components which corresponds to the h0 homology group that we talked about in the previous slide and there we go so we can see that in our first shape on the left we have 20 separate connected components while on the right here we have 13 singly Connected components but there's nothing special about this radius Epsilon sub one so let's do this again but with bigger circles now we can start to see two simplexes appear in other words triangles and the number of connected components decreases but still there's nothing special about Epsilon 2 so let's go even bigger and now we see three simplexes appear I.E tetrahedrons and so on and so forth however there is a special radius value here which is when every circle overlaps with with every other Circle and we are just left with one big connected component and this is a natural limit to this process as we can see with each of these simplicial complexes the shape of our data is evolving and its evolution is captured and Quantified by the number of connected components in other words by the change in its homology so although only four different choices of radi are shown here corresponding to the four different shapes on the screen we can do this for every choice of radius between zero and the limit I mentioned earlier so this gives us a way to sus out which topological features of our data are significant based on how long they persist during this circle growing process in other words the holes that persist over a large increase in radi are more significant than the ones that persist over just a short period okay so how can we track the Persistence of these holes so so one good way to do this is by using a persistence diagram these look something like the plot on the left here which is showing the persistence diagram of a hollow sphere and looking at the plot each of these blue orange and green points corresponds to a topological feature or in other words a hole in blue we have the H Subzero homology Group which are the singly connected components in Orange we have the H1 homology Group which are closed loops and in green we have the H2 homology group in other words W cavities the x axis of this plot indicates the radius at which a hole appeared in the evolution of the data's shape in other words in this circle growing process that we showed in this previous slide and on the y- AIS we have the radius at which that hole disappeared so therefore a point that sits near this black dashed Line This yal X line corresponds to a hole that disappeared soon after it appeared conversely points that sit far away from this line represent holes that disappeared long after they appeared therefore two key points of a persistence diagram are the points close to this yal x line are noise while the points relatively far from this line are significant so in this example we have two points that are far from this line the blue one in the top left here and the green one right here so we can ignore this blue one here because this corresponds to when every and dimensional ball overlaps with every other ball so the significant topological feature of this data is captured by this Green Point here which is telling us that the data is characterized by one cavity and this makes sense since the data for this example are organized on the surface of a sphere okay so up until this point I've discussed only toy examples and meant to give you an idea of what's going on with persistent homology so now we'll switch gears to an example with with real world data so in this example we'll walk through how one could use persistent homology to analyze Market data and I suppose it's worth mentioning that this example is not meant as Financial advice I'm a physicist not a Trader never taken a finance class in my life however I hope this example gives you an idea of what an analysis using persistent homology might look like and Inspire ideas for analyses using data that you might be working with okay so similar to the last video we start by importing python libraries the notable libraries here are y Finance which gives us an API to grab Market data and the riper and pum modules which are part of the same pyit TDA ecosystem from the last video next we load in Market data over a 4-year period using Y Finance here we are grabbing four major Market indexes namely the s&p500 Dow Jones NASDAQ and Russell 2000 we have daily prices for these index organized in a pandas data frame so you can imagine four columns for each market index and many rows corresponding to each day that the markets were open over this 4-year period then we convert this pandas data frame into a numpy array and compute the log daily returns of each index and this choice of data prep follows the procedure used in the paper by gidia and cats which was the inspiration for this example and you can find it at the archive reference here okay so now we get into the TDA SEF so in this analysis we want to track changes in the shape of the markets by looking at how the homology of the market changes over time so to do this we start by initializing this object that constructs simpal complexes from data next we Define a Time window size which will allow us to grab a chunk of data to analyze the homology of so here we're sending this window size to 20 days next we Define the total number of these chunks we will have and finally we create an Umpire rate to keep track of a number that quantifies changes in homology okay next we go down to this for Loop and we do some persistent homology so first we take the first 20 rows of data to do persistent homology and create a persistence diagram that is we grow four dimensional balls around each point where each choice of radius creates a simplicial complex and we track the holes that appear and disappear using a persistence diagram so we do all that with just one line of code and we do the same thing but now for another set of 20 rows specifically the second row all the way down to the 21st row so now we have two persistance diagrams corresponding to two overlapping 20-day windows in which the market was open so next we can quantify the change in the overall homology between these two persistence diagrams using something called the washer Stein distance which is essentially a distance measure between two persistence diagrams so at the end of this whole process we get a single number and store it in the numpy array we created earlier then we repeat this whole process for all the rows in our data set okay so after this whole process we have a set of values which quantify the changes in homology between consecutive days that the market was open and so we can just plot this as a Time series which is what's happening in this block of code here and the plot will look like this blue line here which we can see there's this clear peak near the middle of the time series and then for some context we also have scaled S&P 500 close prices plotted in Orange just above and this vertical red line here is indicating when the crash of 2020 occurred and then as it turns out the peak in this waserstein distance time series seems to correspond very closely with when this crash occurred so did homology changes predict the crash of 2020 well I wouldn't go that far but this is indeed interesting one idea to investigate this further is one could try to use these waserstein distances to predict future market index prices so if past distance values predict future index prices then maybe there's something here so as you may be able to see from this example there is a lot of room for creativity when using persistent homology in practice and in some sense this is more art than science so that bring brings us to the end of our three-part series on topological data analysis A TDA is a young field with a lot of untapped potential so I hope this series was helpful in getting a better idea of what it's all about if you'd like to learn more check out the other videos in this series Linked In the description below there's also a corresponding medium article to this video and the others in this series which you can find in the description if you enjoyed this content please consider liking subscribing or sharing this video like many of you I am indeed still learning so if you have thoughts questions or concerns please feel free to share those in the comment section below and as always thanks for watching
NlMrvCYlOOQ,2022-06-03T10:02:25.000000,The Mapper Algorithm | Overview & Python Example Code,hey folks welcome back this is the second video in a three-part series on topological data analysis or tda for short in this video i'll be talking about a specific technique under the umbrella of tda called the mapper algorithm what this approach allows you to do is translate your data into an interactive graphical representation which enables things like exploratory data analysis and finding new patterns in your data i'll start with a discussion of how the algorithm works before diving into a concrete example with code and with that let's get into the video so in the previous video i discussed a famous problem in math called the seven bridges of koenigsberg so i won't go into all the details of the problem but it was eventually solved by famous mathematician leonard euler and the way he solved it was by drawing a picture and this picture is what we now call a graph and so a graph consists of dots connected by lines more technical term for these things the dots are called vertices and the lines are called edges another equivalent terminology is instead of calling this thing a graph we can call it a network and we can call the dots nodes and we can call the lines links so these are all equivalent terminology that i'll probably use interchangeably for this video and so graphs or networks they typically represent something from the real world so in this case euler drew a graph representing konigsberg where each of the nodes represented a land mass and each of the lines connecting two nodes represented a bridge and so what this does is it boils down the problem to its essential elements and this is what allowed euler to famously solve this problem and as i mentioned in the previous video this is essentially what we're doing when we do topological data analysis we are translating data from the real world into its essential elements or in other words into its underlying shape and so one way of doing this is via the mapper algorithm and the main topic of this video so the mapper algorithm allows us to translate data into a graph so key applications of the map or algorithm one is exploratory data analysis it allows us to take a data set and generate a visually engaging and interactive visualization another application is that it allows you to compress and visualize very high dimensional data so imagine trying to visualize a 500 dimensional data set with mapper algorithm we can take our data set compress it into a two-dimensional graph and then visualize it and try to highlight some key insights and we saw some examples of this in the previous video with discovery of cancer subtypes defining new roles in basketball and characterizing the evolution of the two political parties in the u.s so at a super high level the mapper algorithm takes data and translates it into a graph but how exactly does it work i've broken down the algorithm into five steps and i apologize in advance because it's a bit sophisticated but i will do my best to explain it in plain english so the first step is we start with our data set so here we have a two-dimensional data set because we have two variables x1 and x2 then the second step is we project our data into a lower dimensional space so here we're going from two dimensions and we're projecting down to one dimension and we can do this with any dimensionality reduction strategy like we do something standard like pca we can do something more sophisticated as we will see in the example later another popular strategy is to take just basic statistics to project out into one dimension in other words you could consider two variables x1 and x2 so each point will have a corresponding x1 and x2 value you could take the average of those two and organize them onto a one-dimensional axis you could take the max you could take them in so they're all these different strategies so we've gone from two dimensions down to one dimension so nothing too fancy yet so the next step is we define something called a cover so basically what this means is we're going to define two subsets indicated by this red circle and this green circle and we will have these two subsets have some overlap so we can see here that the red subset and the green subset indeed have some overlap and these are indicated by the yellow points in the center of this picture and so that's what we mean by cover we just define a collection of subsets that have some overlap which include the entirety of the data set another thing is we could do more than just two subsets we could have three subsets four subsets so on but just for this toy example i chose two because it's easy to see what's going on here okay and then the fourth step is we cluster the pre-image there's a lot of jargon here so i'm just gonna break it down so if we look at step three we have these red points green points and yellow points but if we remember that each of these points has a corresponding point in our original data set so what's being shown in this picture in step four is our original data set but the points are colored based on which subset they appear in from step three and so the next step is we're going to iteratively go through all our subsets so we only have two subsets we have a red subset at green subset and we're going to apply our favorite clustering algorithm we'll start with this red subset so in other words we're going to look at the red and yellow points only and we're going to do a clustering algorithm and let's say it looks something like this then we will go to our next subset which is the green and yellow points here and we will cluster those and let's say we get something like this so now we have these four clusters defined with some overlap between them now we're set up to create a graph so we can create a graph where the nodes are these clusters so four nodes corresponding to four clusters and then two nodes are connected by an edge if the clusters have shared members so this middle cluster shares members with the other three and so that's what's being shown here okay so this is just a toy example i hope that was somewhat clear of what's going on here but i'm going to try to make things more concrete with an example with code so in this example we're going to do exploratory data analysis of s p 500 data so our first step is to import some modules so we have the yahoo finance module which allows us to get the stock data we have this k-mapper module which allows us to do the mapper algorithm stuff so we're importing this umap module sklearn and then something from sklearn and we're using these for our dimensionality reduction and then we have numpy and matplotlib to do some standard math and visualization stuff okay so the first step as with any data science project is you're gonna get your data so this is pretty straightforward you just define your ticker names and you define the date range for which you want your data and then with one line of code you can pull all that data and so this code is available on the github everything should just work out of the box and once we have our data we can do some more preparation to make it ready to go to do our analysis so the first step is we're just going to look at adjusted close prices and so now what you can imagine is we have columns corresponding to ticker names and then we have rows which are corresponding to days that the markets open and then what we're going to do is convert this pandas data frame into a numpy array and then we're going to standardize each of the columns so basically what that means is we're going to consider a column compute its mean and standard deviation and then we're going to subtract the mean from each value in this column and we're going to divide it by the standard deviation and then the last step here is we do a transpose just because later this will allow us to compare takers together as opposed to days so we could also not do a transpose and then the analysis wouldn't so much be comparing different tickers together but comparing days that the market was open and then the last step here is we're going to compute the percent return of each of the tickers because later when we generate this interactive network we can color the nodes in the network based on the percent return value of each of the tickers okay so all this talking and explaining and we still haven't really gotten into any topological data analysis so if we think back to that visual overview from earlier this is all still step one we're still getting our data so now we can finally get into the mapper algorithm stuff first we will initialize this object next we're gonna do step two in the process which is project our data into a lower dimensional space so we actually have 495 tickers here and what we're going to do is project that down into two dimensions and the way we do this is a two-step process first we use iso map from this manifold library in sklearn so that'll take us from 495 dimensions down to 100 then we'll use umap which will take us further from 100 down to two dimensions so the nice thing about this syntax is we can define a custom data pipeline to do our dimensionality reduction so we can see this projection keyword is being set to a list and this list is actually a list of functions element in this list is manifold.isomap with all the input arguments there and then the second element of the list is umap with all its input arguments but we could have gone further we could have added a third element and made that pca which took us from two components down to one component or we could have done a completely different data processing pipeline so you can already start to see that you have a lot of flexibility in using the mapper algorithm in practice so we essentially will combine steps three four and five from the overview earlier into one line of code so defining a cover clustering the pre-image and generating an output graph is all compressed down to a single function call in that we pass in the projected data from the previous step the original data set and we defined the clustering strategy that we want to use here we use the db scan with a cosine similarity metric yeah you can also customize the details of the cover but here we're just using the default values and in less than a second it generates the graph and so the next step here i define a file id which isn't really necessary i just like to do it because every time i've used the mapper algorithm i'll try different choices of cover i'll try different projection strategies i'll try different clustering algorithms and so on and i'll typically have these going in a for loop and i don't want the output graphs to get overwritten so i'll define this file id which will automatically generate a unique file name for each output graph and then the last step is we visualize the network you just pass in the graph you define a file name you can give the graph a title you can have these custom tool tips which is the label for each of the members so basically this is our ticker names we can define color values which we will define as the log percent returns we can give a name to the color function and then we can also have multiple options of how these color values are aggregated though we could just do a simple average we could compute the standard deviation the sum the max the min and so on and so what the output of the mapper algorithm looks like is something like this it actually generates a web page which allows you to interact with the graph and blends itself very well to explore to our data analysis which we're doing right now okay so the code that we just walked through will actually generate an html file which we can go ahead and open so first look this does not look like the network i showed earlier but if we go to this help menu we will find different viewing options so we can click e on our keyboard to do a tight layout and already it's starting to look a bit nicer and then we can click p for print mode which will just give the graph a white background and then next we can click on any node we like and it'll start to kind of radiate this glow and then we can go over to this cluster details click on this plus sign and so remember that the nodes in this network are actually clusters of data points and then the way we do the analysis here is we actually have clusters of tickers or in other words stocks so down here you'll see the names of the members of this cluster listed and so this was generated from that custom tool tip option in that last function call we made and then here we also have a histogram which is showing distribution of the log percent returns of the members of this selected cluster so right now the weighted average of the log percent returns of each cluster is what generates each node's color but we could use other statistics so if we go over here to this node color function we can click this drop down menu we could do the standard deviation which doesn't look too exciting we could also do the sum which also looks pretty uniform but then we could also do max so now we're starting to see some variation we then might be curious about the clusters that contain members with high returns so we can click on this yellow node here and then we can look at these ticker names and maybe do some further analysis so i'm no financial expert so i don't have much intuition to offer here but when working with data that you are familiar with you may immediately start to see interesting patterns just by jumping around and you can really do this all day you can click on a particular node see what members are in that cluster and then you can click on adjacent nodes and see what members are in those clusters and then you can go back try out different projection strategies try out different clustering algorithms generate new graphs and then repeat this whole process all right so that's basically it again the code for this example is freely available at the github if you want to learn more check out other videos in the series in the next video i will discuss another specific tda technique called persistent homology if you enjoyed this content please consider liking subscribing and sharing this video like many of you i am still learning so i would also appreciate your questions concerns and feedback in the comments section below and as always thanks for watching you
fpL5fMmJHqk,2022-05-21T19:15:28.000000,Topological Data Analysis (TDA) | An introduction,hey folks welcome back this is the first video in a three-part series on topological data analysis or tda for short tda is an up-and-coming approach to data analysis that focuses on looking at the shape of data this first video will serve as a brief introduction to the topic setting the stage for the next two videos which talk about two specific techniques the mapper algorithm and persistent homology so without any further ado let's get into the video so starting place for all this is the observation that data volumes across different domains seem to be increasing at an accelerating rate and in many cases this is a good thing for example it served as the fuel for so many of the technological innovations in the past decade or so like image search or recommendation systems natural language processing and so on but there's a problem mo data often means mo problems and any practitioner will tell you data from the real world is often noisy and high dimensional and so that's where topological data analysis or tda for short can help and the catch phrase for tda often goes something like data has shape and shape matters so the basic basic idea of tda is to take data and translate it into some kind of shape so the reason this is helpful is that the underlying shape of our data is often much more robust to noise and perturbations that our data generating process may undergo and additionally all the tda algorithms that extract shape from data at least the ones that we will discuss in the series are readily applicable to very high dimensional data sets so all in one package we can both handle the problem of noisy data and high dimensional data with this single framework of topological data analysis so the first key word in the acronym tda topological comes from the mathematical field of topology and the story of topology goes back to a famous problem called the seven bridges of konigsberg and so koenigsberg is an old city which looks something like the image we have here on the left so koenigsberg can be described as these four land masses sitting to the north south east and in the center connected by seven bridges and that's what's being shown in this image here and so the famous problem goes something like this what path crosses all seven bridges without visiting a single bridge more than once so i'll try to say that again in different words so is there a path on this map of konigsberg where we can cross all seven bridges but only visit each bridge a single time so if you want you can take a moment to try to find such a path but it would be futile because no such path exists it's one thing to say it but to demonstrate this mathematically took the work of a very famous mathematician leonard euler and so the way euler solved this problem is by drawing a picture of konigsberg but not like the picture we see on the left here the picture that euler drew was much more simple and it looks something like this it's just a collection of circles connected by lines and today we call this type of picture a graph and we call these circles vertices or nodes and then we call these connections between nodes edges or links and so what this picture what this graph is representing is actually konigsberg where each circle corresponds to a land mass in konigsberg and every line represents a bridge and we have two circles are connected by a line if the corresponding land masses are connected by a bridge so what euler did in drawing this picture drawing this graph is he boiled down this map of konigsberg to the essential elements which eventually allowed him to solve this problem so what euler did all those years ago is basically what we're doing when we do topological data analysis so we take data we can consider this map of konigsberg as data and we want to extract the fundamental shape of this data so as you can imagine there are countless ways in which we can take data and translate it into its underlying shape or structure so in other words there are a lot of ways in which we can do topological data analysis so there's this very nice review article uh by chazal and michelle i probably butchered those names so i apologize but the reference is given here at the bottom so in this article they describe a generalized pipeline for topological data analysis it goes something like this you start with your data you translate that data into some shape some underlying structure it could be a graph just like euler did or it could be something else and from that shape you extract top logical features then you can use those features to inform analysis which can range from just basic statistics all the way to very sophisticated machine learning models so the motivation for this series is that when i was first exposed to topological data analysis it was really exciting i would see these really cool and engaging visualizations applied to very diverse set of problems such as discovering new subtypes of cancer or defining new roles based on basketball statistics or even describing the evolution of politics in the united states but that the problem came when i started to dive a bit more into the details of topological data analysis and i was met with this a wall of terminology in jargon which isn't a bad thing mathematical rigor associated with topological analysis is a good thing a lot of the techniques under the umbrella of topological data analysis are very well founded mathematically but as a non-mathematician as an outsider so to speak trying to figure out what's going on here there was just a barrier to entry to use this stuff in practice and so my goal in this series is to put the terminology aside to a large extent and focus on two main goals one i'd like to highlight key ideas from topological data analysis for non-mathematicians and two i want to share ready to use example codes that showcase what topological data analysis can do so in the next two videos i will be talking about two specific techniques respectively the first technique is called the mapper algorithm what this algorithm provides is something very similar to what euler did we will take any generic data set and we will translate it into a graph then in the following video we will discuss another specific technique under the umbrella of topological data analysis called persistent homology so i hope you've enjoyed this video if you'd like to learn more check out the resources provided in the video description if you found this video helpful please consider liking subscribing or sharing your thoughts in the comments section below and with that thanks for watching
poGxnBR3hEU,2022-02-23T20:52:15.000000,"Multi-kills: How to Do More With Less (no, not by multi-tasking)",in life it can often feel like we have too much to do with too little time and this puts us in a tough spot because we need to figure out how to get more done in less time and this drives people to do crazy things like multitask we all know multitasking it's when you try to do two or more things at the same ish time but really all this is is rapidly switching between tasks and it might work for small things like listening to music on a jog or drive but this can easily get out of hand like trying to do your taxes while cooking dinner there's a lot of stuff out there about the evils of multitasking but to me multitasking isn't so much evil as it is just not an optimal strategy the biggest deal breaker for multitasking is that it splits your attention put another way it doesn't allow you to focus your attention and performance and learning tend to be best when we can really hone in on the task at hand it's like that old saying it's better than a whole ass one thing than to half-ass two things so multitasking isn't a good strategy how do we solve this problem of trying to get more done in less time one idea is we can seek guidance from our bird bird-hating ancestors and try to kill two birds with one stone at first glance this might sound like multitasking but it's not multitasking is like when you do two things around the same time while killing two birds with one stone is getting two things done with a single action the key benefit of this is that we can get more done with less and we're not just talking about saving time here which some might argue multitasking does since the proverbial stone can be anything it can be time money energy attention social capital whatever and on top of all that since you don't have to split your attention in half you can really focus on one thing at a time with this strategy so referring to this idea as killing two birds with one stone is tedious and limited so let's generalize it and define a multi-kill as a single action that results in multiple desired outcomes this word multi-kill might sound foreign but people do this all the time and here i'll share three concrete examples of multi-kills that will hopefully inspire more ideas that you can implement in your life right now the first example is one of my favorite multi-kills which i'll just call a cleaning break the power of breaks is something that took me far too long to figure out and a cleaning break is when you take a break from something mentally tasked take a break from something mentally tax taxing mentally taxing like saying this word a few moments later a cleaning break is when you take a break from something mentally taxing and doing something that's not mentally taxing like tidying up around the house so this could be doing the dishes doing your laundry brushing your teeth you know whatever something that is automatic something that you don't really have to think too much about and notice this isn't multitasking because if you take a break from your computer screen to go wash the dishes you're only doing one thing but you're getting two desired outcomes you're one taking a break and two you're getting clean dishes the second multi-kill boils down to this get paid to do things you do anyway this is like what every adult tells you when you're younger and older too find your passion they'll say things like love what you do and you won't work a day in your life what they also meant to say is love what you do and you'll be multi-killing life if you enjoy your work and get paid for it that's one of the best multi-kills there is and not to mention other things you can get out of your work like developing skills learning new things social interaction like that's like a five thing multi-kill right there this third and last example relaxes the multi-kill definition instead of killing two birds with one stone will use one stone to kill one bird and just slightly injure another so this is like a single action that results in completing one task and then getting started on another so i'll make this a bit more concrete with an example so you may have noticed that each of my youtube videos has a blog associated with them and this isn't just because i like writing and talking about stuff but these videos usually start out as blogs so when writing these blogs i one get a blog out of it and two i get an outline and a starting place for each of these videos so multi kills can be less strict than one might think at first so just to summarize i'll mention three key takeaways one multitasking is not an effective way to get more out of your time two a more effective strategy is multi-kills which is a single action that results in multiple desired outcomes and three we can incorporate multi kills into our lives to get more out of our time and i gave three concrete examples that might inspire ideas of multi kills that you can implement in your life so if you enjoyed this video and you want to learn more check out the blog on medium where i talk about this idea of multi kills more and give a few more concrete examples if you have multi kills that you do already in your life or just some ideas please drop them in the comments section below that would be very helpful to me and others who are trying to get more out of our time and if you enjoyed this content please consider liking subscribing and sharing this video and as always thanks for watching
6m82mLNDCyg,2021-12-29T21:11:41.000000,How to Be Antifragile | 7 Practical Tips,it's been said that the only certainty in life is uncertainty and unfortunately for us uncertainty doesn't seem like something that's beneficial but what if there was a way we could gain from uncertainty in his book anti-fragile author nasim nicholas tele defines something anti-fragile as something that benefits from things like uncertainty harm and disorder and the story goes something like this what's the opposite of fragile i'll give you a second a few moments later most people will say something like robust resilient tough tenacious etc but think of it like this suppose you're sending some very expensive champagne glasses to a good friend in siberia get all the packing material you put the glasses in there and you make sure the contents are securely packed and the box is labeled fragile handle with care the key thing here is this label of handle with care what does this imply this implies things that are fragile don't like don't benefit from variability or disorder or being shaken up so if things that are fragile don't like change don't like variability it's opposite should love it things like robust resilient tough tenacious they don't quite fit under this category something robust doesn't care if there's variability or stability something robust is apathetic toward its environment in the book anti-fragile celeb defines the opposite of fragile as anti-fragile and this is something that benefits from variability and volatility and all these things that don't typically sound like good things so these three concepts of fragile robust or something like it and anti-fragile form what naseem calls a trinity and there are three stories from greek mythology which highlight these three concepts the first story is the sword of democracy which is the story about a servant who sees the life of a king with all his gold all his power and wants nothing more to be in his position and so one day the servant trades places with the king and what he soon realizes is that it's not so great being king because when you have everything everyone wants to take your stuff everyone wants to take your power and the sword sitting above the king's throne is a symbol for the constant threat you are under when you are in a position of power so being king is an example of being fragile because when you have everything you have nothing to gain and everything to lose any changes in the environment and situation will probably not benefit you the second story is the story of the phoenix which is a bird who when it dies it rises again from its ashes and this highlights the concept of robustness the phoenix doesn't care if it lives in a stable tranquil environment or an erratic constantly changing environment because worst case scenario if the phoenix dies it'll rise again from its ashes the phoenix is apathetic toward its situation and the final story is the story of the hydra monster which is the monster that hercules fought and whenever he would chop off one of its heads three more would grow in its place the hydra is anti-fragile because the more you harm it the stronger it gets so when you hear this idea of anti-fragility and you've spent any time in the real world which is constantly changing a natural question is how can i be anti-fragile i'll talk about seven points toward this idea of anti-fragility so i'll just briefly talk about these seven points and i'll leave further exploration and discussion for future blog posts and videos so the first point is having more upside than downside another way to think about this is limiting your losses and leaving your gains unbounded and really this is the core idea behind anti-fragility naseem gives a more technical definition of anti-fragility and fragility in terms of something called convexity which i talk a little bit more about in the blog the second point is having options as opposed to not having options or in other words having a specific and detailed plan in life unexpected things inevitably happen if you sit down and chart out your whole life your five-year plan like this is exactly what my life's gonna be and this is exactly what i'm gonna get out of it you're setting yourself up for disappointment because i hate to break it to you it's probably not going to happen exactly the way you think right now so detailed plans and having no options is fragile while having options helps you be anti-fragile the third point is bottom-up traditions as opposed to top-down rules this isn't so much a specific protocol but is a potentially useful observation bottom-up tradition is evolution at work things get passed down through generations based on usefulness or some kind of appeal and then each generation adds their little twist and nuance to it and updates it based on their situation in other words traditions can adapt and evolve over time based on the needs of the people that practice them as opposed to top-down rules which are rigid it's kind of like having a detailed plan there's only one way in which the top-down rules typically work and if things deviate from how they were when the rules were written they're probably not going to give you the desired outcome the fourth point will sound kind of silly but it's failure failure makes you anti-fragile and success makes you fragile it's just like the king from the sword of democracy story the king is fragile because he's successful he has nothing to gain and everything to lose while failure and mistakes help you be anti-fragile because when you're a failure a loser when you have nothing no one expects anything from you you don't expect anything from yourself so you have a lot more to gain than to lose just like when you hit rock bottom when you hit rock bottom there's literally nothing to lose and everything to gain point number five is in a similar vein and that is staying small as opposed to being big when you're small your mistakes are small but when you're big your mistakes are big it's like that old piece of wisdom mo money mo problems the bigger you get the bigger your problems get what would have just been a small inconvenience when you're small becomes a significant thing when you're big so the sixth point is diversification as opposed to specialization diversification helps you be anti-fragile and this is just like having options when you diversify you're essentially giving yourself more options naseem gives a specific protocol in the book called barbelling which is kind of like this 80 20 rule people typically hear it in the context of business like 20 of your clients earns you 80 of revenue or just something like that when naseem calls bar billing is when you put eighty percent of your resources in a low risk situation while twenty percent of your resources in a high risk situation so a concrete example could be working a nine to five job that pays you a steady salary which you spend 80 percent of your time on but 20 percent of your time you spend on something more risky like a new business venture or side hustle the problem with specialization is the same problem as having a detailed plan if something happens your specialization is no longer needed and in some ways you are no longer needed or if you put all your eggs in one basket you just specialize all your assets you specialize all your resources into one venture you put all your money into that high risk side hustle business venture there's a chance that you're gonna lose everything the seventh point is this idea of independence as opposed to dependence on others when you're independent either financially emotionally or however you want to look at it you're setting yourself up to be anti-fragile while if you're dependent on others for a house a car money a job a hobby whatever it is you're setting yourself up to be fragile so what this ultimately comes down to is if you lose access to these external things to other people or other things you're going to find yourself in an undesirable situation well if you're self-reliant you're not dependent on external things you're independent you're setting yourself up to be anti-fragile because you're not going to lose there's no downside to when your environment changes so as usual i probably rambled on way too much so i want to leave you with three key takeaways the first is something anti-fragile is something that benefits from things like uncertainty harm and disorder the second takeaway is we can classify things into these three categories as fragile robust and anti-fragile and by classifying things into these categories we can study them and hopefully learn what we should do and should not do based on our goals the third and final takeaway is we can do things that make us more anti-fragile and i highlighted seven points that can help us on this journey so there's no way i could have gone into all the details of such a big idea in one video that's why this video is just part of a larger series of videos and blogs exploring this idea of anti-fragility and trying to answer this question how can i be more anti-fragile so if you enjoyed this video please consider liking sharing subscribing and commenting your thoughts if you have ideas or answers to this question how can i be more anti-fragile please leave that in the comment section i think others and myself would love and benefit from hearing that and as always thanks for watching
tufdEUSjmNI,2021-10-26T13:23:44.000000,Causal Discovery | Inferring causality from observational data,hey folks welcome back this is the final video in the three-part series on causality in this video i'll be talking about causal discovery which aims at inferring causal structure from data start by introducing what causal discovery is sketching some big ideas and then finishing with a concrete example with code in python so with that let's get into the video in the previous video i was talking about causal inference which aims at answering questions about cause and effect we talked about a lot of great things we talked about the do operator which stimulates interventions we talked about confounding which talked about estimating causal effects and all these things were great however there was one key assumption that was necessary in order to do causal inference which was a causal mod and obviously a lot of times in the real world we don't have a causal model in hand when we're starting our analysis and it's not always clear which variables cause which causal discovery is one thing that might help with obtaining a causal model and the goal of causal discovery is to find causal structure in data so basically given data inferring the underlying causal model so causal discovery is an example of a so-called inverse problem and inverse problems can be understood in contrast to forward problems for example imagine you have an ice cube sitting on your kitchen counter you know the shape of the ice cube you know the volume and if you were to let that ice cube sit there for a few hours you could probably predict with some reasonable degree of accuracy what the resulting puddle of water would look like the inverse problem is like the opposite of this in other words the inverse problem would be given a puddle of water on the kitchen counter predicting the shape of the ice cube that made that puddle and clearly this is a hard problem because there are several different shapes of ice that could create the same puddle of water connecting this to causal discovery the shape of the ice cube is like our causal model and the puddle of water is like the statistics that we observe in our data so following this analogy there are several causal models that could potentially generate the same statistics we observe in a given data set the approach to solving inverse problems is to make assumptions basically we narrow down the possible number of solutions through assumptions and although assumptions help they often do not fully solve the problem this is where we need to use some tricks to go a little further here i'll talk about three different tricks for causal discovery the first trick is conditional independence testing i start here with a definition of statistical independence which is shown here in other words two variables x and y are said to be independent if their joint distribution is equal to the product of their individual distributions from this we can get a definition of conditional independence which is basically the same thing however now we look at distributions of each variable when conditioned on a particular variable say z we can use this idea of conditional independence testing to do causal discovery and this is actually the main idea behind one of the first causal discovery algorithms called the pc algorithm which is named after its authors clark glymore and peter spears i probably butchered that so i apologize but there's a reference to a review paper by them at the bottom here so i'll just briefly go through the main idea of the pc algorithm more details can be found in the blog linked in the description the first step is to form a fully connected undirected graph so we have a node for each variable in our data set and we connect undirected edges between each of these nodes in step two we do pairwise independence tests so we do an independence test between every possible pair of variables and if two variables are independent we delete the undirected edge between them the third step are conditional independence tests so basically we do the same thing however we pick a variable to condition on then if two variables are found to be conditionally independent we delete the edge between them and we add that conditioned node to the separation set and we continue these conditional independence tests until there are no more candidates for conditional independence testing then in step four we orient colliders so if we have three variables say i j and k we form a collider out of them meaning we make directed edges pointing from i to k and j to k given k is not in the separation set of i and j then in step 5 we add more directed edges to the graph following two constraints namely we do not create any new v structures in our graph nor do we create any directed cycles and hopefully after all that we output a directed acyclic graph which represents the causal connections of our system again more details on the blog and the two references at the bottom here have a great description of the pc algorithm so trick two is a greedy search of the dag space so there are three key concepts here first is a dag which should be familiar since they've been discussed in the previous two videos next is a dag space or in other words the space of all possible dags for example consider the space of dags with two nodes in one edge which is shown here there are only two possibilities x could point to y or y could point to x then finally we have the notion of a greedy search which is a widely used idea in optimization in short a greedy search is an optimization strategy that picks what's best in the short run as opposed to the long run and this is usually done using a heuristic or rule of thumb for example suppose you're trying to get out of a forest you may think i'm trapped in a forest forests have trees so to get out of the forest i should go where there aren't any trees in other words every step you take should be in the direction with the least number of trees so you repeat this strategy and go all the way along this black line until you finally get out of the forest which we can call the greedy path because it is the result of a greedy search however if at the start you were to go in the exact opposite direction of the greedy path you would make it back to civilization much faster so you might say why would we ever want to use a greedy search from the look of it they just seem to give some optimal solutions well the problem is that a lot of the time computing the optimal solution is intractable meaning if you ran an algorithm that tried out all possible solutions all possible paths out of the forest and compared them to each other you would be waiting a long time for it to run and maybe your grandkids would see the solution in their lifetime and this is the problem we face in causal discovery we want to find the optimal dag that best explains a given data set the problem is if we tried an exhaustive search we have to deal with the fact that the number of possible dags is a super exponential and the number of nodes in a graph in other words if we have just three variables the number of possible dags is 25 if we have six nodes we're already over three million possibilities and if we have a measly ten variables or ten nodes in our graph the possible dags is on the order of 10 to the power of 18. so even though greedy searches do not guarantee the optimal solution at least they give us a solution in a reasonable amount of time and so a causal discovery method that uses a greedy search is the so-called greedy equivalence search algorithm so the basic idea of this algorithm is you start with a complete unconnected dag and you iteratively add edges to this unconnected graph such that you maximize a score value so in other words you start with a set of nodes that correspond to each of your variables but no edges between them then you add edges one by one according to some score so the question is what is the score that i'm talking about basically this quantifies how good your dag is or how well the dag explains the data so there are a few options to defining this score one is the so-called bayesian information criterion and source number one reference at the bottom here has a brief discussion for anyone that is interested then you can repeat this process until you reach some stopping criterion which could be some number of edges have been added or the score stops increasing or whatever that may be okay so the final trick is exploiting asymmetry and so as i discussed in the first video asymmetry is a fundamental property of this causality framework so it's natural to think maybe we can leverage asymmetry to help us find good causal models from data and there are three flavors of asymmetry that i've come across and i'll say as a disclaimer that these aren't any kind of standard classification for these things these are just some labels i'm putting on some themes that i have gleaned from looking at this stuff so the first flavor is what i call time asymmetry which is based on the idea that causes precede effects this is what is used in granger causality which is a method to quantifying a asymmetric relationship between two variables based on prediction and more information about grainger causality can be found in this first reference here and there's a lot of stuff out there on grade your causality you can just do a simple google search and you'll probably find a bunch of stuff the second asymmetry is what i call complexity asymmetry which is basically the occam's razor principle that simpler models are better so going back to our ice cube example from earlier following this principle we would say the ice cube that's actually cube is preferred over the more complicated eyes because it is simpler and finally the third flavor is what i call functional asymmetry where better functional fits are better candidates for a causal model so one method that uses this is the non-linear additive noise model the way this works is suppose we start with two statistically dependent variables x and y we then model y in terms of a non-linear function of x then we compute a noise term by taking the difference between y and this non-linear functional fit and then finally we test whether the noise term n is independent of x if it is we accept the model and say x causes y and if not we reject it and then we can do the same thing in the opposite direction where we model x in terms of a non-linear function of y and repeat the same procedure and more details on this method can be found in reference number three and generally all of these are great resources if you're trying to learn more about causal discovery okay so wrapping up these tricks we have a trick based taxonomy and it's important to note that these tricks are not mutually exclusive in all cases there are indeed algorithms that will mix and match different ones for causal discovery as shown in the bottom row of the table here and as a bit of commentary causal discovery seems to me at least to be a relatively young field so there still has not emerged a single or small set of causal discovery algorithms that beat out all others in all situations and i'll also say that this is by no means an exhaustive list of causal discovery techniques however this is probably a good start for anyone trying to get into causal discovery and the references given at the bottom here can get the ball rolling for you okay so i will conclude with a concrete example like in the previous video so we're going to be using the same census data set as before but instead of having just three variables of age education and wealth we're going to include more variables and instead of using the microsoft do y library for causal inference we're going to be using the causal discovery toolbox so again first step is importing libraries loading data then for a lot of these causal discovery algorithms it helps to start with a so-called graph skeleton so this is like step two that we saw with the pc algorithm where we do the pairwise independence testing and we have bi-directed edges or undirected edges between variables that are statistically dependent and then you can visualize the network pretty easily using network x so the first causal discovery algorithm that i use here in this example is the pc algorithm so again we just do that in two lines and it spits out this causal graph the graph is somewhat reasonable it's not perfect but we can see that we have has graduate degree which is like our education variable causes a variable greater than 50k which is our income variable and then we also have age causing our income variable which is what we expected but what was not expected is our education variable has graduate degree is pointing toward age so this is saying whether or not someone has a graduate degree has a causal impact on their age which is not true if you give someone a graduate degree it's not going to have any effect on their age another interesting thing is we have several variables having a causal effect on the number of hours that someone works in a week so whether or not they have a graduate degree has a causal effect on the number of hours they work their age has an effect and whether or not they're female so this is basically their two options male or female in this data set and then the ethnicity information captured by is white is a bi-directional so the pc algorithm wasn't able to break that symmetry but what's interesting is uh hours per week causes a single variable which is in relationship so what this is saying is the number of hours you work per week has a causal effect on whether you're in a relationship or not so we could look at this all day and kind of craft whatever stories we want in our minds but this should definitely be taken with a grain of salt so the next algorithm that we try out is the greedy equivalent search algorithm which uses trick number two greedy search of the dagspace and this gives us a causal graph that is somewhat similar to what the pc algorithm gave us notably that edge between hours per week and is white the symmetry was broken so it's not a bi-directed edge and then finally we use the lingam algorithm and this one doesn't really give us something sensible it's basically everything is causing past graduate degree so whether you make more than 50 000 impacts your graduate degree how many hours per week you work has an impact on your graduate degree and these edges seem backwards this algorithm doesn't seem to do a great job and that's because it's assuming linear relationships between variables and since most of these variables are boolean that's not something that necessarily makes sense code can be found at the github linked at the bottom here put the link in the description feel free to take this data run with it feel free to leave a comment i'd be interested to hear the results of your analysis so that concludes our series on causality we started in the first video introducing this new science of cause and effect the second video we talked about causal inference and finally in this video we concluded with causal discovery if you enjoyed the series please consider liking subscribing sharing commenting your thoughts if you're interested in learning more check out the blog check out the github to get your hands on the example code discussed in this video as always thanks for watching [Music] you
PFBI-ZfV5rs,2021-10-15T22:02:16.000000,Causal Inference | Answering causal questions,hey folks welcome back this is the second video in a three-part series on causality in this video i'll be talking about causal inference which aims at answering questions involving cause and effect so i'll start by giving um introduction to causal inference and sketching some big ideas and then i'll finish with a concrete example with code using the microsoft do y library in python so with that let's get into the video okay so here we're talking about causal inference which aims at answering questions about cause and effect so given a causal model here we have a directed acyclic graph which i talked about in the previous video and from that how can we estimate causal effects for example how can we estimate the effect of x on y so some examples of questions that fall under the umbrella of causal inference are did the treatment directly help those who took it or was it the marketing campaign that led to increased sales this month or the holiday or how big of an effect would increasing wages have on productivity so these are very practical and significant questions that may not be so readily answered using traditional means and i'll try to highlight what causal inference is good at through what i call the three gifs of causal inference so the first gift is the do operator and the do operator simply simulates a physical intervention and we're all familiar with interventions in the real world this is like when your friend's candy habit gets completely out of control and you just have to sit him or her down and say this has got to stop this is what the do operator does but for a causal model in other words it is a mathematical representation of an intervention so suppose we have this model on the left here we have z causes x which causes y what an intervention in x looks like in this mathematical representation is we delete all the incoming edges into x and manually set x to some predetermined value say x naught so significant contribution from judea pearl and colleagues are the rules of do calculus what these rules provide is a way to translate probabilities that include the do operator into probabilities that do not include the do operator so the power of this is that often we can't perform interventions in the real world this could be because it's physically impossible or unethical or whatever reason for example intervening in someone's height by making them taller to measure the response in basketball ability is not physically feasible or intervening in smoking by forcing someone to smoke a pack of cigarettes every day to measure the response in the risk of lung disease is unethical so in other words often in the real world we have no way to collect data about the interventional probability distribution that is we don't have access to data about probabilities that include the do operator in these situations the rules of do calculus may provide a way to re-express to rewrite probabilities that we are interested in but can't measure directly so the second gift of causal inference is clarifying this notion of confounding and confounding at least for me was something that did not have a clear definition until i read judea pearl's book the book of why so in his book pearl defines confounding as anything that makes the interventional distribution different from the observational distribution in other words anything that makes a probability of y given an intervention in x different from the probability of y given an observation in x so this is easy to see in the three variable case so here we have an example of a causal model which shows the relationship between age education and wealth in this example age is the confounder and this can be understood as age is a common cause of education and wealth which is an idea that's been around for a while as pearl discusses in his book many people took this kind of common cause definition as a definition for confounding but what pearl does in defining confounding in this way in terms of the interventional versus observational distribution is becomes much more easy to generalize this notion to much more than just three variables okay so what does this mean practically if we know age is a confounder this can help inform our analysis of data that we might collect of these three variables so suppose we have this data here of age education and income and we want to assess the impact of education on income if we don't take into consideration age bank a confounder the naive thing to do would be to just partition the data into two subgroups one group has just a high school education and the other group has a college education and just compare their difference in income but since age is confounder this wouldn't give you the best result so knowing what the confounders are of your problem allow you to perform this analysis a different way so in this specific case since age is a confounder we shouldn't compare data between age groups we should compare data within age groups so that's what i'm showing here you can imagine this single data set being split off into four separate data sets uh where we have the blue data set people in their 20s the yellow dataset people in the 30s people in the 40s in red people in the 50s in green and then we repeat the analysis i was talking about before where we kind of compare the incomes of people without just high school education versus college education so you may ask why do we care about this do operator why do we need to talk about interventional probabilities versus observational probabilities and so on ultimately what these tools provide are a way to estimate causal effects so a causal effect is a way to quantify the causal impact that one variable has on another and this is a core part of causal inference so this is what we were naturally doing in the previous slide when we were trying to assess the impact of education on income what we were really doing is quantifying the causal effect that education had on people's incomes but this is obviously applicable to other situations when we ask questions like what productivity be increased if we increase wages or how would sales change if we increase the marketing budget with these questions and several more we're talking about causal effects what is the causal impact of wages on productivity what is the causal effect of marketing spend on sales so looking at the same example as before we have a causal model including age education and wealth we know from the previous slide that age is a confounder because it creates a discrepancy between the interventional and observational probability distributions we can consider education to be a treatment and wealth to be a response to that treatment and then suppose with this causal model in hand we collect some data very similar to what we were talking about in the previous slide but now we're set up to do causal inference we're set up to ask and attempt to answer a question involving cause and effect so a question might be is grad school worth it which might be something someone watching this video is thinking about or something someone is reminiscing upon and wishing they would have known about causal inference before deciding to go to grad school and they're already waist deep into it either way one way to frame this question of is grad school worth it could be what is the treatment effect of education on wealth i'm not saying this is the best way to do it but this is a way we can do it so i'll use this opportunity to run through a concrete example with code in python so the example code is at the github link at the bottom here i also put the link in the description but basically here we're going to estimate the treatment effect of education on income so first we download some libraries load some data this is real census data from the uci university of california irvine the machine learning data repository i don't know the specific name but here is the uh link here to do the causal effect calculation i use the do y library which is a microsoft library for doing causal inference so the next step is we have to define our causal model so again the starting point of all causal inference is a causal model so we need to start with our dag which is the same as we saw in an earlier slide just that education now has a new name called has graduate degree and income has a different name which is greater than 50k so these are both boolean variables which means they're true or false variables so either someone can have a graduate degree or they don't or either they make more than fifty thousand dollars a year or they don't and then age is just an integer next we need a s demand which is basically a recipe for estimating our causal effect you can just do this in one line using the do y library and then finally we can estimate the causal effect so here we're using a t learner which is a type of meta learner i can link a paper talking about meta learners in the description i won't jump into all the details i'll just kind of jump to the result which is the average causal effect is 0.2 so one way to interpret this is having a graduate degree increases your chances of making more than 50 000 a year by 20 however we had a lot of samples in this data set and we've just reduced all those samples to a single number which was the average which may not always be the most representative number so it's always good to plot the distribution and when we plot the distribution so here we have on the x-axis the causal effect the y-axis is the count the number of records or people that had that individual causal effect we see that the distribution is not gaussian so if the distribution is not gaussian that means the average is not a very representative number for that distribution so in other words even though a lot of people had a 0.2 treatment effect there were also a significant number of people that had no treatment effect so it seems we're no closer to answering the question of is grad school worth it however one thing one could do is to dive into these different cohorts kind of look at the samples that had no causal effect from a graduate degree and then look at the people that had a significant causal effect and then you can start to answer the question like what kinds of people benefit from a graduate degree and what kinds of person don't benefit from a graduate degree and then maybe that can kind of help you answer this question so again codes on the github feel free to take it run with it do whatever you want extend the analysis further post your own youtube video about it i'll be really interested to see if anyone actually takes a look and tries to answer this question of is grad school worth it but i guess it's a little too late for me at this point so that was the second video in the three-part series on causality we talked about causal inference which aims at answering questions involving causality however the starting point of all causal inference is a causal model which may not be so easy to have in hand that's where the topic of the next video can be helpful which is causal discovery and that aims at obtaining causal structure from data alone so if you enjoyed this video consider liking subscribing sharing commenting your thoughts i'm always happy and interested in reading the comments check out the blog if you want to get some more details on causal inference and check out the github to get the example code talked about in this video and thanks for watching [Music]
WqASiuM4a-A,2021-10-04T20:02:15.000000,Causality: An Introduction | How (naive) statistics can fail us,hey folks welcome back i'm finally sharing another data science series this video is the first in a three-part series on causality so this idea of causality is mainly based on the work of judea pearl and other researchers working this space pearl actually has a very accessible book out called the book of why geared toward a public audience which i will share in the description in this video i will introduce this idea of causality kind of highlight why traditional statistics isn't the most helpful for understanding it and then finally introducing a new mathematical formulism for understanding causality if you want to dive a bit more into the details check out the blog which i will link in the description and without any further ado let's get into the video well you're probably thinking is why is there a banana on the screen why did i click on this youtube video so we are constantly asking ourselves why why did this happen what is the cause of this or where is this going what's the effect we ask ourselves why to help us craft stories narratives to help us make sense of the world and even though this is a very natural thing for us in understanding and reasoning one of our most powerful tools in statistics is in many ways inadequate for handling cause and effect i'll try to highlight these inadequacies with what i call the three traps of statistics the first trap we have is spurious correlation so this is a statistical correlation with no causal implication so this is like the old saying correlation is not causation and you don't have to look far to find examples of this uh there's a website uh tylervegan.com i have it at the bottom left here and i'll link it in the description as well so here we have a case where we have a spurious correlation so the number of people who drown by falling into a pool correlates with the number of films nicholas cage appeared in so even though this relationship is hilarious it is not causal because we know these two things are not causally uh related to each other correlation is not causation which is something that we all know so sperry's correlation is pretty well known we've all heard correlations not causation uh however trap number two is less well-known and this is simpson's paradox which basically um highlights that how you look at your data matters so let's imagine we do a study for an experimental treatment for heart disease and we collect a bunch of data and we plot it so on the x-axis we have our experimental treatment this could be a drug or behavioral protocol the y-axis we have risk of heart disease and if we look at the plot we would say to ourselves this is a terrible treatment for heart disease it seems the more treatment someone gets the higher the risk of heart disease however if we were to look at two subpopulations say men and women we would get the exact opposite effect so this is summarized nicely by a quote from the man himself judea pearl who said we have a treatment that's good for a man good for a woman but bad for a person here's another example of simpson's paradox but with numbers i took this from the wikipedia page on simpson's paradox so here we have batting averages of derek jeter and david justice over the years 1995 and 1996 so if you look at those two years individually you see that david justice has a better batting average but if you were to combine those two years together derek jeter has a better batting average so again how you look at your data what variables you condition on how you slice your data set has an impact on the conclusions that you can make the final trap of statistics is symmetry which from many perspectives isn't much of a setback but when you're talking about something like causality which is inherently asymmetric it can cause some issues so i'll highlight this by an example let's say we want to model the causal effect between a disease and the severity of symptoms so we model this by a linear expression so y is the severity of our symptom x is the severity of a disease b is all other factors involved and m is just a coefficient that relates x and y but here we have an equal sign so the left left-hand side equals the right-hand side that's what equals means so that means using algebra we can rearrange this expression to get a equation of x in terms of y but here's the problem if we interpret the first equation as diseases cause symptoms then we have to interpret the second equation as symptoms cause disease which is not true we know that's not true so this fundamental symmetry makes algebra perhaps not the best formulism for representing causality so this whole video is supposedly an introduction on causality and i have not defined what it is there are few ways we can define causality the one i like is x causes y if when all confounders are adjusted an intervention in x results in the change in y but an intervention in y does not necessarily change x so i have a little cartoon here let's say we have four variables x w z and y if we intervene in x that means we jiggle it a bit we if x causes y we'll see why jiggle as well however if x causes y but y does not cause x if we intervene in y that is we jiggle y a bit x will not respond so that's causality it is fundamentally asymmetric so if we can't use algebra which relies on symmetry it has this equal sign how can we represent causality so there are the so-called structural causal models which is the kind of way we can represent causality and this consists of two parts one is a directed acyclic graph or a dag so this is a type of graph which comes from the mathematical field of graph theory which consists of vertices these circles here and edges which are these arrows and this is called a directed graph because the lines connecting the different uh circles together have arrowheads on them so that's called a directed graph because the information so to speak flows in one direction and it's acyclic because if you start at a vertex or one of these circles and you follow the arrow heads you'll never return back to the same variable or the same vertex so that's a directed acyclic graph and then there's a second part which are the structural equation models so these are equations that kind of outline the details of the causal connections and so they have these funny looking equal signs here which is basically saying you can't invert these expressions for example you can't invert f sub 1 to get an equation for x in terms of w so these are two key pieces of causality so that was the first video in the three-part series on causality in the next video we will be applying this idea to answering practical real world questions with causal inference if you like this video please consider liking subscribing sharing and commenting your thoughts if you're interested in diving a bit more into the details check out the blog on medium thanks for watching
m19FqRrmvIE,2021-07-18T23:47:21.000000,Why Conflict Is Good & How You Can Use It,are we recording yeah have you ever all right let's just relax have you ever wanted to avoid an uncomfortable conversation or maybe tell someone off or raise your voice or even three hours later if yes then you've been part of a conflict hey folks welcome back this marks the second productivity video i hope you guys enjoyed the first one even though it's not doing so hot but i'm still gonna make this video because i love it in this video i'll be talking about conflict and even though conflict doesn't feel so good it can have a powerful and positive impact on our lives so there's a great book the five dysfunctions of a team this was probably what really sparked something for me um i'd always been interested in like working with people in teams just because you know just look around anything that has made a significant impact on the world it's not just one person doing it alone it's a group of people all moving in the same direction but in the five dysfunctions of a team the author patrick lencioni he describes five key dysfunctions that teams go to i'm not gonna go through all of them but i'm gonna highlight one of them specifically which is the fear of conflict the fear of conflict is widespread no one likes conflict it's very uncomfortable it's very unpleasant and like most things that are uncomfortable and unpleasant we will do almost anything to avoid them so a tail tail sign of the fear of conflict is what lencioni calls artificial harmony so basically what this means is even though there's no obvious conflict in your team no one is yelling no one is trying to imitate karate actions in front of each other there is kind of hidden conflicts there's things that bother people but no one says anything about it and that is bad the metaphor i like to think of is that's like storing barrels of gasoline in your basement all it takes is one little spark and the house goes kaboom so instead of storing gasoline in the basement teams people need to have controlled burns so in other words what that means is you need to have conflict which brings me to the solution or a solution to the fear of conflict which is ironically conflict itself so the solution to the fear of conflict is conflict itself and conflict is a very scary and negative notion in our minds so what makes the fear of conflict so bad is that well it's two things really one it kills honesty when there's hidden conflicts when you have these barrels of gasoline in your basement it kind of constrains the space of allowed speech you can't say certain things some things are taboo some things are off limits some things you know you're holding something back and if you say it it might explode and you know everything's going to be terrible so this fear of conflict kills honesty people can't be honest with each other and maybe not even honest with themselves and the second thing is without these conflicts teams don't go through the things they need to go through in order to build trust it's when we kind of overcome obstacles and challenges where relationships are really strengthened informed by avoiding conflict we are denying ourselves the opportunity for honesty and trust okay so now let's talk about fear the fear of conflict like many other fears doesn't usually stem from any kind of actual danger what it really comes from is the unknown so consider the example of driving to work driving is probably one of the riskiest and most dangerous things we do on a daily basis but no one's afraid to commute to work and that's because we do it every day it's familiar and things that are familiar aren't scary so in a similar way just like how everything every time you drive and you survive it becomes less scary every time a team goes through a conflict and survives conflict becomes less scary okay so conflict solves the fear of conflict but what else does conflict do i think there are two main things two main benefits of conflict number one if there's conflict that's a signal that there is diversity in your team and diversity is not just like a you know corporate term or something that people do for their pr diversity is actually critical if you're trying to solve a diverse set of problems and then the second thing which i touched on earlier is that conflict builds trust it's when we kind of experience hard times we experience something uncomfortable and come out the other side still alive that we start to trust each other and form long-term relationships and here's the reality of conflict conflict isn't just arguments it's not just fisticuffs conflict is a spectrum it could be a reasonable rational disagreement with someone or it could be a full-blown you know rumble royale whatever they call them and even though this isn't directly helpful okay conflicts can be you know reasonable or it can be very unreasonable even though that's not directly helpful what it does is it highlights the flexibility we have in what kind of controlled conflicts we engage in so a conflict could be normalizing criticism it could be making sure everyone has the opportunity to say their opinion even if it's vastly different than what other people might think or it could just be promoting productive discourse or discussions and we can actually use conflict as a tool as opposed to an obstacle so the first thing is conflict is uncomfortable which is obvious but what this means is any sign of discomfort like the idea of talking to a co-worker about something that happened the other day or something they may have said if that notion is uncomfortable to you then that is a sign of underlying insecurity and or underlying conflict so what does this mean that means discomfort as opposed to being an obstacle is an opportunity for us to resolve underlying conflicts or resolve underlying insecurities that our team may have so what this does is it informs a three-part kind of three-step protocol for growth the first step is instead of moving away from discomfort you move toward discomfort this could be in the context of a team or even in the context of an individual or a personal life the second when you move toward discomfort you uncover conflicts and then the last step is once you've uncovered conflict you've discovered underlying insecurities and if you follow that three-step protocol the first step being moving toward discomfort you're gonna find yourself uh growing at the end of it even if it's very uncomfortable and the last thing about conflict is although some conflicts are more productive than others i believe all conflict is good given a single requirement is met and that requirement is people need to be willing to see it through to the end people need to be willing to stick around to see the resolution if that happens i believe any kind of conflict is good conflict what makes bad conflict is things go wrong things get heated  happens quoting the last video and people run away people avoid the resolution because they feel some negative emotions or something that's the worst thing you can do because you're denying yourself all the benefits of conflict by not seeing it through to the end and the last thing this is really the last thing people think that the worst thing for a team is conflict and that's wrong the worst thing for a team is apathy apathy is a sign that people have given up on the team the effort of being involved in the team is not worth the benefits of being in the team and apathy is really the start of a countdown once apathy sets in if people don't if that apathy is not resolved the teams start to fall apart so once again i rambled uh to a tremendous degree about something so i'll just kind of wrap everything up with three key takeaways the first takeaway is the fear of conflict is a fundamental dysfunction of teams with a unexpected solution which is conflict itself the second takeaway is even though moving toward conflict and discomfort is very unnatural it provides tremendous opportunities for teams and individuals and then the third and last takeaway is all conflict is good given everyone is willing to see it through to the end so that ends the second productivity video on conflict i hope you guys enjoyed the video if you have crazy experiences with conflict that you just gotta share please feel free to leave them in the comments section and as always like comment subscribe share and thanks for watching
OmNVB3ff98s,2021-05-10T20:13:32.000000,"Shit Happens, Stay Solution Oriented",i hate to break it to you guys but this camera angle is too close i hate to break it to you guys but happens and ironically when s happens people become less productive and more pro destructive i'm back with another video but this time i won't be talking about technical math and science stuff i know i know you guys are upset calm down this is the first of hopefully many videos on a completely different topic productivity so there are two main motivations for making this video one although i hope people have been getting some value from the technical videos i know they're geared toward a narrow audience and two which is hilariously clear to people that know me i have a bit of an obsession with optimization and i view productivity as the most practical optimization problem namely how can i get the most out of my time obviously this is not a question that can be answered in one video a series of videos or even a lifetime worth of videos however i want to share some hard learned lessons and general tips that have been helpful to me and my goal is not only to kind of share them so it can help you but also by putting this out there i'll have the opportunity to learn from you guys get your tips and your thoughts on productivity so each of these videos will focus around a central idea i'll kind of talk around these ideas and hopefully leave you guys with some practical tips and advice that you can implement in your day-to-day topic of this video is probably the first big piece of advice that i learned in my productivity journey which is stuff happens stay solution oriented which is happens stay solution oriented when i was an undergrad i worked at my university student activities board as an event planner and in event planning there's no way you can prepare for everything but what you can count on is something going wrong and when things go wrong there's one common trap that we fall into people start to play the blame game the blame game is a trap basically the goal of the game is to assign blame as opposed to work toward a solution you know there are too many negatives of the blame game to even list but the first few that come to mind are one it's a waste of time and energy basically every minute you spend playing the blame game the minute you don't spend working toward a solution or the second thing it doesn't solve the problem but it superficially satisfies negative emotions it's like a double-edged sword where both options are bad so not only are you not working toward a solution people will get this kind of like relief from the problem blame is like a proxy for the solution so they no longer kind of think about it or they don't think it's such a big deal anymore and kind of just move on to the next thing and then the last thing is that it can often breed further conflict so like you know you're doing an event or you're working like a service job at like retail or a restaurant you have an audience in front of you and the last thing you need is negative emotions to kind of hijack the situation and take you further from the solution and closer to even more problems so even with all these obvious flaws of the blame game it's natural it will happen you know people just through no fault of their own just kind of get wrapped up in all the emotions the heat at the moment and they just kind of need someone to blame in order to move forward the best piece of advice that i've learned kind of navigating situations like this if you're a leader just take the blame so your team can move forward and even if you're not in charge or you're not the leader or manager or whatever it is the same advice usually applies but uh it's probably a tougher situation to be in so this s stuff happens and we're supposed to say solution oriented during all this stuff how do we do that taking the blame is hard and staying solution oriented with all these negative emotions flying around is also hard our bodies want to keep us safe we're ready to run we're ready to fight we're ready to do all these things you know in a survival context makes a lot of sense in our everyday situations everyday problems not very helpful unfortunately i don't know any hacks on how to stay solution oriented and it's not something that comes naturally usually for me i just try to think of things very simply i ask myself what's the problem then what's the solution everything else is irrelevant so a big part of this is you know exactly managing all these negative emotions like your own that of your team um you know part of a great team that's pretty easy you know you have that trust that safety um and it's pretty easy to talk about um things that are uncomfortable but a lot of teams they have like underlying conflict which is okay you know as long as you're working toward trying to make things better it can be very challenging to manage the negative emotions i really buy into this idea that uh emotions are infectious so like how do we use this the basic idea is if you're able to maintain a positive and productive mindset you know keeping things simple what's the problem with the solution staying solution oriented focusing on the positives this mindset will kind of cascade out to everyone on your team whether you're in charge or not you don't have to explain anything to them all you have to do is be you just gotta do that and naturally we pick up on each other's emotions and we just kind of conform no one wants to feel negative no one wants to be stressed and no one wants to be mad so i feel like there is like a bit of a gravitational pull uh of being positive so find a way to be solution oriented i i try to keep things simple i'm sure there are a bunch of ways if you have ways that you do it that's really helpful to you please share that in the comments i'm very interested and then on this topic of negative emotions you know stress stress is what we think of when we think of negative emotions and it makes me think of a story an idea a thought i learned from an old colleague of mine kj he was a master salesman he kind of had every position you could have in the auto industry from sales person all the way up to general manager which is kind of like as high as you can go eventually he was the general manager for a new car mercedes-benz store but basically i'm paraphrasing uh if you want to hear the full story to my it's in the blog in the description basically he said he doesn't get stressed it's simple there are things he can control there are things he can't control for the things that he can't control there's no need to worry about it the things he can control is just a matter of knowing the solution knowing the process and just doing it stress is when people worry about the things they can't control or they don't follow the process they don't execute the solution for the things they can't control so this kind of goes back to this idea what's the problem what's the solution you can't just at will remove negative emotions they're here you have to find a way to kind of navigate that situation and a lot of times if you say i don't want like don't think about that thing or you try to ignore something you try to ignore negative emotion it does the opposite it actually brings it in in my experience if you say don't think about this don't think about this every time you say that become it gets closer and closer and then that's all you can think about what's usually more helpful more doable is to not ignore the negatives but try to focus on the positives don't ignore don't try to ignore uncertainty just try to focus on certainty you know your attention and your focus is limited so there's only so much that you can you know think about and put in your head at the same time so if you just fill up all that all those slots of readily accessible memory with certainties and positivity and solutions there's no room for negatives there's no room for uncertainty there's no room for unproductive things so to package that all up it's about managing what you're focusing on not what you're trying to avoid all right so i rambled about a lot of stuff today i'm just gonna summarize everything with three key takeaways okay so one beware of the blame game two take the blame so everyone can move forward and three focus on the positives and others will follow and as always like comment subscribe share that's it thanks for watching
zJsl4lFyr6w,2021-04-05T19:19:10.000000,Kmeans-based Blink Detecter DEMO,n/a
GgLaP4Des1Q,2021-03-17T14:53:33.000000,Independent Component Analysis (ICA) | EEG Analysis Example Code,hey guys welcome back this is video number two in the two-part series on principal component analysis pca and independent component analysis or ica ica is the topic of this video so much like the last video i'll start with a brief introduction into the technique dive into the math a little bit i will compare the two approaches talk about their similarities and differences and then i'll finish with a concrete example of how you can use ica with some example code provided in the github repository so let's get right into it okay so the standard problem for independent component analysis is the cocktail party problem so in its simplest form you can think of two people having a conversation at a cocktail party and for whatever reason you happen to have two microphones kind of set up next to these two speakers both microphones are going to pick up audio from both of the speakers uh kind of like our purple microphone and pink microphones here the purple microphones a little closer to the blue speaker so it picks up more of the blue speaker's audio relative to the red speaker and vice versa the pink microphone picks up more audio from the red speaker than the blue speaker so then the problem is how can we take these audio recordings that have both speakers kind of side of the conversation mixed together and separated out um to two audio files uh each of which only contain audio from a single speaker well that's exactly what independent component analysis does it trans uh transforms a set of vectors so you can think of the raw or you can think of the recorded audio by these two microphones into a maximally independent set so so that's what's being represented here um so you have the purple and pink audio signals then they get translated uh to the original the sources of the audio which was uh the speech from the blue speaker and then the speech of the red speaker respectively so again the purple and pink are your measured signals the blue and the red are the independent components or the source of the information or the audio okay so there are a couple key assumptions to independent component analysis so assumption number one your independent components are statistically independent and that's defined in the typical way in statistics the joint distribution of two variables x and y is equal to the probability distribution of x times the probability of y and then the second key assumption is that your independent components are non-gaussian which might be a little strange you know in statistics and science we love to say everything's gaussian it makes things much nicer and it allows us to do a lot of rigorous analysis but this is one of the instances where we actually need the independent components to be non-gaussian for this to work okay so we have our measured signals again that's from your microphone example and then the independent components which is what your speakers are saying in the cocktail party problem so we can use our independent components we can combine them in some way so that's what's being represented by this expression to kind of recreate our measured signals x you can think of the independent components as being sources hence that's why this vector is an s they are sources of information or audio that are being combined in some way to generate what's being measured at your microphone for example so we have x1 and x2 your measured signals and then your independent components or the sources of your signals s1 and s2 but you can also kind of turn this around and you can combine your measured signals to express your independent components if this is the case if you can just have some linear combination of your measured signals to derive your independent components the set of values defined by w is all we need in order to do ica okay so mathematically the goal is as follows so given some uh measured signals given some data x we want to solve for the matrix w such that the set of independent components or the set of source vectors s sub i are maximally independent this concept of maximally independent what does that mean how do we quantify that so there are two ways you can define w in such a way that it minimizes the mutual information between all your independent components or you can maximize the non-gaussianity of the independent components defined by uh this w matrix um i'm not going to go any further than that if you're interested in more information check out the blog post linked in the description i kind of go into a bit more detail on the math pc and ica they're similar techniques in a lot of ways but ultimately they're distinct they are different approaches that kind of aim at different tasks or they make different goals so pca typically compresses information if you saw the pca video the example was hot dogs and hot dog buns those are two quantities that are heavily correlated so instead of representing that information with two variables you can represent it with just one and so that's where pca is a good thing to use because it will compress those variables into those two variables into a single variable on the other hand ica separates information it's going to take two variables for example like your two speakers or the the audio picked up by two microphones placed close to two speakers and it's going to separate out the independent components or the sources or the independent drivers of those measured signals so kind of similar but they are different goals and different final outcomes a commonality between pc and ica is auto scaling so this is a critical part of the preprocessing so auto scaling is for each variable you have to subtract the average of that variable and divide each element by the standard deviation of that variable and then that's one of the reasons why it's typically advantageous to apply pca to your data set before applying ica because it kind of all the pre-processing is already handled for you pca will clump all the information together the correlated variables and then ica will come in and separate out independent drivers if it's applicable okay so as always i'm going to include a concrete example so here this is something that that's relevant to my research and this is where i kind of came across the whole technique of independent component analysis was to solve this specific problem in my research we deal with uh eeg data so what is eeg uh eeg is a technique of measuring brain activity uh by placing a set of electrodes on the head you know eeg is a very powerful technique because it has a very good temporal resolution and it's also non-invasive people can kind of move around with the this cap on but that kind of also leads to one of its more fundamental weaknesses is that since the electrical signals that it's trying to measure from the brain are so weak eeg has to be very sensitive to these kind of fluctuations and voltage which makes it very prone to artifacts or perturbations oscillations in your signal that do not come from brain activity so this could be like blinking which is the what we're trying to resolve in this problem or motion artifacts people talking or other kinds of noise that can kind of get injected into the data so here we have a plot of the voltage versus time sorry i should have had labels on these axes but the y-axis is voltage in millivolts the x-axis is a time index essentially and this is for the fp1 electrode which sits near the front of the head on the left side so on your left forehead and so this electrode is particularly prone to blink artifacts because it's the one of the closest electrodes to your eye and then you can actually see the blinks occurring because you'll have these giant spikes in the signal so we're trying to get rid of that because with eeg we're trying to measure brain activity not blink activity okay so the first step here is applying pca so here we have 64 electrodes on our eeg so that translates to 64 variables so we can use pca to kind of clump that down to just 21 variables and i just did some trial and error to find the right number of principal components to go down to and at the bottom here you can see the explained variation is 99.5 percent and in matlab it's really simple you may notice i don't explicitly auto scale the data that's because the pca function in matlab does this automatically which is pretty nice but it's all done in one line in matlab and it can be done in one line in psyc or a couple lines in psychic learn and if you didn't check out the previous video on pca that'll share some example code on how to do that okay and then again we can apply ica to the set of principal components that we got from pca so that's what's being done here so now we can just plot all the independent components okay so again we had 64 electrodes on our eeg cap that translates to 64 variables we then use pca to reduce the dimensionality from 64 variables to just 21 and then finally we applied ica to those 21 variables to kind of separate out the independent components and then just looking at this visually uh kind of independent component 10 5 and 12 are reminiscent of those blank artifacts we saw in that initial plot and again these aren't the just the independent components themselves or the raw independent components i actually squared them so that all the values would be positive and then the blank artifacts would be a bit more prominent okay so i just used a rough heuristic basically i picked out the independent components which had four prominent peaks and so this isn't a robust way to do it i was doing something fast and wanted it to be repeatable so it picked out independent components 10 and 12 to correspond to the blanks okay so 10 and 12. i'll buy that maybe five should have been included but we'll see how it turns out okay and then we can essentially just drop independent components 10 and 12 because they contain blink information which we're not interested in we only want brain information so we can drop those two components and then just work backwards we'll reconstruct our score matrix basically the output of pca and then we can reconstruct our original 64 variables by going backwards in pca so doing that and plotting everything before the blink removal fp1 had these four prominent peaks corresponding to blinks and then afterwards uh they went away so it's kind of like magic and this is a rough way to do it there are other ways to do it but this is more so just as an example of what ica can be used for so that concludes the two-part series on principle component analysis and independent component analysis if you found this video helpful please like subscribe comment share i would very much appreciate that and i would love to hear your feedback i look forward to seeing you guys in the next video and thanks for watching
WDjzgnqyz4s,2021-02-22T19:39:27.000000,Principal Component Analysis (PCA) | Introduction & Example (Python) Code,okay this is good i got the angle quite on the set hey guys welcome back hey guys welcome back i'm back with another series if you missed the first one it's available on my channel it was on time series signals the fourier transform and the wave of the transform in this new series i'll be talking about two things one principle component analysis and two independent component analysis so principal component analysis or pca is the topic of this video so i'll give you a little intuition share some math and then i'll finish with a concrete example of how you can use pca to analyze the stock market so let's get right into it so the analogy i like to think of for pca is imagine like a massive rock band with like 20 members in the ensemble and you have you know two drummers several guitarists you have several keyboardists or pianists you have a string section a horn section vocalist percussionist the whole works so you have this 20 person band and you know that's not a big deal that's uh you know that's the kind of band made for huge arenas and stadiums but if a band like this is just getting started they're gonna have a hard time fitting in smaller venues like coffee shops and restaurants so a natural solution to this problem is to just kind of reduce the number of players at specific performances so instead of like a keyboardist pianist and whatnot you could just have one person on the keyboard instead of having multiple guitars you could just have one person do an acoustic guitar instead of two drummers and a percussionist you can have someone banging on the bongos and so on in a lot of ways this is basically what pca does so this is the big band on the left is before pca and then you can kind of boil it down to its core elements for the same band to play at the coffee shop but instead of uh a band you can think of a pca applying to a data set instead of musicians or players in the band you can think of the variables in your data set and instead of a song or the music you can think of what your data set is representing a bit more concretely principal component analysis pca reduces input dimensionality and redundancy so we can think of two variables x and y this could be something like hot dogs sold and hot dog bun sold which are directly correlated but in a lot of ways contain redundant information so it may be practical to represent this underlying information instead of through two variables through just one variable and then that's uh application of pca so we can transform our axes from this x and y axis to a new set of axes we'll call them pc1 and pc2 and then if you want to take it a step further you can just remove pc2 and just operate with one variable so essentially we've reduced the dimensionality from two variables x and y to just one pc one if we choose to drop pc2 okay so how does it work the basic idea the goal of pca is to reduce variable redundancy or input variable redundancy by creating a new set of variables where the variance along each subsequent variable is maximized so in the previous example we saw pictorially that we changed from a set of two variables hot dog sold and hot dog bun sold to a new pair of variables we call them pc1 and pc2 and essentially pc1 contained all the relevant information we needed and the way we got pc1 is basically rotated the axes to be kind of along this linear slope of points defined by the hot dog bun and hot dog sales what does that translate to mathematically so we can think of this situation so we have x which is a matrix of data where the rows are data records and the columns are variables we have w which is a vector of weights and then we have t which is a score vector and what i'm going to be calling a principal component so t is what we're interested in we have our data x and we're trying to find uh a w that is going to create this principle component for us okay so here's here's the magic of pca here's the trick to it all so the goal here is to maximize the variance of t subject to the constraint that the norm squared of w so w transpose times w is equal to one okay and then variances uh defined in the usual way so you take every element subtract the mean of the variable you square it and then you divide by uh the number of elements minus one and then you just add this up for every single element in the set of numbers um and so one really important thing when doing pca is you want to auto scale your data so basically what does that mean for each number in each column of your matrix uh you want to subtract the average and divide by the standard deviation so if we do that then the mean of the principal component will turn out to be zero which allows us to kind of drop the mean term in the variance here it turns out that the variance will just be equal to the norm squared of t divided by uh the number of elements minus one okay so what does that mean that means we can rewrite this optimization problem instead of maximizing the variance we can just maximize the norm squared of t because the the vector w that maximizes the norm squared of t is also going to be the same vector w that maximizes the variance of t okay so we can rewrite uh the optimization problem using our above expression for t and it turns out this is actually a pretty straightforward optimization problem to solve and don't be intimidated by the matrices and vectors we can use a very well known and common technique in calculus known as the method of lagrange multipliers which basically allows us to rewrite an optimization problem with constraints a constrained optimization problem as a optimization problem without constraints or an unconstrained optimization problem if none of that makes sense that's fine we just need these relevant expressions here so we can write out the lagrangian which is this l of x uh term here for our pca optimization problem and then we can have the associated equations and this is the exciting part here this first equation if we rearrange it is just an eigenvalue problem which is a standard problem in linear algebra and then the second equation is just a restatement of our original constraint so writing it explicitly here we can solve for the eigenvalue lambda and the vector of weights w using standard eigenvalue approaches if you're doing this in some programming language every programming language like r python matlab they're going to have built-in functions that allow you to solve this problem and then once we have this vector of weights we have everything we need we can just multiply that by x and we can get our principal component and then this naturally extends to multiple components so this we started out just looking for a single component but if you solve the eigenvalue problem your and you have n columns in your matrix x and x is square you're going to end up with n eigenvalues and n corresponding eigenvectors and then if you kind of sort these eigenvalues and eigenvectors from largest to smallest you sort from the largest eigenvalue all the way down to the smallest each corresponding eigenvector w is going to be a set of weights which define a principal component and the principal components associated with the larger eigenvalues contain more information than components associated with smaller eigenvalues so you can define some threshold like in the first slide where we could have just dropped pc2 because it wasn't giving us much additional information you can do the same thing and kind of truncate your variables after a certain amount of information is captured with your principal components okay so just as a recap principle component analysis it reduces input dimensionality and redundancy some key points are new variables are created to be a linear combination of input variables so that's kind of what we saw in the previous slide where you had a matrix multiplied by a vector of weights that's equivalent to a linear combination of your input variables and then each subsequent new variable contains less information we kind of saw that once you sorted your eigenvalues from largest to smallest the principal components associated with the larger eigenvalues contain more information and the principal components corresponding to smaller eigenvalues contain less information and then there are a lot of applications for pca relating variables together so if two variables get kind of clumped together kind of like hot dog bun sold and hot dog sold there's some underlying correlation there you can use it for clustering where you can transform your space from your original input space to like a new pca space and then you can do a clustering algorithm like k-means and then you can also do some outlier identification so you can plot all your points in your principal component space and just kind of visually inspect if there are any outliers all right so here's a fun example i guess at the outset i'm going to say i'm not a financial advisor i've never taken a finance class so in no way is this a recommendation of how you should invest your money this is just a fun example of what pca can do so here we're going to use pca to create an s p 500 index fund so an index fund is basically a set of investments that are meant to follow or track with a specific market the example codes on the github so i'll probably just fly through this i used the yahoo finance module to get real actual stock data so this is all real data this isn't made up and then i use pandas and numpy for all the number crunching so i write some code to input the ticker names from wikipedia and then graham guthrie had a nice medium post of how you can grab all these s p 500 names so i just stole some code from that post and made some edits okay then i pull s p 500 data for 2020 i drop nands get a pandas data frame of just close prices as opposed to all the other information that's available get a list of names ticker names of all the companies in the data frame so we have 253 rows and 499 columns so here i i guess the comments aren't updated so i apologize for that but here we're initializing pca with 10 components and then we'll ex we'll apply pca to our data set and we'll print the explained variants so you can see you know the first three components you're already at more than 90 of the explained variants uh if you just sum up the first three elements of that array there um okay and then we can create an index fund so there's countless ways you can do this i just arbitrarily took the weights defining the first three principal components i sum them together and then i only included the top 61 weights we can represent the uh overall portfolio of this index fund with a bar plot it's a natural way to do it so the y-axis is the relative weight you can also think of this as the number of dollars relative number of dollars you're gonna invest in each specific company and then the x-axis is just the individual ticker names okay and then we can see how our index fund compares to the actual s p 500 over 2020 and just you know visually approximately it doesn't do such a bad job there's some discrepancies uh along the way but everyone cares about percent return so if you would have just bought one share of every single stock in the s p 500 at the beginning of 2020 and then sold those uh same shares at the beginning of 2021 you would have made 20 return if you would have instead followed the investing strategy of this particular index fund derived from pca you would have made 25 so that was the video on principal component analysis i hope that cleared things up if you want to learn more about principal component analysis i have provided a link to my blog post on medium on the topic stay tuned for the next video where i'll be talking about a similar but different technique independent component analysis if you enjoyed this video be sure to like comment subscribe hit the bell share with your friends and family so they too can learn about principal component analysis thanks for watching you
MX7ymkYGiZ0,2020-12-21T00:24:45.000000,The Wavelet Transform | Introduction & Example Code,hey guys welcome back in this video we'll be talking about the wavelet transform hey guys welcome back uh this is the final video in the three-part video series on the 48 in wavelet transforms in this video we'll be talking about the wavelet transform which basically takes the ideas we learned in the fourier transform and extends it to a thing called wavelets so let's get right into it alright so in the last video we were talking about the 40 transform which was basically decomposing any signal into a basis of waves you can do the same basic idea but instead of waves you use something called wavelets so wavelet is simply a wave-like oscillation that's kind of localized in space or time whatever your x-axis is so here's an example we have the first derivative of the gaussian function or a bell curve and the equation is given here to the right of the plot so there are two basic properties of wavelets just like there were two basic properties of waves amplitude and frequency wavelets have two basic properties which are scale which is basically how stretched or squished our wavelength is and location since wavelets are localized and not extending from negative infinity to positive infinity like waves we need to know where the wavelet is located in space so here are some examples of kind of our example wavelet at different scales so if it has a smaller scale we're essentially squishing our wavelet if it has a larger scale we essentially stretch our wavelet and this is controlled by the parameter a in our first derivative of the gaussian function given above and then here's the same example with different locations so we can shift our wavelet a little to the left by changing this by tuning this b parameter or we can do this double whammy situation where we shift our way to the right and we squish it and then here's a reference if you want to learn more about wavelets okay so what's the wavelet transform so just like before with the fourier transform we decomposed our signal into a basis of waves we can decompose our signal using wavelets of various scales and locations so the basic idea is you get a wavelet you pick a scale and then you just slide it across your signal and then at every kind of time step or every location in space you multiply that wavelet by your signal and then you do this for one scale then you pick another scale and then you just keep going like that so here's a visualization of what it looks like and if you're familiar with convolutions that's exactly what we're doing here uh you're just convolving your signal and wavelets of different scales so why are wavelets useful if the fourier transform is so powerful as i was uh going on and on about in the first video uh why do we need the wavelet transform well one thing about the fourier transform is that it gives you global frequency information so uh sines and cosines are defined from negative infinity to positive infinity so it gives you a kind of global average of the frequency information in your signal and if you're interested in uh kind of oscillations that happen over a short time scale the 40 transform may obscure that information and that's exactly what the wavelet transform is good at extracting localized information on the second point the wavelet can not only extract the local spectral information but it can also extract temporal information at the same time which is really nice so you kind of have this trade-off of frequency information and time information but the wave of the transform is kind of like a happy medium of uh between those two and there's also a variety of wavelets to choose from so let's say the signal the sub signal inside of your audio signal that you're interested in has a characteristic shape then you can kind of choose your wavelet in a clever way such that it kind of matches the signal of interest the thing you're trying to extract from your signal and then this is just a lot of math i'm not going to spend too much time here but there are two basic kinds of wavelets there's the continuous wavelet transform and the discrete wavelet transform the major difference is the continuous wavelength transform you pick basically every possible scale and location to do your wavelet transform so you do scale 1 and 1.1 1.2 whatever your resolution is every possible scale uh while the discrete wavelet transform there's only a discrete number of wavelets uh discrete number of wavelet scales and locations that you use to do your transform and then here's just you know just some more general information wavelets create a complete set which means you can represent any function in terms of wavelets which is nice okay and then we're gonna try to bring everything together with a concrete example so this is something relevant to my research which is uh extracting our peak information which is basically the biggest peak you see in uh your favorite medical tv show in ecg which is basically the um electrical activity resulting from your heart beating so we can extract this characteristic our peak or this giant jump in the ecg signal using wavelets so we're using a specific kind of discrete wavelength transform called maximal overlap discrete wavelength transform in this example and you can find the this example code with all the plots and a nice pdf of it at the github link here okay so the first step is we do the wavelet transform here i picked a number of levels or a number of different scales to b6 and then we use the sim 4 wavelet for our wavelength transform so at the top we have our original signal you can kind of see the characteristic the signal of interest which is like this localized peak that's the rpeak we're interested in but the signal is really noisy so it's kind of hard to you know do a simple fine peaks function to extract the rpeaks but if we do this wavelet transform we can see at the first scale the smallest scale which will correspond to the highest frequency because our wavelength is very squished it just looks like a lot of noise so that's not really helpful we go to the next scale we can kind of see the rpeak signal coming out a little bit but still really noisy the next scale is a little bit better um the fourth scale is kind of the goldilocks we we see a really large oscillation near our peaks and then everything else is basically uh zero so that's promising and then we just keep going down and then you can see for the largest scale we have like this low frequency oscillation which makes sense big scale means lower frequency so the rest of the code is just making this plot here so now we can reconstruct our signal using the inverse maximal overlap a discrete wavelet transform uh so that's what that's what's happening here um you know just to run through the code really quickly we are only going to use that fourth scale which was oscillating a lot around our peaks and it was zero everywhere else so that was really promising so we're only going to use that information to reconstruct the signal and then matlab and a lot of programming languages there's probably equivalent functions in uh python or r uh that let you do the inverse maximal overlap discrete wavelet transform in one line so we're going to reconstruct it using only the fourth scale the two to the fourth scale and then we're going to use the sim 4 wavelet again and then the rest of it is just plotting this this image on the right here so the on the top we have the original signal so it's pretty noisy but you can still see where the rpgs are visually but then our reconstructed signal is a lot better behaved and you can see it's almost trivial um picking out the r peaks in the reconstructive system and then this last bit of code is just leveraging that so our reconstructed signal it's really easy to pick out the art peaks so we can just do a simple fine peaks function and then we can plot those peaks on our original signal and it does a reasonable job and so again you can find this code on the github if you want to take a closer look at it if you want to just completely steal it that's completely fine with me i kind of took a lot of this code from a matlab example that did a similar thing with a different data set so i encourage you to take a look at that so that was wavelets i hope you guys enjoyed these three videos found this video helpful like subscribe comment i'm always trying to get better improve my understanding learn grow all that fun stuff yeah thanks for watching you
rPUytg38b6Q,2020-12-04T01:10:36.000000,The Fast Fourier Transform | How does it (actually) work?,hey guys welcome back in this video we'll be talking about the fast fourier transform if you missed the last video it was a bit of a primer introduction on time series signals and the fourier transform so if you missed that definitely check that out and stay tuned for the next video which will kind of extend this idea to a thing called wavelets in the wavelet transform so let's get right into it alright so in the last video we were looking at the fourier transform so if you give it some function in terms of time or space it'll spit out a function in terms of frequency or wave number but most of the time we don't know a functional form of anything we're dealing with in the real world so that's where this idea of the discrete for you transform is very useful like we were looking on the previous video you can have an audio signal so you're playing your favorite record you record it with a microphone and then you can kind of digitize it so you have it in a form that your computer can understand and just how you can digitize your audio signal you can discretize your fourier transform so basically you convert your infinite integral to a sum over n elements so let's take a closer look at this expression so we have f f sub k is equal to the sum over zero to big n minus one uh with x sub n times e to the minus 2 pi i kn over big n so we can define this term let's call it big r sub k n and then we'll set that equal to e to the minus 2 pi i k n divided by big n so we can rewrite our sum in terms of these two elements and then just so we're all on the same page f sub k is an element of what i'll call a column vector x sub n is an element of a column vector and then r sub k n is an element of a matrix so because of the nice properties of matrix multiplication it turns out we can write our sum equivalently as matrix multiplication so that's nice discrete fourier transform is just matrix multiplication but so what how does this help us if we're coming to code this whole thing up we're still going to naively need two for loops to do this double sum this is what they call n squared time so that means the time complexity of this operation will take n square so if you have n elements you're going to have to do n squared computations so we converted our discrete fourier transform to matrix multiplication didn't seem entirely useful but let's just consider a concrete example so let's assume the case where big n equals big k so big n is the size of our column vector where x sub n was an element of and big k is the length of the column vector where f k is an element of that uh so we recall that big r sub k n is equal to e to the 2 pi i k n over big n and then we can just plug it in so we can plug in k and n which will correspond to the indices of our matrix and then big n is equal to four so we plug in those values and we get this matrix on the right here lots of ones lots of minus ones lots of eyes and minus sighs and just looking at it and this matrix is symmetric which means that if you were to swap the rows and columns it would be the same matrix as you started with so that's pretty nice and then this is a special element in the matrix so this element kind of defines the resolution of the discrete fourier transform so we can call it something special like k9 and then we can write our any fourier matrix in terms of this k naught and so this is for the four by four case that we have here and then we can write a few more examples so we have the eight by eight case which uh looks like that big scary matrix on the top there um where k naught is defined on the right hand side here uh we have the four by four case which is the same thing as before and then we have the two by two case and then again k naught is defined as e to the minus two pi i over big n and this extends to any fourier matrix of any size now where n where your fourier matrix has to be square and n gives you the dimensionality of that square matrix so this is the trick so we had the n squared there was a lot of symmetry in our four-year matrices we saw the symmetric matrix there were a lot of redundant terms a lot of ones a lot of minus ones a lot of eyes minus size uh in the 2x2 case the 4x4 case the 8x8 case so it would be really nice if we could leverage this redundancy and symmetry to make our algorithm a bit more efficient and that's exactly the idea behind the fast fourier transform so it's just a fast way of computing the discrete fourier transform and the basic idea is given by this expression so r sub n is our n by n for your matrix any fourier matrix you want that's n by n with the added caveat then n is a power of two that's an important point um and then it turns out you can write any for your matrix uh that's m by n by this expression here i can define these terms individually but i think it's better to just look at a concrete example so again let's return to our four by four case um so here we have i sub big n over two so big n is four so we're looking at i sub two and that's just our two by two identity matrix then we have d sub two which will be the first two diagonal elements of the four by four fourier matrix so we have uh one and minus i respectively and it's a d is defined to be a diagonal matrix so we only keep the diagonal terms from the fourier the four by four fourier matrix and we set everything else to zero and then finally we have the 2x240 matrix which i flashed on the previous slide but it turns out to be this and then the last thing is we have this permutation matrix which is defined by this expression on the right here so basically you if you have a permutation matrix and you multiply it some vector by it it'll just reshuffle the elements in your vector such that the even indexed elements are in the top half and the odd indexed elements on the bottom half okay so it's just plug and chug so we're just plugging in the two by two identity matrix the two by two diagonal matrix and these slots here we plug in the 2x240 matrix here and here and then we have our 4x4 permutation matrix so it's just we're just plugging into this expression um and it may not be immediately obvious but one thing that is promising is that these matrix have a lot of zeros in them which is good we like zeros because that means there's no multiplication to do so on the left hand side again the time complexity is n squared uh but it turns out on the right hand side we don't have n squared time complexity uh so let's take a closer look this permutation matrix is basically free so if you represent your data in a smart way in a nice data structure and use a good algorithm you can basically do this in one step for this first matrix half of it is just an identity matrix basically and then the other half is two diagonal matrices stacked on top of each other so that means half the elements so in this case 40 elements are zero so that means there are only four non-trivial terms we have to worry about so that's where this time complexity of order n comes from and then for this middle matrix half the elements are zero so there's only um two n or eight elements that we have to worry about uh so it turns out as n gets larger and larger this gives you uh you can devise an algorithm that has n log n time complexity uh so the fast fourier transform is just efficient matrix multiplication and so you know it might not be immediately obvious why this is order n log n but i want to give you some uh intuition of what's going on here so let's look at another example we have the 16 by 16 for your matrix which is given we're just plugging into the expression that i showed on the previous slide but there's nothing stopping us from playing the same game for the eight by eight fourier matrix in this middle matrix right here so we can plug that in and then we get a whole bunch more zeros again we go from uh n squared uh time complexity to order n time complexity plus order two n time complexity plus constant time complexity and then we have a 4x440 matrix here so we plug that in and we pick up even more zeros so this is kind of the intuition of what's going on we just recursively apply this matrix multiplication we rewrite our kind of n by n for your matrix in terms of the n over 2 by n over 2 for your matrix and some other terms and we get the n log n time complexity let's try to bring everything together with a concrete example uh so i wrote this example in matlab the code is available at the github page linked here and in the description so we're going to look at the spectrum of an audio signal so in this case i'm just playing the low e string on electric guitar so we can see what that looks like the first step is we read in our audio file then in one line of code in matlab we can apply the fast fourier transform uh here we're just defining some terms we're getting the length of the audio signal we're converting our frequency indices to actual frequency values uh we're computing the two-sided and then from the two-sided the one-sided power spectrum in these lines here and then we just plot everything so at the top we have our audio signal and then the bottom we have our fourier transform so you see we have these kind of discrete peaks at different frequencies so the first one is e2 the low e string 82 hertz so that's what we expected but the cool thing is we're getting exactly or approximately the harmonic series so you're just getting integer multiples of the fundamental so the fundamental frequency in this case is a 282 hertz and then we're just getting integer multiples of it so we start with 82 164 247 330 415 and so on and um you know as someone that is into both physics and music it's really nice when those two fields come together and if you keep going down the harmonic series it turns out you approximately get a major scale which is pretty cool for the physics and slash music personalities that exist inside my brain so that was the fast for you transform if you found this video helpful like subscribe comment i'd love to hear your feedback stay tuned for the next video which will extend this idea to a thing called wavelets in the wavelet transform thanks for watching
mj86XmfOniY,2020-11-15T20:41:53.000000,"Time Series, Signals, & the Fourier Transform | Introduction",yeah hey guys welcome my name is sean i'm a physics phd student at the university of texas dallas and in this video i'll be talking about the fourier transform i know there's tons maybe not even tons thousands millions of videos about fourier transforms on youtube but i just wanted to post a quick digestible video on the topic mainly to serve as a primer for my next two videos which will be on the fast fourier transform and something called the wavelet transform by no means am i an expert in any of these topics i just use them sometimes in my research in the future i'll be posting more videos relating to physics research data science machine learning so stay tuned for those again i'm not an expert in these topics so if you hear me say something stupid make a mistake say something wrong i invite you to please leave a comment let me know i'm gonna do my best to read all the comments and reply to them if you find this video helpful and you want to see more like in the future uh subscribe like share comment all that good stuff all right without any further ado let's get into the video so everyone's undoubtedly familiar with time series even even if you haven't heard the term so time series is simply a set of values indexed by time so you flip on your favorite news network you they're typically talking about how stocks are amazing or stocks are terrible or whatever and typically they're showing uh like a stock price or here we have an index price uh plotted over time so we have time on the x-axis and we have the price on the y-axis um or even more uh relevant to our daily lives seven day forecast so here we have uh the temperature plotted over time so these are time series it's just a set of values indexed over time so a signal is a specific kind of time series the only difference here is a signal typically represents a physical event uh so for example you could be listening to your favorite record that could be represented as an audio signal so you have like this time oscillating amplitude uh or so a lot of times it's going to be a voltage or you could have a biometric signal so this is something i deal with in my research uh here we have the surface body temperature plotted over time so time series signals very common everyone's probably really familiar with them this is just terminology okay so another very common thing and is very fundamental to understanding the fourier transform is waves so a wave is simply an any oscillating quantity around an equilibrium so there are two basic properties of waves amplitude which is the magnitude of the quantity and frequency which characterizes the oscillation so here we have a sine wave so the amplitude is characterized by the y-axis and the frequency is characterized by the number of peaks in a given time interval so now getting to the bread and butter the main topic main event of the video the fourier transform so before we transform intuitively is a decomposition of a signal into sines and cosines so here we have this purple signal f x plotted over time if we do the fourier transform it turns out we can decompose this into two simple sine waves of different frequencies and the way we do this is via the 40 transform which is given by this expression so we have f of k is equal to the infinite integral of f x which is our signal our time series or function whatever you want to plug in there times e to the minus 2 pi i k x dx um and if you're confused then i've been talking about sines and cosines and you don't see a sine or cosine in this expression i just want to put here as a reminder we have euler's formula which relates e to the x to cosine and sine so another way to think of the fourier transform is in simple terms is if we have a signal plotted over time uh a 40 transform simply changes our x-axis from time to frequency so here we have the same signal uh it's made up of those two simple sine waves as we saw in the previous slide so if you were to compute the fourier transform and plot the resulting what they call power spectrum uh resulting from the fourier transform you get something that looks like this bottom plot so we have two spikes at one hert and two hertz so this is just another way of thinking of the fourier transform so in the previous slide it was expressed in terms of x and k so x is usually like a spatial coordinate and k is typically like a what they call a wave number which as has units of inverse space and here we have things in terms of time t and angular frequency omega which is defined in this way two pi times the frequency the code to generate these plots is available on the github so check that out so here we're kind of getting into the practical applications of the 4u transform spectral analysis is just one of many applications of before you transform uh so spectral analysis is uh simply the examination of a signal so again it could be your audio signal your body temperature over time based on its constituent frequency energies so this is applications in uh you can look at the spectrum of light so we're all familiar with perhaps the pink floyd album dark side of the moon white light comes in rainbow comes out because white light is made up of basic constituent frequencies of different colors you can extend this idea to different light sources like the sun or led bulbs in your house or fluorescent bulbs in your house or if you are if you're really struggling incandescent bulbs also spectral analysis is very relevant to audio production i'm a musician this is something i do a lot you can kind of turn down or turn up different frequency values or ranges of your audio signal and this is relevant to make your music or any kind of audio speech given a lecture or something you can make the audio sound a lot better doing this kind of audio production and then finally something i do and deal with in my research is eeg which is measuring the surface brain activity so you can understand eeg signals you can look at them through the lens of their frequency bands some very widely used uh frequency ranges of eeg are the delta theta alpha and beta band and you may have heard of like alpha waves or beta waves in popular culture so that's what they're referencing so that was before you transform stay tuned for the next video on the fast 40 transform which makes this idea more practical again like and subscribe if you found this video helpful please leave a comment i'd love to hear your feedback i'm a phd student like i said earlier so i'm learning posting these videos as part of the learning process see you next time see you next time until next time catch you later you
Gwz4zXPeP_Q,2020-11-12T22:58:00.000000,biometricDahboard3 DEMO,n/a
lciC1s4FO0g,2020-09-23T13:02:57.000000,biometricDashboard2 DEMO,n/a
